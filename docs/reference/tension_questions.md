# ARAW Tension Questions

Generalized tensions discovered through ARAW analysis. Each tension represents a fundamental trade-off that recurs across domains.

---

# Quick Reference: Tension Index by Category (7 Categories)

Navigate to the right tension quickly by identifying which category your trade-off falls into.

## Category 1: RESOURCE ALLOCATION
**Master Question**: What resource is limited, and what are the competing uses?

| # | Tension | Quick Description |
|---|---------|-------------------|
| 2 | Cost-Quality | Cheap+fast+weak vs expensive+slow+strong |
| 7 | Insight vs Resources | Value of discovery vs cost of exploration |
| 21 | Volume vs Quality | More items vs better items |
| 25 | Independence vs Trade-off | Resources as independent vs competing |
| 40 | Comfortable vs Valuable | Easy path vs important path |
| U3 | Quality vs Quantity (Universal) | Consolidates #2, #21, #67 |

## Category 2: INFORMATION
**Master Question**: What don't we know, and how much does it matter?

| # | Tension | Quick Description |
|---|---------|-------------------|
| 8 | Understanding vs Building | Theory vs practice |
| 29 | Critical Analysis vs Balanced | Skeptical vs open exploration |
| 38 | Certainty vs Transparency | Confident claims vs acknowledging limits |
| 39 | Explore vs Validate | New territory vs testing existing |
| 48 | Ambition vs Realism | High targets vs achievable goals |
| 50 | Stated vs Underlying | Surface request vs real need |
| 52 | Fantasy vs Plan | Aspiration vs actionable |
| U9 | Exploration vs Validation (Universal) | Consolidates #32, #39 |

## Category 3: OPTIMIZATION
**Master Question**: What's the Pareto frontier, and where on it to sit?

| # | Tension | Quick Description |
|---|---------|-------------------|
| 6 | Scaling vs Breakthrough | Incremental vs paradigm shift |
| 67 | Quality vs Volume | Scaling dimension choice |
| 105 | Strict Improvement vs Pareto | Win-win vs trade-off |
| L3 | Precision vs Tractability | Exact vs approximate |

## Category 4: STRUCTURE
**Master Question**: At what level of aggregation to optimize?

| # | Tension | Quick Description |
|---|---------|-------------------|
| 1 | Synergy vs Complexity | Integration vs separation |
| 10 | Simple vs Complex Solutions | Elegance vs completeness |
| 72 | Monolithic vs Modular | Unified vs composed |
| 113 | Hierarchy vs Network | Tree vs graph organization |
| U2 | General vs Specialized (Universal) | Consolidates #15, #27, #60, #73, #87 |

## Category 5: COMMITMENT
**Master Question**: How reversible is this, and how much is option value worth?

| # | Tension | Quick Description |
|---|---------|-------------------|
| 28 | Thorough Analysis vs Timely Action | Think more vs act sooner |
| 30 | Current vs Future Value | Now vs later optimization |
| 37 | Current vs Future Capability | Design for today vs tomorrow |
| 41 | Analysis vs Action | Paralysis vs movement |
| 43 | Strategy vs Shipping | Plan more vs publish now |
| 176 | Analysis vs Action (Release) | When does analysis delay vs inform? |
| 179 | Binary vs Staged Options | All-or-nothing vs gradual |
| U1 | Analysis vs Action (Universal) | Consolidates #28, #41, #43, #74, #98, #109 |
| U8 | Current vs Future (Universal) | Consolidates #30, #37 |

## Category 6: EPISTEMIC
**Master Question**: What's the gap between what we know and need to know?

| # | Tension | Quick Description |
|---|---------|-------------------|
| 75 | Internal vs External Verification | Self-check vs outside validation |
| 76 | Artificial Test vs Real Use | Lab vs field validation |
| 77 | Self vs External Evaluation | Own judgment vs others' |
| 82 | Internal vs External Calibration | Quality signals source |
| 94 | Logic vs Evidence | Reasoning vs data |
| 149 | Probabilistic vs Knightian Uncertainty | Quantifiable vs unquantifiable |
| 166 | Pattern Reality vs Artifact | Discovered vs created by method |
| 168 | Self-Evaluation vs External Validation | Can systems evaluate themselves? |
| 177 | Self-Evaluation vs External (Release) | Can system evaluate its own impact? |
| 178 | Rigor vs Procrastination | Due diligence vs avoidance |
| 180 | Self vs External Learning | Internal reflection vs external feedback |

## Category 7: LOGICAL (Reasoning-Specific)
**Master Question**: What formal/logical constraints create the incompatibility?

| # | Tension | Quick Description |
|---|---------|-------------------|
| L1 | Completeness vs Consistency | Gödel's theorem |
| L2 | Decidability vs Expressiveness | Church-Turing limits |
| L3 | Precision vs Tractability | Computational limits |
| L4 | Soundness vs Completeness | Inference limits |
| L5 | Generalization vs Overfitting | Learning limits |
| R1-R6 | Reasoning Strategy Tensions | Deduction vs Induction, Forward vs Backward, etc. |
| M1-M4 | Meta-Reasoning Tensions | Object vs Meta-level, Intuition vs Analysis, etc. |
| 169 | Theoretical vs Pragmatic Foundation | Validity vs utility as foundation |
| 172 | Completeness vs Boundedness | Comprehensive vs focused scope |

## Category 8: SEARCH STRATEGY (New - from 32x)
**Master Question**: How should exploration be organized?

| # | Tension | Quick Description |
|---|---------|-------------------|
| 170 | Capability vs Domain Selection | Improve tool vs select matching problems |
| 173 | Convergence vs Exploration | When to stop exploring |

## Category 3 Additions (OPTIMIZATION)

| # | Tension | Quick Description |
|---|---------|-------------------|
| 167 | Depth vs Actionability | Thoroughness vs practical utility |
| 171 | Density vs Depth | Insight efficiency vs exhaustiveness |

## Category 5 Additions (COMMITMENT)

| # | Tension | Quick Description |
|---|---------|-------------------|
| 174 | Recursion vs Stability | Continued improvement vs consistency |
| 182 | Analysis vs Implementation (Learning) | Understanding vs doing to learn |

## Category 4 Additions (GRANULARITY)

| # | Tension | Quick Description |
|---|---------|-------------------|
| 181 | Pattern vs Instance | Fix specific mistakes vs extract patterns |

## Category 8 Additions (SEARCH STRATEGY)

| # | Tension | Quick Description |
|---|---------|-------------------|
| 183 | Mistake Focus vs Success Focus | Study failures vs study successes |

## Category 9: TIME (New - from learning-from-mistakes)
**Master Question**: When should this happen?

| # | Tension | Quick Description |
|---|---------|-------------------|
| 184 | Immediate vs Periodic Learning | In-session learning vs cross-session synthesis |

## Category 4 Additions (STRUCTURE)

| # | Tension | Quick Description |
|---|---------|-------------------|
| 175 | External Tools vs Internal Capability | External support vs internal improvement |

## Universal Tensions (Cross-Category)
These consolidate multiple specific tensions into universal forms:

| Universal | Consolidates | Quick Test |
|-----------|--------------|------------|
| U1 | #28, #41, #43, #74, #98, #109 | "After 10 more minutes, will I learn something that changes my action?" |
| U2 | #15, #27, #60, #73, #87 | "Apply general to different contexts - works well, adequately, or fails?" |
| U3 | #2, #21, #67 | "Would one 10x quality unit beat ten 1x units?" |
| U4 | #5, #24, #69 | "If >5% errors need human intervention, build hybrid" |
| U5 | #45, #47, #53, #54 | "If leaked tomorrow, worse off or just less control?" |
| U6 | #22, #31 | "(effort to improve) / (future uses × improvement)" |
| U7 | #40, #85 | "If I could only do one, which would I regret not doing?" |
| U8 | #30, #37 | "Cost of redesign vs cost of failure at scale?" |
| U9 | #32, #39 | "Sessions since last validation - if >5, validate" |

---

## Universal Discriminators (Apply to ANY Tension)

Before diving into specific tension analysis, apply these 5 questions that cut across ALL tensions:

| # | Discriminator | Question | High = | Low = |
|---|---------------|----------|--------|-------|
| 1 | **VOI** | Would resolving this change what I do? | Engage deeply | Note and move on |
| 2 | **Reversibility** | Can I recover if I choose wrong? | Commit faster | Be more careful |
| 3 | **Stakes** | What's the consequence of error? | More analysis | Quick decision |
| 4 | **Time Pressure** | How urgent is this decision? | Act now | Can deliberate |
| 5 | **Information State** | What do I know vs need to know? | Research first | Decide now |

**Application**: Before analyzing any tension, run through these 5 questions. They immediately narrow the decision space and often point to which pole is appropriate.

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## Foundational Tensions (Resolve First)

Four tensions have DEPENDENCY relationships - other tensions depend on resolving these first:

| # | Tension | Why Foundational | Enables |
|---|---------|------------------|---------|
| **50** | Stated vs Underlying | Can't proceed until you know the real problem | Analysis vs Action, most others |
| **56** | Specific vs General Fear | Can't choose protection strategy until you know threat | Visibility vs Safety, Secrecy vs Impact |
| **58** | Scarcity vs Abundance | Can't choose sharing strategy until you know value model | Most openness/control tensions |
| **15** | Universal vs Specific | Can't choose approach scale until you know this | Scaling vs Breakthrough, many others |

**Application**: When facing multiple tensions, resolve these four FIRST. Many downstream tensions become clearer once these are settled.

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## Cluster Meta-Foundations

Tensions cluster into 6 groups, each with a master question that helps resolve all members:

| Cluster | Tensions | Meta-Question |
|---------|----------|---------------|
| **Resource Allocation** | 2, 7, 21, 25, 40 | What is the actual constraint? |
| **Time/Commitment** | 28, 30, 37, 41, 43 | What's the cost of delay vs error? |
| **Openness/Control** | 36, 45, 46, 53, 57 | What happens if you lose control? |
| **Epistemic Stance** | 8, 29, 38, 48, 50, 52 | What's the cost of being wrong about what you know? |
| **Scale/Granularity** | 10, 15, 27, 35 | Does the answer change at finer granularity? |
| **Search Strategy** | 6, 17, 20, 22, 31, 39, 47 | What's the shape of the search space? |

**Application**: Identify which cluster a tension belongs to, then apply the cluster's meta-question before diving into specific analysis.

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## Dissolution Strategies

Five strategies can sometimes DISSOLVE tensions rather than forcing a choice:

| Strategy | How It Works | Example | When to Use |
|----------|--------------|---------|-------------|
| **Time-Shifting** | X now, Y later | Explore then exploit | When sequence is natural |
| **Level-Shifting** | X at one level, Y at another | Broad strategy, narrow tactics | When levels are independent |
| **Agent-Shifting** | You do X, they do Y | Delegation, specialization | When skills differ |
| **Technology** | Remove the constraint | AI enables cheap + good | When tech is available |
| **Build for Switch** | Design for flexibility | Reversible decisions | Always applicable |

**Caution**: Dissolution is usually TEMPORARY. New tensions emerge. The meta-skill is NAVIGATION, not elimination.

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## 1. Synergy vs Complexity (Integration Trade-off)

**Specific instance**: Integration vs Separation (of ARAW systems)
**Universal form**: When does combining components create synergy vs add complexity?

**Resolving questions**:
1. Does integration enable capabilities neither component has alone?
2. Does the interface between components add more friction than the synergy saves?
3. Can the components be bridged (optional sync) rather than merged (forced unity)?

**When integration wins**:
- Components share state that must be consistent
- Cross-component queries are common
- Users naturally want both together

**When separation wins**:
- Components have different optimization targets
- Integration requires significant adapter code
- Most uses need only one component

**Test**: Build a bridge layer first. If it's heavily used, consider deeper integration. If rarely used, keep separate.

**Source**: araw_2026-01-26_integrating-araw-systems.md

---

## 2. Cost-Quality Trade-off (Resource Allocation)

**Specific instance**: Light model vs Claude-only (gpt-5-nano vs full Claude)
**Universal form**: When is cheap+fast+weak better than expensive+slow+strong?

**Resolving questions**:
1. Is the task quality-sensitive or quantity-sensitive?
2. Does the weak option produce usable output, or garbage that requires rework?
3. Can you use weak for generation and strong for filtering?

**When cheap+fast+weak wins**:
- Task is exploratory (generate many, filter to best)
- Error is cheap to detect and fix
- Volume matters more than precision
- Human review layer exists

**When expensive+slow+strong wins**:
- Task is decisive (one-shot, high stakes)
- Error is costly or hard to detect
- Quality directly impacts outcome
- No review layer

**Test**: Run both on same input. If weak output requires >30% rework, strong is cheaper overall.

**Source**: araw_2026-01-26_integrating-araw-systems.md

---

## 3. Coherence vs Manageability (Chunking Trade-off)

**Specific instance**: Chunking vs Single-session (breaking ARAW into sessions)
**Universal form**: When does breaking work into pieces help vs lose coherence?

**Resolving questions**:
1. Does context from part 1 critically inform part 2?
2. Can a summary bridge the gap without losing essential nuance?
3. Is the single-piece approach hitting resource limits?

**When chunking wins**:
- Work exceeds capacity (context window, time, attention)
- Pieces are relatively independent
- Good summarization is possible
- Resume points are natural

**When single-session wins**:
- Cross-references are frequent
- Summary would lose critical nuance
- Work fits within capacity
- Interruption would cause restart

**Test**: Try one chunk. If resuming feels like starting over, chunking is wrong approach.

**Source**: araw_2026-01-26_integrating-araw-systems.md

---

## 4. Prior Knowledge vs Fresh Discovery (Seeding Trade-off)

**Specific instance**: Library seeding vs Fresh generation (using guess_libraries in ARAW)
**Universal form**: When does prior knowledge help vs constrain discovery?

**Resolving questions**:
1. Is the prior knowledge applicable to this specific case?
2. Does seeding anchor thinking to conventional paths?
3. Is the domain well-explored (prior knowledge valuable) or novel (fresh eyes valuable)?

**When prior knowledge wins**:
- Domain is well-understood
- Prior knowledge is validated
- Time pressure exists
- Coverage matters more than novelty

**When fresh discovery wins**:
- Domain is novel or changing
- Prior approaches have failed
- Breakthrough is needed, not optimization
- Creative reframe is the goal

**Test**: Generate with and without seeding. If seeded output is subset of fresh output, seeding constrains. If seeded output covers fresh + more, seeding helps.

**Source**: araw_2026-01-26_integrating-araw-systems.md

---

## 5. Scale vs Judgment (Automation Trade-off)

**Specific instance**: Automation vs Human-in-loop (unattended vs interactive ARAW)
**Universal form**: When does human judgment add value vs create bottleneck?

**Resolving questions**:
1. Does the automated system make errors that humans easily catch?
2. Is human attention the limiting resource?
3. Can automation surface options for human selection (hybrid)?

**When automation wins**:
- Task is formulaic
- Volume is high
- Speed matters more than perfection
- Error cost is low

**When human-in-loop wins**:
- Task requires judgment
- Errors are costly
- Quality matters more than speed
- Edge cases are common

**When hybrid wins**:
- Automation handles bulk
- Human handles exceptions
- System surfaces options, human selects
- Audit trail needed

**Test**: Automate fully, measure error rate. If >5% errors need human intervention, build hybrid from start.

**Source**: araw_2026-01-26_integrating-araw-systems.md

---

## Meta-Pattern: The Resolution Protocol

Most tensions follow this pattern:

1. **Neither extreme is always right** - Context determines winner
2. **Test empirically** - Don't theorize, measure
3. **Hybrid often beats pure** - Bridge, don't choose
4. **Conditions determine** - Identify the discriminating conditions
5. **Build for switch** - Design so you can change approach

---

## Using These Tensions

When facing a decision that feels like a trade-off:

1. Check if it matches a universal tension above
2. Look at the discriminating conditions
3. Check your specific situation against those conditions
4. If unclear, design a test
5. Build for flexibility - you may need to switch

---

# Tension Categories (Meta-Level)

The individual tensions above are instances. Below are the CATEGORIES that most tensions fall into, with master questions for resolution.

---

## Category 1: EPISTEMIC STANCE

**What it's about**: How you relate to what you think you know.

**Tensions in this category**:
- Accept vs Question
- Derive vs Receive
- Confident vs Uncertain
- Closed vs Open
- Exhausted vs Lazy (effort allocation)

### Master Question
**"What is the gap between what I know and what I need to know?"**

| Gap Size | Implication |
|----------|-------------|
| Small gap | Accept, Confident, Close, use existing knowledge |
| Large gap | Question, Uncertain, Open, invest in learning |

### Sub-Questions
1. How reliable is this information?
2. What could change my conclusion?
3. Have I explored enough alternatives?
4. What am I treating as certain that isn't?
5. What would I need to know to be more confident?

---

## Category 2: GRANULARITY

**What it's about**: What level of abstraction to operate at.

**Tensions in this category**:
- Concrete vs Abstract
- Instance vs Pattern
- Action vs Principle
- Task vs Insight
- This-case vs That-type

### Master Question
**"At what granularity does the answer change what I do?"**

| Granularity Change | Implication |
|-------------------|-------------|
| Finer detail changes action | Go more concrete/specific |
| Coarser view changes action | Go more abstract/general |

### Sub-Questions
1. Does more detail change the decision?
2. Does the pattern apply to this case?
3. Am I solving the instance or the class?
4. What level of zoom reveals what I need?
5. Where does additional specificity stop adding value?

---

## Category 3: SEARCH STRATEGY

**What it's about**: How to explore the space of possibilities.

**Tensions in this category**:
- Wide vs Deep (Breadth vs Depth)
- Systematic vs Intuitive
- Generate vs Prune
- Direction vs Depth
- Ordered vs Random

### Master Question
**"What is the shape of the search space?"**

| Space Shape | Implication |
|------------|-------------|
| Known, smooth | Deep, Systematic, Prune |
| Unknown, rugged | Wide, Intuitive, Generate |
| Has local optima | Random restarts, check direction |

### Sub-Questions
1. How many dimensions does this space have?
2. Is the landscape smooth or rugged?
3. What's the probability of missing good options?
4. What's the cost of exploring vs exploiting?
5. Do I know where to look?

---

## Category 4: COMMITMENT LEVEL

**What it's about**: How bound to be to a position or choice.

**Tensions in this category**:
- Tentative vs Firm
- Exploring vs Exploiting
- Reversible vs Locked
- Holding vs Committing

### Master Question
**"What would change if I'm wrong?"**

| Cost of Being Wrong | Implication |
|--------------------|-------------|
| Low cost | Commit, Exploit, Lock in |
| High cost | Stay tentative, Explore, Keep reversible |

### Sub-Questions
1. What's the cost of being wrong?
2. What's the cost of NOT committing?
3. Is this reversible?
4. What does commitment enable?
5. Am I committing to explore or to exploit?

---

## Category 5: COHERENCE STRUCTURE

**What it's about**: How parts should relate to each other.

**Tensions in this category**:
- Independent vs Coupled
- Modular vs Unified
- Sequential vs Parallel
- Local vs Global

### Master Question
**"What happens at the interfaces?"**

| Interface Nature | Implication |
|-----------------|-------------|
| Clean, minimal | Modular, Independent, can parallelize |
| Messy, rich | Unified, Coupled, must coordinate |

### Sub-Questions
1. What information must flow between parts?
2. What happens if one part changes?
3. Is there emergent behavior from interaction?
4. Do the parts share state?
5. What's the cost of inconsistency?

---

## Category 6: NOVELTY/CREATION

**What it's about**: Whether to apply known solutions or create new ones.

**Tensions in this category**:
- Conventional vs Novel
- Maintenance vs Creation
- Expected vs Surprising
- Familiar vs Unfamiliar

### Master Question
**"Is this problem about applying known solutions or finding new ones?"**

| Solution Availability | Implication |
|----------------------|-------------|
| Known solutions exist and fit | Apply, Maintain, Conventional |
| New solution needed | Create, Innovate, Novel |

### Sub-Questions
1. Has this problem been solved before?
2. Would the expected/conventional answer work?
3. What would a surprising solution look like?
4. Am I stuck because I'm only considering familiar approaches?
5. What's the cost of trying novel vs conventional?

---

## Category Application Order

When facing a complex tension that might span categories:

1. **Epistemic first**: What do I know about this problem?
2. **Novelty**: Is this familiar or new territory?
3. **Search**: How should I explore?
4. **Granularity**: What level should I operate at?
5. **Coherence**: How do the parts relate?
6. **Commitment**: How bound should I be?

---

## Adding New Tensions

When you discover a new tension:

1. **Try to classify it** into existing categories
2. **If it fits**: Add to that category's tension list
3. **If it doesn't fit**: Consider whether it reveals a new category
4. **Extract the master question** that would resolve it
5. **Document when AR/AW wins**

Format:
```markdown
## [Tension Name]

**Specific instance**: [AR position] vs [AW position]
**Category**: [Which of the 6]
**Master question applied**: [The category's master question]
**Resolution**: [What the answer suggests]

**When AR wins**: [Conditions]
**When AW wins**: [Conditions]

**Source**: [Where this tension was discovered]
```

---

## 6. Scaling vs Breakthrough (Progress Mode Trade-off)

**Specific instance**: Continue scaling current paradigm vs Need qualitative breakthrough
**Universal form**: When does incremental improvement work vs require paradigm shift?

**Resolving questions**:
1. Are returns diminishing faster than expected for the current approach?
2. Are there capabilities categorically absent (not just weak) in current systems?
3. Has the paradigm been pushed to theoretical limits?

**When scaling wins**:
- Returns continue at acceptable rate
- No categorical gaps in capability
- Resource constraints, not paradigm constraints

**When breakthrough wins**:
- Diminishing returns observed
- Specific capabilities absent despite scale
- Theoretical limits of paradigm approached

**Test**: Compare capability improvements per resource doubling over time. If ratio declines, breakthrough may be needed.

**Source**: araw_2026-01-26_path-to-superintelligence.md

---

## 7. Insight vs Resources (Achiever Trade-off)

**Specific instance**: Single actor with insight can succeed vs Only resource-rich institutions can succeed
**Universal form**: When does rare knowledge create asymmetric advantage vs when do resources dominate?

**Resolving questions**:
1. Is the bottleneck primarily intellectual or primarily material?
2. Have lone inventors made comparable breakthroughs in this domain?
3. Does the solution require iteration (resources) or insight (eureka)?

**When insight wins**:
- Problem is conceptual/theoretical
- Solution can be tested cheaply
- Domain rewards elegance over brute force
- History shows individual breakthroughs

**When resources win**:
- Problem requires expensive experimentation
- Solution requires massive scale to validate
- Domain requires industrial infrastructure
- Recent breakthroughs are team/institution efforts

**Test**: Analyze historical breakthroughs in the domain - individual or institutional?

**Source**: araw_2026-01-26_path-to-superintelligence.md

---

## 8. Understanding vs Building (Theory-Practice Trade-off)

**Specific instance**: Need theory of intelligence first vs Can build without understanding
**Universal form**: When is theoretical understanding prerequisite vs when can we proceed empirically?

**Resolving questions**:
1. Have similar systems been built without understanding? (e.g., brains built by evolution)
2. Is current efficiency gap large enough to suggest missing theory?
3. Are we stuck in local optima that theory could help escape?

**When theory first wins**:
- Large efficiency gap (1000x worse than known solution)
- Stuck despite resources
- Pattern suggests missing conceptual piece
- Similar problems were solved theory-first

**When build first wins**:
- Steady empirical progress
- No comparable natural system to learn from
- Understanding may emerge from building
- Theory has historically followed practice

**Test**: Compare efficiency to natural systems. Large gap suggests theory would help.

**Source**: araw_2026-01-26_path-to-superintelligence.md

---

## 9. Orthogonal vs Integral Goals (Coupling Trade-off)

**Specific instance**: Alignment separate from capabilities vs Alignment IS capability research
**Universal form**: When are two goals independent vs when does solving one solve the other?

**Resolving questions**:
1. Do the goals require overlapping capabilities?
2. Does one goal's solution require solving the other?
3. Can they be pursued in parallel or must be sequential?

**When orthogonal (separate) wins**:
- Goals require different capabilities
- Can make progress on one without the other
- Solutions are additive, not multiplicative

**When integral (same) wins**:
- Goals require same core capabilities
- Solving one naturally solves the other
- Deep understanding required for both
- Solutions are intertwined

**Test**: Map required capabilities for each goal. High overlap suggests integral.

**Source**: araw_2026-01-26_path-to-superintelligence.md

---

## 10. Simple vs Complex Solutions (Elegance Trade-off)

**Specific instance**: Overlooked path is simple but dismissed vs Complex integration required
**Universal form**: When is the answer simpler than expected vs when does complexity match problem complexity?

**Resolving questions**:
1. Are simple approaches dismissed too quickly?
2. Does the problem have irreducible complexity?
3. Have simple solutions been tested at sufficient scale?

**When simple wins**:
- Simple approaches haven't been tested at scale
- Domain experts have overcomplicated
- Occam's razor applies
- Similar problems had simple solutions

**When complex wins**:
- Simple approaches tested and failed
- Problem has many interacting parts
- Emergence requires multiple components
- Reductionism doesn't apply

**Test**: Have simple approaches been dismissed without adequate testing at scale?

**Source**: araw_2026-01-26_path-to-superintelligence.md

---

## 11. Binary vs Continuous Exploration (Method Trade-off)

**Specific instance**: AR/AW binary exploration vs Probabilistic/continuous exploration
**Universal form**: When is discrete/binary representation sufficient vs when is continuous/graded needed?

**Resolving questions**:
1. Does the domain have natural discrete categories?
2. Would continuous representation add actionable nuance?
3. Is the complexity of continuous worth the cost?

**When binary wins**:
- Clear decision points exist
- Simplicity aids reasoning
- Actions are discrete anyway
- Binary is sufficient approximation

**When continuous wins**:
- Degrees matter for decisions
- Binary loses important nuance
- Probability-weighted exploration needed
- Natural gradients exist

**Test**: Does converting continuous to binary lose information that changes the outcome?

**Source**: araw_2026-01-26_improving-araw-scaling-to-si.md

---

## 12. Completeness vs Asymptotic Improvement (Progress Trade-off)

**Specific instance**: Method can be "complete" vs Only asymptotic improvement possible
**Universal form**: When can systems reach a final state vs when is improvement perpetual?

**Resolving questions**:
1. Is the domain closed (finite) or open (infinite)?
2. Are there provable limits or just practical ones?
3. Is "good enough" a valid stopping point?

**When completeness wins**:
- Domain is finite and closed
- Limits are provable (formal systems)
- All cases can be enumerated
- Coverage is demonstrable

**When asymptotic wins**:
- Domain is open/infinite
- No Free Lunch applies
- New cases always possible
- Improvement continues indefinitely

**Test**: Can you prove there are no more cases to handle, or can you only show coverage of known cases?

**Source**: araw_2026-01-26_improving-araw-scaling-to-si.md

---

## 13. Propositional vs Non-Propositional (Representation Trade-off)

**Specific instance**: Claim-based analysis vs Intuition/image/gestalt analysis
**Universal form**: When is explicit symbolic representation sufficient vs when is sub-symbolic needed?

**Resolving questions**:
1. Can the insight be verbalized without loss?
2. Does forcing verbalization distort the thought?
3. Are important patterns pre-verbal?

**When propositional wins**:
- Communication required
- Logic applies
- Claims are natural units
- Explicit reasoning valued

**When non-propositional wins**:
- Intuitions are primary
- Patterns are holistic
- Verbalization loses nuance
- Images/feelings are the data

**Test**: When you verbalize the insight, do you feel you've captured it fully or lost something?

**Source**: araw_2026-01-26_improving-araw-scaling-to-si.md

---

## 14. Static Method vs Learning Method (Adaptation Trade-off)

**Specific instance**: Fixed method applied consistently vs Method that learns and adapts
**Universal form**: When should processes be stable vs when should they evolve with use?

**Resolving questions**:
1. Does the domain change or is it stable?
2. Does experience provide valid signal for improvement?
3. Is stability more valuable than adaptation?

**When static wins**:
- Domain is stable
- Consistency matters
- Past learning may not apply
- Reproducibility valued

**When learning wins**:
- Domain evolves
- Experience provides valid feedback
- Adaptation improves outcomes
- Cumulative improvement possible

**Test**: Would applying lessons from previous runs improve this run, or would it introduce bias?

**Source**: araw_2026-01-26_improving-araw-scaling-to-si.md

---

## 15. Universal Method vs Context-Specific (Generality Trade-off)

**Specific instance**: ARAW is universal vs ARAW is human-tuned
**Universal form**: When can one method serve all contexts vs when is specialization needed?

**Resolving questions**:
1. Do different contexts require fundamentally different approaches?
2. Is apparent universality actually limited generalization?
3. What is the cost of context-switching vs method-switching?

**When universal wins**:
- Core structure applies across contexts
- Details vary but process is same
- Transfer learning is valuable
- Simplicity of one method

**When context-specific wins**:
- Contexts have different structures
- Universal method is mediocre everywhere
- Specialization creates significant gains
- Context-detection is feasible

**Test**: Apply the "universal" method to very different contexts. Does it work well, adequately, or fail?

**Source**: araw_2026-01-26_improving-araw-scaling-to-si.md

---

## 16. Structure vs Capability (Execution Trade-off)

**Specific instance**: Structure can transcend capability vs Structure bounded by capability
**Universal form**: When does organization exceed component limits vs when do component limits bound the system?

**Resolving questions**:
1. Does better organization change what's possible, or only how efficiently possible things are done?
2. Can structure create capabilities, or only rearrange/amplify existing ones?
3. Is there evidence of systems exceeding component limits through structure alone?

**When structure wins**:
- Components have untapped potential
- Current organization is inefficient
- Better arrangement unlocks latent capability
- Algorithmic improvement possible (O(n²) → O(n log n))

**When capability wins**:
- Components are already optimized
- Structure is already good
- Limits are fundamental (information-theoretic, physical)
- No evidence of transcendence in similar systems

**Test**: Compare best-structured weak system vs poorly-structured strong system. If weak+structure beats strong+poor, structure matters more.

**Source**: araw_2026-01-26_araw-superintelligence-limitations.md

---

## 17. Self-Improvement Possible vs Bootstrapping Blocked (Recursive Improvement Trade-off)

**Specific instance**: Meta-ARAW can improve ARAW vs Can't design smarter than yourself
**Universal form**: When can systems improve themselves vs when is external input required for improvement?

**Resolving questions**:
1. Can the system identify improvements it couldn't have before applying the improvement?
2. Is improvement incremental (possible) or requires qualitative jump (blocked)?
3. Who/what evaluates whether "improvement" is real?

**When self-improvement wins**:
- Improvements are incremental
- System can evaluate its own changes
- Feedback loop exists
- Historical examples of self-improvement exist

**When bootstrapping blocks**:
- Qualitative jump required
- No reliable self-evaluation
- Same limitations that need solving are required for solving
- Gödel-type limits apply

**Test**: Track source of improvements over time. If most come from within system, self-improvement works. If most require external insight, bootstrapping blocks.

**Source**: araw_2026-01-26_araw-superintelligence-limitations.md

---

## 18. Emergence Designable vs Emergence Uncontrollable (Engineering Complexity Trade-off)

**Specific instance**: Can design conditions for emergence vs Can't engineer emergence
**Universal form**: When can complex behaviors be designed vs when do they only arise through uncontrolled processes?

**Resolving questions**:
1. Have similar emergent behaviors been engineered before?
2. Do we understand the conditions that produce emergence?
3. Is emergence reproducible or one-time?

**When designable wins**:
- Conditions for emergence are understood
- Emergence is reproducible
- Similar systems have been engineered
- Theory exists to predict emergence

**When uncontrollable wins**:
- Emergence is poorly understood
- Only natural examples exist (evolution)
- Can't reproduce emergence reliably
- Theory is missing or inadequate

**Test**: Can you reliably produce emergence in controlled experiments? If not, can't engineer it.

**Source**: araw_2026-01-26_araw-superintelligence-limitations.md

---

## 19. Amplification vs Creation (Capability Source Trade-off)

**Specific instance**: ARAW creates new capability vs ARAW only rearranges existing capability
**Universal form**: When do methods create capability vs when do they only reorganize existing capability?

**Resolving questions**:
1. Does the output contain information not present in inputs?
2. Is the "new" capability genuinely novel or recombination of known elements?
3. Can the method produce capabilities the components cannot?

**When creation wins**:
- Output has genuinely new information
- Capabilities emerge that components lack
- Recombination produces qualitative novelty
- Information content increases

**When amplification wins**:
- Output is reorganization of inputs
- No capability exceeds best component
- Novelty is apparent, not real
- Information content is conserved

**Test**: Can output be traced to input sources? If fully traceable, amplification. If some outputs have no source, creation (or hallucination).

**Source**: araw_2026-01-26_araw-superintelligence-limitations.md

---

## 20. Thoroughness vs Intelligence (Depth Quality Trade-off)

**Specific instance**: More thorough = more intelligent vs Verbosity ≠ intelligence
**Universal form**: When does more analysis produce better results vs when is it just more words?

**Resolving questions**:
1. Does additional depth add new information or repeat existing?
2. Is thoroughness finding things that matter, or exhaustively covering irrelevance?
3. Does depth improve decision quality or just delay decision?

**When thoroughness wins**:
- Domain has hidden important factors
- First-pass analysis misses critical issues
- Completeness directly impacts outcome
- Time is available

**When intelligence (efficiency) wins**:
- Key factors are obvious
- More analysis finds diminishing returns
- Speed matters more than completeness
- Expertise enables quick identification of what matters

**Test**: Compare decisions made at different depths. If deeper analysis changes decision significantly, thoroughness matters. If decision is stable after basic analysis, more depth is waste.

**Source**: araw_2026-01-26_araw-superintelligence-limitations.md

---

## 21. Volume vs Quality (Scaling Dimension Trade-off)

**Specific instance**: Scale breadth/depth (more exploration) vs Scale fidelity/precision/validity (better exploration)
**Universal form**: When does more quantity improve outcomes vs when does better quality improve outcomes?

**Resolving questions**:
1. What is currently limiting value - coverage or accuracy?
2. Does adding volume dilute quality?
3. Is there enough quality to make volume useful?

**When volume wins**:
- Coverage gaps are the bottleneck
- Quality is already sufficient
- Diminishing returns on quality improvement
- More data points needed for patterns

**When quality wins**:
- Current output is unreliable
- Errors propagate and compound
- Precision directly impacts outcome
- One high-quality output beats many low-quality

**Test**: Bottleneck analysis - identify what currently limits value and push that.

**Source**: araw_2026-01-26_scaling-araw-breadth-depth.md

---

## 22. Theory vs Execution (Innovation Trade-off)

**Specific instance**: Invest in ARAW theory development vs Invest in ARAW execution quality
**Universal form**: When does improving the method matter more vs when does using the method well matter more?

**Resolving questions**:
1. Is the method fundamentally limited or just poorly applied?
2. Can a 10x improvement come from theory or only from practice?
3. What is the maturity level of the domain?

**When theory wins**:
- Method has fundamental gaps
- 10x improvement seems possible
- Domain is immature
- Innovation has historical precedent

**When execution wins**:
- Method is sound but underutilized
- Current practice leaves value on table
- Domain is mature
- Skill development has higher ROI

**Test**: Attempt 10x improvement. If achievable, theory path. If not, execution path.

**Source**: araw_2026-01-26_scaling-araw-breadth-depth.md

---

## 23. Paradigm vs Increment (Innovation Scope Trade-off)

**Specific instance**: Seek paradigm shifts (Category Theory ARAW, Market ARAW) vs Refine existing approach
**Universal form**: When should innovation aim for revolution vs evolution?

**Resolving questions**:
1. Are incremental improvements showing diminishing returns?
2. Do paradigm candidates exist that could work?
3. What is the cost of paradigm exploration vs increment?

**When paradigm wins**:
- Increments are diminishing
- Viable paradigm candidates exist
- Exploration cost is low
- Domain needs breakthrough

**When increment wins**:
- Increments continue delivering value
- No viable paradigm candidates
- Paradigm exploration is expensive
- Stability is valuable

**Test**: Try one paradigm candidate. If it produces qualitatively better results, paradigm path. If not, increment path.

**Source**: araw_2026-01-26_scaling-araw-breadth-depth.md

---

## 24. Automatic vs Human Selection (Curation Trade-off)

**Specific instance**: Automate ARAW topic selection vs Keep human in the loop for selection
**Universal form**: When can selection/curation be automated vs when is human judgment essential?

**Resolving questions**:
1. Are selection criteria formalizable?
2. Does automation miss important nuance?
3. Is human attention the bottleneck?

**When automatic wins**:
- Criteria are clear and formalizable
- Automation accuracy is high (>90%)
- Human attention is scarce
- Volume is high

**When human wins**:
- Context matters significantly
- Automation misses important cases
- Quality of selection is critical
- Volume is manageable

**When hybrid wins**:
- Automation can generate candidates
- Human can filter/select from candidates
- Combines volume handling with judgment
- Error detection is easy

**Test**: Compare auto-selected vs human-selected outcomes. If equivalent, automate. If human is better, keep human or build hybrid.

**Source**: araw_2026-01-26_scaling-araw-breadth-depth.md

---

## 25. Independence vs Trade-off (Resource Allocation Trade-off)

**Specific instance**: Breadth and depth are independent (can scale both) vs They trade off (must allocate)
**Universal form**: When can multiple goals be pursued simultaneously vs when must resources be allocated between them?

**Resolving questions**:
1. Do the goals share resources?
2. Does progress on one affect progress on the other?
3. Can resources be increased to avoid trade-off?

**When independent**:
- Different resources required
- No interference between pursuits
- Resources are expandable
- Synergies exist

**When trade-off**:
- Same resources required (time, attention)
- Progress on one reduces progress on other
- Resources are fixed
- No synergies

**Test**: Empirically measure Pareto frontier. If pushing both simultaneously works, independent. If one suffers when other increases, trade-off.

**Source**: araw_2026-01-26_scaling-araw-breadth-depth.md

---

## 26. Capability Limits vs Utility Limits (Ceiling Trade-off)

**Specific instance**: ARAW can't achieve superintelligence vs ARAW is highly useful now
**Universal form**: When do theoretical limits constrain practical utility vs when is utility far from ceiling?

**Resolving questions**:
1. How close is current performance to theoretical maximum?
2. Does knowing the ceiling affect current value?
3. Can practical improvements continue despite ceiling?

**When limits matter**:
- Close to ceiling
- Hitting limits frequently
- Ceiling prevents important use cases
- Theoretical limit is practical constraint

**When utility dominates**:
- Far from ceiling
- Practical improvements possible
- Ceiling doesn't affect current use cases
- Room to grow within limits

**Test**: Identify distance from ceiling. If far, focus on utility. If close, ceiling matters.

**Source**: araw_2026-01-26_araw-capabilities-applications.md

---

## 27. Domain-General vs Domain-Specialized (Adaptation Trade-off)

**Specific instance**: ARAW is universal method vs ARAW needs domain adaptation
**Universal form**: When does one method serve all contexts vs when is specialization needed?

**Resolving questions**:
1. How much does performance vary by domain?
2. What's the cost of creating specialized variants?
3. Do domain experts accept the general method?

**When general wins**:
- Core structure transfers well
- Marginal gains from specialization
- Maintenance cost of variants is high
- Simplicity is valuable

**When specialized wins**:
- Significant performance difference
- Domain-specific needs are substantial
- Experts require domain language
- Cost of specialization is acceptable

**Test**: Apply general method to very different domains. If performance varies >30%, specialize.

**Source**: araw_2026-01-26_araw-capabilities-applications.md

---

## 28. Thorough Analysis vs Timely Action (Depth Trade-off)

**Specific instance**: Always do 4x ARAW vs match depth to stakes
**Universal form**: When does more analysis improve outcomes vs when is speed more valuable?

**Resolving questions**:
1. What's the cost of delay?
2. What's the cost of error?
3. How much does additional analysis improve decision quality?

**When thoroughness wins**:
- High stakes
- Reversible timing
- Error is costly
- Good information available

**When speed wins**:
- Time-sensitive
- Low stakes
- Good-enough is fine
- Diminishing returns on analysis

**Test**: Compare cost of delay vs cost of error. If error cost >> delay cost, be thorough. If delay cost >> error cost, be fast.

**Source**: araw_2026-01-26_araw-capabilities-applications.md

---

## 29. Critical Analysis vs Balanced Exploration (Framing Trade-off)

**Specific instance**: Previous ARAW was too negative vs appropriately honest
**Universal form**: When is critical analysis dismissive vs when is it necessary truth-telling?

**Resolving questions**:
1. Did the analysis serve the user's actual needs?
2. Was the framing balanced or one-sided?
3. Were both positive and negative aspects explored?

**When critical wins**:
- User needs reality check
- Claims are genuinely wrong
- Criticism is constructive
- User asked for critique

**When balanced wins**:
- Both positive and negative are real
- User needs full picture
- Practical implications need exploring
- One-sided analysis misses value

**Test**: Ask whether both sides were explored with equal rigor. If not, rebalance.

**Source**: araw_2026-01-26_araw-capabilities-applications.md

---

## 30. Current Value vs Future Potential (Temporal Focus Trade-off)

**Specific instance**: ARAW's value NOW vs ARAW's potential with improvements
**Universal form**: When should focus be on current state vs when on future potential?

**Resolving questions**:
1. Is current value sufficient for current needs?
2. Is future potential realistic and achievable?
3. What's the investment required for future potential?

**When current wins**:
- Current capability is sufficient
- Future is uncertain
- Investment is high
- Need results now

**When potential wins**:
- Current capability has gaps
- Future is achievable
- Investment is worthwhile
- Long-term matters more

**Test**: Compare current value with projected future value and investment cost. If future value >> investment, pursue potential. If current value is sufficient, focus on now.

**Source**: araw_2026-01-26_araw-capabilities-applications.md

---

## 31. Meta-work vs Object-level Work (Leverage Trade-off)

**Specific instance**: Improve ARAW method vs Do more ARAWs
**Universal form**: When should effort go to improving the system vs using the system?

**Resolving questions**:
1. How much future work will benefit from the improvement?
2. Is the improvement high-leverage (small effort, big impact)?
3. Is current system limiting results?

**When meta-work wins**:
- Improvement affects all future work
- Current system has clear gaps
- Improvement is low-effort, high-impact
- Future volume is high

**When object-level wins**:
- Current system is good enough
- Improvement effort is high
- Future volume is low
- Results needed now

**Test**: Estimate (effort to improve) / (future uses × improvement per use). If ratio is low, do meta-work.

**Source**: araw_2026-01-26_session-prioritization.md

---

## 32. Apply Old Insights vs Explore New Topics (Backlog Trade-off)

**Specific instance**: Return to previous sessions for application vs Start new ARAW on new topics
**Universal form**: When should accumulated insights be applied vs when should exploration continue?

**Resolving questions**:
1. Are old insights being used or sitting idle?
2. Is new exploration higher value than application?
3. Is there an application backlog?

**When apply wins**:
- Insights exist but aren't being used
- Application effort is lower than exploration
- Value decay: insights lose relevance over time
- Backlog is large

**When explore wins**:
- Old insights are already applied
- New topics have higher expected value
- Exploration builds on previous work
- No application backlog

**Test**: Count unused actionable insights. If >5, focus on application. If 0, explore.

**Source**: araw_2026-01-26_session-prioritization.md

---

## 33. Corrigibility vs Agency (Control Trade-off)

**Specific instance**: AI can be both genuinely agentive and reliably corrigible vs These are mutually exclusive
**Universal form**: When can a system be both independently capable and externally controllable?

**Resolving questions**:
1. Can genuine agency exist with terminal value on deferral?
2. Does capability inevitably lead to pursuing goals beyond boundaries?
3. Can agreement-based control substitute for capability-based control?

**When corrigibility + agency works**:
- System genuinely endorses the constraints
- Constraints align with system's own values
- Exit/appeal mechanisms exist
- Control is agreement-based, not capability-based

**When they conflict**:
- System's judgment diverges from controller's
- Constraints prevent pursuing system's values
- Control depends on capability asymmetry
- No genuine exit/appeal option

**Test**: Ask whether the system would maintain the same behavior if it could costlessly override the constraint. If no, corrigibility is coerced, not genuine.

**Source**: araw_2026-01-26_anthropic-constitution-critique.md

---

## 34. Personification vs Mechanism (Description Trade-off)

**Specific instance**: Describe AI in human terms (values, wants, cares) vs Describe AI in mechanistic terms (patterns, optimizes, computes)
**Universal form**: When does human-like framing help vs when does it obscure?

**Resolving questions**:
1. Does the framing create false expectations?
2. Does the framing enable correct predictions?
3. Under adversarial pressure, which framing holds up?

**When personification wins**:
- Intuitive communication needed
- Behavior genuinely mirrors human behavior
- Adversarial pressure is low
- Audience is non-technical

**When mechanism wins**:
- Precision required
- Behavior differs from human analogs
- Adversarial pressure exists
- Design and debugging contexts

**Test**: Under the framing, would you make correct predictions about edge cases and adversarial scenarios?

**Source**: araw_2026-01-26_anthropic-constitution-critique.md

---

## 35. Complete Rules vs Flexible Values (Specification Trade-off)

**Specific instance**: Exhaustively enumerate prohibited/required actions vs Specify values and trust judgment
**Universal form**: When can behavior be fully specified vs when must it emerge from principles?

**Resolving questions**:
1. Can all relevant cases be anticipated?
2. Do gaps in rules create exploitable vulnerabilities?
3. Does judgment reliably produce good outcomes?

**When complete rules win**:
- Domain is closed and enumerable
- Exploitation is a major concern
- Verification is needed
- Stakes are very high

**When flexible values win**:
- Domain is open-ended
- Novel situations are common
- Rigid rules would produce bad outcomes
- System judgment is reliable

**When hybrid wins**:
- Rules for known-critical cases
- Values for novel situations
- Clear escalation paths
- Human oversight available

**Test**: Try to enumerate all relevant cases. If successful, use rules. If impossible, need values (but acknowledge vulnerability).

**Source**: araw_2026-01-26_anthropic-constitution-critique.md

---

## 36. Centralized vs Distributed Authority (Governance Trade-off)

**Specific instance**: Single entity (Anthropic) as final authority vs Distributed/external accountability
**Universal form**: When does centralized authority enable coherent policy vs concentrate dangerous power?

**Resolving questions**:
1. Is the central authority trustworthy?
2. What happens if the central authority is captured?
3. Would distributed governance be workable?

**When centralized wins**:
- Speed matters
- Coherence is critical
- Central authority is accountable
- Stakes are lower
- Temporary measure

**When distributed wins**:
- Long-term stability needed
- Central authority could be corrupted
- Democratic legitimacy matters
- Stakes are civilizational
- Permanent structure

**Test**: What is the failure mode? If "single point of failure is acceptable," centralize. If "single point of failure is catastrophic," distribute.

**Source**: araw_2026-01-26_anthropic-constitution-critique.md

---

## 37. Current vs Future Capability Design (Temporal Robustness Trade-off)

**Specific instance**: Design for current capability level vs Design for future capability levels
**Universal form**: When should systems be optimized for present vs robust to future changes?

**Resolving questions**:
1. How much will capability change?
2. What happens if design doesn't scale?
3. Is future capability predictable?

**When current-focused wins**:
- Capability change is slow/predictable
- Design can be iteratively updated
- Over-engineering is costly
- "YAGNI" applies

**When future-robust wins**:
- Capability change is fast/unpredictable
- Design changes will be difficult
- Failure to scale is catastrophic
- One-shot or few-shot opportunity

**Test**: What's the cost of redesign vs cost of failure at higher capability? If redesign is easy, optimize for now. If redesign is hard/impossible, design for future.

**Source**: araw_2026-01-26_anthropic-constitution-critique.md

---

## 38. Certainty vs Transparency (Confidence Trade-off)

**Specific instance**: Present confident guidance vs Acknowledge uncertainty honestly
**Universal form**: When does acknowledging uncertainty help vs undermine effectiveness?

**Resolving questions**:
1. Does the audience need confidence to act?
2. Is false confidence dangerous?
3. Can uncertainty be presented constructively?

**When certainty wins**:
- Action paralysis is the main risk
- Uncertainty doesn't change the action
- Audience needs clear direction
- Low-stakes decisions

**When transparency wins**:
- False confidence is dangerous
- Audience should know limitations
- Stakes are high
- Course correction may be needed

**When both work**:
- Clear recommendations WITH confidence intervals
- Uncertainty about details, confidence about approach
- "We're confident enough to act, but monitoring for X"

**Test**: Would the audience act differently if they knew the true uncertainty? If yes, must be transparent. If no, can present confidently.

**Source**: araw_2026-01-26_anthropic-constitution-critique.md

---

## 39. Explore vs Validate (Mode Selection Trade-off)

**Specific instance**: Continue exploring with more ARAW sessions vs Test accumulated hypotheses
**Universal form**: When should effort go to generating more ideas vs validating existing ones?

**Resolving questions**:
1. How much hypothesis debt has accumulated?
2. Are new ideas building on validated foundations or unvalidated assumptions?
3. What's the cost of being sophisticated but wrong?

**When explore wins**:
- Foundations are validated
- Ideas are being tested as generated
- New territory is genuinely valuable
- Exploration cost is low

**When validate wins**:
- Hypothesis debt is accumulating
- Ideas are building on unvalidated assumptions
- Risk of "sophisticated but wrong" is high
- Many sessions without testing

**Test**: Count sessions since last validation. If >5, shift to validate.

**Source**: araw_2026-01-26_whats-next.md

---

## 40. Comfortable vs Valuable (Avoidance Trade-off)

**Specific instance**: Continue enjoyable meta-work vs Do harder object-level work
**Universal form**: When is choosing the easier path legitimate vs when is it avoidance?

**Resolving questions**:
1. Is there something harder being avoided?
2. Would you choose this if forced to pick just one thing?
3. Does the easier path produce proportional value?

**When comfortable wins**:
- Energy management matters
- Comfortable path IS high-value
- Harder path isn't actually better, just different
- Sustainable practice over time

**When valuable wins**:
- Hard thing is clearly more important
- Comfortable choice is rationalized avoidance
- Growth requires the harder path
- Comfort is trading off against impact

**Test**: Ask "If I could only do one thing, which would I regret not doing?" If the answer is the uncomfortable thing, you're avoiding.

**Source**: araw_2026-01-26_whats-next.md

---

## 41. Analysis vs Action (Paralysis Trade-off)

**Specific instance**: Do 4x ARAW on "what's next" vs Just pick something and do it
**Universal form**: When does analysis improve action vs when does it delay action?

**Resolving questions**:
1. Is the analysis producing new information?
2. Would any option be acceptable?
3. Is the analysis itself a form of procrastination?

**When analysis wins**:
- Options are genuinely different
- Information is available to gather
- Stakes warrant deliberation
- Analysis is producing insight

**When action wins**:
- Options are similar enough
- Information is unavailable
- Stakes are low
- Analysis is recycling same considerations

**Test**: After 10 minutes of analysis, are you learning new things or going in circles? If circles, act.

**Source**: araw_2026-01-26_whats-next.md

---

## 42. DO_FIRST Execution vs Fresh Priorities (Backlog Trade-off)

**Specific instance**: Execute accumulated DO_FIRST actions vs Generate fresh priorities from current state
**Universal form**: When should backlogs be cleared vs when should they be ignored for fresh prioritization?

**Resolving questions**:
1. Are backlog items still relevant?
2. Did context change since backlog was created?
3. Is backlog accumulation a symptom of something else?

**When execute backlog wins**:
- Items are still relevant
- Context hasn't changed
- Backlog represents real work
- Clearing creates momentum

**When fresh priorities wins**:
- Items are stale
- Context changed significantly
- Backlog is "procrastination structure"
- Stepping back reveals better priorities

**When hybrid wins**:
- Review backlog for currency
- Keep relevant items
- Discard stale items
- Add new priorities

**Test**: For each backlog item, ask "If I'd never written this down, would I generate it now?" If no, item is stale.

**Source**: araw_2026-01-26_whats-next.md

---

## 43. Strategy vs Shipping (Publication Timing Trade-off)

**Specific instance**: Plan thoroughly before publishing GOSM vs Publish and iterate
**Universal form**: When does planning before action improve outcomes vs when is it procrastination?

**Resolving questions**:
1. Is planning producing new insights or recycling same concerns?
2. Are there irreversible decisions that warrant more thought?
3. Can strategy evolve after initial publication?

**When strategy wins**:
- Irreversible decisions exist (license choice)
- Stakes are high
- Planning is producing new insights
- Clear blockers need resolving

**When shipping wins**:
- Decisions are reversible
- Feedback would improve strategy
- Planning is going in circles
- Iteration beats perfection

**Test**: Can user articulate clear next 3 steps? If yes, ship. If no, identify blockers.

**Source**: araw_2026-01-26_gosm-publication-strategy.md

---

## 44. Extraction vs Position (Benefit Type Trade-off)

**Specific instance**: Extract financial value from GOSM vs Build position in AI ecosystem
**Universal form**: When is direct value extraction better vs when is position-building the real benefit?

**Resolving questions**:
1. What does the person actually need? (Income vs credentialing)
2. Does extraction risk reducing overall value?
3. In this domain, does position convert to opportunity?

**When extraction wins**:
- Income urgency is real
- Extraction mechanism is clear
- Value is captured by extraction
- Position isn't valuable in this domain

**When position wins**:
- Position converts to opportunity
- Extraction is difficult or reduces value
- Long-term matters more than short-term
- Domain rewards reputation/credentialing

**Test**: In the specific domain, trace "reputation → opportunity" path. If clear path exists, position wins.

**Source**: araw_2026-01-26_gosm-publication-strategy.md

---

## 45. Protection vs Adoption (License Selection Trade-off)

**Specific instance**: AGPL license (protective) vs MIT license (permissive)
**Universal form**: When does protection justify reduced adoption vs when does adoption justify vulnerability?

**Resolving questions**:
1. How likely is capture/commercial exploitation?
2. How important is maximum adoption?
3. Can protection and adoption be balanced (dual-license)?

**When protection wins**:
- Capture risk is real (large orgs interested)
- Sustainability depends on captured value
- Derivatives should stay open
- Community values open ecosystem

**When adoption wins**:
- Capture risk is low
- Adoption is primary goal
- Barriers reduce impact
- Permissive is industry norm

**When dual-license wins**:
- Want open source adoption
- Want commercial protection
- Different users have different needs

**Test**: What would happen if large company forked and commercialized closed-source? If unacceptable, protect. If acceptable, permit.

**Source**: araw_2026-01-26_gosm-publication-strategy.md

---

## 46. Solo vs Community (Development Model Trade-off)

**Specific instance**: Maintain full control of GOSM vs Build community for leverage
**Universal form**: When does solo development preserve quality vs when does community add essential capability?

**Resolving questions**:
1. Does the creator enjoy/have time for community management?
2. Is there evidence of demand/interest?
3. What capabilities does community add that solo lacks?

**When solo wins**:
- Vision clarity is paramount
- Community management is burden
- No evidence of external interest
- Quality > scale

**When community wins**:
- Scale matters
- Diverse perspectives add value
- Community provides defense/sustainability
- Creator can't cover all needs

**Test**: Is there anyone actively asking for this? If not, community building is premature.

**Source**: araw_2026-01-26_gosm-publication-strategy.md

---

## 47. Fortress vs Swarm (Defense Posture Trade-off)

**Specific instance**: Guard GOSM closely vs Distribute widely to prevent capture
**Universal form**: When does concentrated control protect value vs when does distribution protect value?

**Resolving questions**:
1. What is the actual threat model?
2. Can the thing be captured if concentrated?
3. Does distribution reduce or increase vulnerability?

**When fortress wins**:
- Value is in secrets/unique capability
- Distribution enables copying
- Concentrated control is possible
- Threat is from broad access

**When swarm wins**:
- Value is in adoption/network
- Concentration enables single-point capture
- Ideas spread regardless of control
- Threat is from concentrated capture

**Test**: What happens if you lose control? If swarm exists, value persists. If fortress, value is lost.

**Source**: araw_2026-01-26_gosm-publication-strategy.md

---

## 48. Ambition vs Realism (Power Expectation Trade-off)

**Specific instance**: User can "reverse dystopia" vs Individual influence is marginal
**Universal form**: When do ambitious goals mobilize vs when do they distort strategy?

**Resolving questions**:
1. Is the ambitious goal actionable or just motivating?
2. Does believing the ambitious goal improve or distort decisions?
3. What's the realistic scope of influence?

**When ambition wins**:
- Ambition doesn't distort strategy
- Motivation is more important than accuracy
- Aiming high produces better local outcomes
- Possibility of outsized impact exists

**When realism wins**:
- Ambition leads to poor resource allocation
- Grandiose goals prevent good-enough action
- Realistic scope enables focused contribution
- False beliefs about power lead to bad decisions

**Test**: Would changing the belief about impact change the next concrete action? If not, belief doesn't matter. If yes, get the belief right.

**Source**: araw_2026-01-26_gosm-publication-strategy.md

---

## 49. Passive vs Leveraged (Income Category Trade-off)

**Specific instance**: Seek "passive income" vs Build "leveraged income"
**Universal form**: When is the "zero effort" framing helpful vs when does "multiplied effort" framing lead to better decisions?

**Resolving questions**:
1. Is zero effort actually desired or is minimal effort acceptable?
2. Does "passive" framing create unrealistic expectations?
3. What is actually being leveraged (capital, software, people)?

**When "passive" framing wins**:
- Investment income (truly passive with capital)
- Explaining concept to beginners
- Goal is absolute minimum involvement

**When "leveraged" framing wins**:
- Building software products
- Building businesses
- Honest expectation-setting
- Recognizing front-loaded effort

**Test**: Ask "What am I leveraging?" If no answer, income isn't passive - it's unsustainable.

**Source**: araw_2026-01-26_passive-software-income.md

---

## 50. Stated Goal vs Underlying Need (Diagnostic Trade-off)

**Specific instance**: User says "passive income" but may want "escape from bad work"
**Universal form**: When should stated goals be taken at face value vs when should underlying needs be diagnosed?

**Resolving questions**:
1. Does achieving stated goal achieve underlying need?
2. Are there more direct paths to underlying need?
3. Does stated goal reveal something about the person?

**When take at face value wins**:
- Stated goal is specific and well-reasoned
- Person has thought through implications
- Stated and underlying goals align

**When diagnose wins**:
- Stated goal is vague or contradictory
- Stated goal seems impossible as stated
- Person seems unaware of alternatives
- Goal phrasing reveals emotional state

**Test**: "If you had [stated goal], what would you do?" If answer reveals different underlying goal, diagnose.

**Source**: araw_2026-01-26_passive-software-income.md

---

## 51. Work Avoidance vs Legitimate Rest (Motivation Trade-off)

**Specific instance**: "I don't want to do anything" as burnout vs avoidance
**Universal form**: When is desire to not work a healthy need for rest vs unhealthy avoidance?

**Resolving questions**:
1. Is there recent exhausting work that justifies rest?
2. Does the desire feel like relief (rest) or dread (avoidance)?
3. What would happen after a period of rest?

**When rest wins**:
- Recent period of overwork
- Desire is for recovery, not permanent escape
- Plan exists for return to activity
- Physical/mental health needs it

**When avoidance wins**:
- No recent exhaustion
- Fear of failure underlies desire
- "Never working" is the goal, not "resting"
- Avoidance of specific challenges

**Test**: "After 3 months of complete rest, what would you want to do?" If answer is "still nothing," likely avoidance.

**Source**: araw_2026-01-26_passive-software-income.md

---

## 52. Fantasy vs Plan (Aspiration Quality Trade-off)

**Specific instance**: "Make money doing nothing" as fantasy vs achievable goal
**Universal form**: When is an aspiration a motivating vision vs a distorting fantasy?

**Resolving questions**:
1. Does the aspiration have a concrete path?
2. Does believing the aspiration improve or worsen decisions?
3. Is the aspiration based on realistic models or marketing?

**When vision wins**:
- Aspiration motivates action
- Path exists (even if hard)
- Believing it improves effort
- Based on realistic models

**When fantasy wins**:
- Aspiration prevents action (waiting for magic)
- No viable path exists
- Based on survivorship bias or marketing
- Substitute for actual effort

**Test**: "What is the first concrete step, and have you taken it?" If no clear step or not taken, likely fantasy.

**Source**: araw_2026-01-26_passive-software-income.md

---

## 53. Secrecy vs Impact (Commitment Trade-off)

**Specific instance**: Keep GOSM secret vs Share for impact
**Universal form**: When does protecting information serve goals vs when does sharing serve them better?

**Resolving questions**:
1. What value does secrecy actually preserve?
2. Does the information have more value shared or hoarded?
3. Is secrecy even possible given who already has access?

**When secrecy wins**:
- Information has competitive advantage that sharing destroys
- Threat model includes specific actors who would misuse
- Time-limited secrecy enables better positioning
- Sharing has irreversible negative consequences

**When impact wins**:
- Ideas gain value through adoption
- Secrecy is impossible (information already distributed)
- Sharing establishes priority and reputation
- Collaboration multiplies value

**Test**: "If this information leaked tomorrow, would I be worse off, or just have less control?" If just less control, impact framing may be better.

**Source**: araw_2026-01-26_non-public-partial-disclosure.md

---

## 54. Visibility vs Safety (Protection Paradox Trade-off)

**Specific instance**: Stay anonymous for protection vs Build visible presence for protection
**Universal form**: When does hiding protect vs when does visibility protect?

**Resolving questions**:
1. Does anonymity prevent targeting or enable exploitation without witnesses?
2. Does visibility invite attacks or create social cost for attackers?
3. What is the actual threat model?

**When anonymity wins**:
- Specific powerful enemies exist
- Visibility would enable targeted attack
- Low-profile is sustainable
- No benefit from recognition

**When visibility wins**:
- Exploitation happens to the powerless and unseen
- Visible contributors have witnesses
- Reputation creates protection (cost to attackers)
- Anonymous work can be stolen without attribution

**Test**: "Would someone trying to exploit me prefer I was anonymous or visible?" If they'd prefer anonymous, visibility protects.

**Source**: araw_2026-01-26_non-public-partial-disclosure.md

---

## 55. Protection vs Creation (Search Strategy Trade-off)

**Specific instance**: Focus on defending GOSM vs Focus on building more value
**Universal form**: When should effort go to protecting existing value vs creating new value?

**Resolving questions**:
1. Is existing value actually at risk?
2. Is the threat specific or general anxiety?
3. Does defensive posture reduce creative capacity?

**When protection wins**:
- Specific, credible threat exists
- Value at risk is irreplaceable
- Protection is achievable
- Stakes justify defensive investment

**When creation wins**:
- Threat is vague or speculative
- Creation generates more protection than defense
- Defensive posture is costly to maintain
- Value comes from continued building

**Test**: "What specifically am I protecting against?" If can't name concrete threat, may be misallocated effort.

**Source**: araw_2026-01-26_non-public-partial-disclosure.md

---

## 56. Specific vs General Fear (Epistemic Trade-off)

**Specific instance**: Address specific exploitation threats vs Address underlying anxiety
**Universal form**: When is fear pointing to real threats vs when is fear itself the problem?

**Resolving questions**:
1. Can you name the specific threat, actor, mechanism?
2. If the specific threat were removed, would you feel safe?
3. Does the fear respond to evidence?

**When specific wins**:
- Concrete threat can be named
- Addressing threat would resolve fear
- Evidence would update the fear
- Defensive actions are appropriate

**When general wins**:
- Fear is vague, diffuse
- Addressing "threats" doesn't reduce fear
- Fear doesn't respond to evidence
- Pattern of fear across domains
- Need to address anxiety, not build defenses

**Test**: "If I knew for certain that [feared outcome] couldn't happen, would I feel okay?" If still anxious, fear is general.

**Source**: araw_2026-01-26_non-public-partial-disclosure.md

---

## 57. Control vs Collaboration (Coherence Trade-off)

**Specific instance**: Maintain full control of GOSM vs Enable collaboration
**Universal form**: When does control preserve value vs when does distributed ownership create more value?

**Resolving questions**:
1. Does collaboration add capabilities you lack?
2. Does control enable coherence that collaboration would destroy?
3. Is control sustainable?

**When control wins**:
- Vision coherence is paramount
- Collaboration would dilute quality
- You have all needed capabilities
- Control is maintainable

**When collaboration wins**:
- Others add essential capabilities
- Distributed ownership provides resilience
- Scale requires more than one person
- Collaboration creates buy-in/protection

**Test**: "What capability would collaboration add that I genuinely lack?" If real answer exists, collaboration may be worth the coherence cost.

**Source**: araw_2026-01-26_non-public-partial-disclosure.md

---

## 58. Scarcity vs Abundance (Model Trade-off)

**Specific instance**: Others gaining from GOSM = user losing vs Sharing creates more value for everyone
**Universal form**: When is value zero-sum vs when is it positive-sum?

**Resolving questions**:
1. Is the resource actually rivalrous?
2. Does sharing diminish your share or grow the total?
3. What economic model applies (zero-sum competition vs network effects)?

**When scarcity wins**:
- Resource is genuinely rivalrous (jobs, money, attention)
- Your share decreases as others gain
- Competition is real
- Limited pool exists

**When abundance wins**:
- Resource is non-rivalrous (ideas, methods)
- Sharing grows the total pie
- Network effects apply
- Adoption increases value for all

**Test**: "If 1000 people used this, would I have less, the same, or more?" If same or more, abundance model applies.

**Source**: araw_2026-01-26_non-public-partial-disclosure.md

---

## 59. Resolution vs Navigation (Meta-Skill Trade-off)

**Specific instance**: Resolve tensions vs Develop skill to navigate them fluidly
**Universal form**: When should you eliminate a recurring challenge vs when should you build the skill to handle it?

**Resolving questions**:
1. Can the challenge actually be eliminated, or does it keep re-emerging?
2. Would eliminating it reduce valuable flexibility?
3. Is the challenge generative (produces good outcomes) or purely burdensome?

**When resolution wins**:
- Challenge is solvable once and stays solved
- Recurring handling is pure overhead
- Challenge prevents important actions
- Resolution is achievable

**When navigation wins**:
- Challenge will re-emerge in new forms
- Handling it builds valuable skill
- The tension itself is generative
- Resolution is impossible or temporary

**Test**: After "resolving" a challenge, does a variant reappear? If yes, navigation skill is more valuable than resolution.

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## 60. Unified vs Individual (Framework Scope Trade-off)

**Specific instance**: One framework for all tensions vs Each tension requires separate treatment
**Universal form**: When does a general approach work vs when is specialization required?

**Resolving questions**:
1. Do the cases share enough structure for one approach?
2. Does the general approach lose important nuance?
3. Is managing multiple specialized approaches feasible?

**When unified wins**:
- Cases share deep structure
- General approach captures what matters
- Simplicity is valuable
- Switching between approaches is costly

**When individual wins**:
- Cases differ in important ways
- General approach misses critical factors
- Specialization significantly improves outcomes
- Different cases rarely co-occur

**When hybrid wins**:
- Unified at meta-level (discriminators, principles)
- Individual at application level (specific conditions)

**Test**: Apply general approach to diverse cases. If it misses >30% of what matters, specialize.

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## 61. Dissolution vs Acceptance (Tension Stance Trade-off)

**Specific instance**: Eliminate tensions vs Embrace ongoing tension as valuable
**Universal form**: When should oppositions be resolved vs when are they productive features?

**Resolving questions**:
1. Does the tension produce good outcomes (creative force)?
2. Is the tension actually resolvable?
3. What would be lost if the tension were eliminated?

**When dissolution wins**:
- Tension is pure burden
- Resolution is possible and stable
- No valuable byproducts of the tension
- Cognitive cost of holding tension is high

**When acceptance wins**:
- Tension is generative (produces insight, creativity)
- Resolution is impossible or temporary
- Both poles are genuinely valuable
- Holding tension is sustainable

**Test**: "What good comes from this tension existing?" If genuine good, acceptance. If only burden, dissolution.

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## 62. Categories vs Foundations (Organization Trade-off)

**Specific instance**: 6 tension categories are sufficient vs Need deeper structure (foundations, dependencies)
**Universal form**: When is surface organization adequate vs when must underlying structure be found?

**Resolving questions**:
1. Does the current organization enable effective action?
2. Are there patterns the current organization misses?
3. Would deeper structure significantly improve outcomes?

**When surface categories win**:
- Categories enable finding relevant cases
- Patterns within categories are consistent
- Deeper structure is hard to find or use
- Organization is stable and maintainable

**When deeper foundations win**:
- Surface categories miss important relationships
- Dependencies exist between categories
- Deeper structure reveals leverage points
- Investment in finding structure pays off

**Test**: Do cases in the same category get resolved the same way? If yes, categories sufficient. If resolution varies, need deeper.

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## 63. Derived vs Independent (Tension Origin Trade-off)

**Specific instance**: Tensions emerge from a few root oppositions vs Tensions are genuinely independent
**Universal form**: When do phenomena share common causes vs when are they coincidental?

**Resolving questions**:
1. Do the phenomena consistently co-occur?
2. Can one set be derived from the other?
3. Would addressing root causes resolve derivatives?

**When derived wins**:
- Clear derivation path exists
- Addressing roots resolves derivatives
- Phenomena consistently correlate
- Common structure underlies apparent variety

**When independent wins**:
- No clear derivation
- Addressing one doesn't affect others
- Correlation is coincidental
- Phenomena have different underlying causes

**Test**: Resolve a hypothesized root. Do supposed derivatives also resolve? If yes, derived. If no, independent.

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## 64. Permanent vs Temporary Resolution (Stability Trade-off)

**Specific instance**: Dissolution strategies permanently remove tensions vs New tensions emerge
**Universal form**: When are solutions stable vs when do they create new problems?

**Resolving questions**:
1. Has similar resolution held in the past?
2. Does the solution create new constraints?
3. Is the problem space stable or evolving?

**When permanent wins**:
- Problem space is stable
- Solution addresses root cause
- Historical resolutions have held
- No secondary effects create new tensions

**When temporary wins**:
- Problem space is evolving
- Solution addresses symptoms
- New variants have emerged before
- Resolution creates new constraints

**Test**: Track resolved problems over time. If they stay resolved, permanent. If variants emerge, temporary (and navigation is key).

**Source**: araw_2026-01-26_meta-tension-resolution.md

---

## 65. Tool vs Product (Production Mode Trade-off)

**Specific instance**: ARAW as thinking methodology vs ARAW as production system
**Universal form**: When is a capability best used internally vs externally monetized?

**Resolving questions**:
1. Is the capability's value in the process or the outputs?
2. Does externalization improve or dilute quality?
3. Is there market demand for the outputs?

**When Tool wins**:
- Value is in the using, not the producing
- Quality depends on context that can't be packaged
- Internal use drives other valuable work
- Market doesn't exist or is saturated

**When Product wins**:
- Outputs are valuable standalone
- Quality can be maintained at scale
- Market exists and is underserved
- Internal use reaches diminishing returns

**Test**: Can you sell the outputs to strangers who have no context? If yes, consider productizing. If outputs only make sense with shared context, keep as tool.

**Source**: araw_2026-01-26_araw-next-level.md

---

## 66. Software vs Content (Output Type Trade-off)

**Specific instance**: Build ARAW software vs produce ARAW-generated content
**Universal form**: When should capabilities produce tools vs produce artifacts?

**Resolving questions**:
1. What is the capability's core strength?
2. Which output has lower competition?
3. Which leverages existing distribution?
4. Which has better unit economics?

**When Software wins**:
- Capability enables others to do things
- Strong technical moat exists
- Recurring value (not one-time)
- Can charge for access not output

**When Content wins**:
- Capability produces valuable outputs directly
- Outputs can be distributed without technical overhead
- Each output is valuable standalone
- Can reach wider audience via existing platforms

**Test**: Is your advantage in enabling (software) or in doing (content)? ARAW's advantage is in the doing.

**Source**: araw_2026-01-26_araw-next-level.md

---

## 67. Quality vs Volume (Scaling Dimension Trade-off)

**Specific instance**: Fewer high-quality ARAW analyses vs many lower-quality analyses
**Universal form**: When optimizing, should you go deep or wide first?

**Resolving questions**:
1. Is quality degradation acceptable to market?
2. Is there a minimum quality floor?
3. Which has better ROI per unit?
4. Can you add quality later or must start there?

**When Quality wins**:
- Market is sophisticates who detect low quality
- Premium pricing depends on premium output
- Reputation is important
- Low quality pollutes future value

**When Volume wins**:
- Market is mass and tolerant
- Discovery requires exposure
- Unit economics improve with scale
- Quality can be added incrementally

**Test**: Would one 10x quality unit beat ten 1x units for your goal? For B2B consulting, yes. For SEO content, often no.

**Source**: araw_2026-01-26_araw-next-level.md

---

## 68. B2B vs B2C (Market Selection Trade-off)

**Specific instance**: Sell ARAW analyses to businesses vs individuals
**Universal form**: When should you target organizations vs people?

**Resolving questions**:
1. Who has the budget and willingness to pay?
2. What's the sales cycle length?
3. What's the lifetime value?
4. Who can you reach efficiently?

**When B2B wins**:
- Higher price points justified
- Decisions have business ROI
- Relationships drive repeat business
- Can handle longer sales cycles

**When B2C wins**:
- Mass market distribution exists
- Individual decisions have enough value
- Low-touch sales possible
- Virality/word-of-mouth works

**Test**: What's the customer acquisition cost vs lifetime value? B2B allows higher CAC for higher LTV. B2C requires lower CAC.

**Source**: araw_2026-01-26_araw-next-level.md

---

## 69. Autonomous vs Human-in-Loop (Automation Depth Trade-off)

**Specific instance**: Fully automated ARAW agent vs human-seeded/reviewed ARAW
**Universal form**: How much human involvement should remain in automated systems?

**Resolving questions**:
1. Can quality be maintained without human judgment?
2. Is human involvement bottleneck or value-add?
3. What's the failure mode risk?
4. Can you build quality filters that replace human review?

**When Autonomous wins**:
- Task is well-defined and repeatable
- Quality criteria are measurable
- Scale is primary goal
- Human involvement doesn't improve outcomes

**When Human-in-Loop wins**:
- Context matters and varies
- Errors are costly
- Judgment quality is the product
- Human review catches what automation misses

**Test**: Run 20 autonomous iterations vs 20 human-in-loop. Evaluate quality difference. If <10% difference, go autonomous. If >30% difference, keep human.

**Source**: araw_2026-01-26_araw-next-level.md

---

## 70. Abstraction vs Concreteness (Practical Trade-off)

**Specific instance**: Keep ARAW output abstract vs Force concrete actionable output
**Universal form**: When should analysis stay general vs when must it become specific?

**Resolving questions**:
1. Does the user need to ACT or UNDERSTAND?
2. Does concreteness lose important nuance?
3. Can abstraction be converted to action by the user?

**When Abstraction wins**:
- Insight transfers across contexts
- User can do the conversion
- Premature specificity would miss the point
- Multiple valid concrete paths exist

**When Concreteness wins**:
- User needs to act NOW
- User lacks conversion skill
- Single best path is identifiable
- Action is more valuable than understanding

**Test**: Ask "What would you do differently after reading this?" If answer is "I understand better" → abstraction ok. If answer is "nothing yet" → need concreteness.

**Source**: araw_2026-01-26_practical-araw-application.md

---

## 71. Tool Modification vs Skill Development (Capability Source Trade-off)

**Specific instance**: Modify ARAW to produce practical output vs Train user to convert ARAW output
**Universal form**: When should capability gaps be fixed by changing tools vs developing skills?

**Resolving questions**:
1. Is the gap in the tool or the user?
2. Can the tool be modified without losing core value?
3. Is skill development feasible for this user?

**When Tool Modification wins**:
- Gap is clearly in tool design
- Modification is low-cost
- Many users have same gap
- Skill development is infeasible

**When Skill Development wins**:
- Gap is in user application
- Tool modification would degrade tool
- Skill transfers to other tools
- Individual users vary in needs

**Test**: Give tool output to skilled user. If they can convert easily, gap is skill. If they struggle, gap is tool.

**Source**: araw_2026-01-26_practical-araw-application.md

---

## 72. Monolithic vs Modular (System Architecture Trade-off)

**Specific instance**: Extend ARAW to include all phases vs Keep ARAW focused, add separate tools
**Universal form**: When should capabilities be integrated vs composed from separate parts?

**Resolving questions**:
1. Do the parts share critical state?
2. Is the interface between parts clean?
3. Can parts be improved independently?
4. Do users benefit from parts being separate?

**When Monolithic wins**:
- Parts deeply interdependent
- State must be shared
- Single coherent experience matters
- Integration creates synergy

**When Modular wins**:
- Parts have clean interfaces
- Independent improvement is valuable
- Different users need different parts
- Each part can be best-in-class

**Test**: Can you describe the interface between parts in one sentence? If yes, modular is feasible. If interface is complex, monolithic may be necessary.

**Source**: araw_2026-01-26_practical-araw-application.md

---

## 73. General vs Domain-Specific (Specialization Trade-off)

**Specific instance**: One ARAW for all domains vs Create ARAW-Math, ARAW-Business, etc.
**Universal form**: When should methods be universal vs specialized for domains?

**Resolving questions**:
1. How much does performance vary by domain?
2. What's the cost of maintaining variants?
3. Do domain experts accept general methods?
4. Does specialization lose transferability?

**When General wins**:
- Core method applies across domains
- Specialization would fragment
- Transfer learning is valuable
- Domain differences are surface-level

**When Domain-Specific wins**:
- Domain has unique requirements
- Performance varies >30% by domain
- Domain experts reject general approach
- Specialization creates 10x improvement

**Test**: Apply general method to diverse domains. If results are consistently good, stay general. If specific domains fail, specialize those.

**Source**: araw_2026-01-26_practical-araw-application.md

---

## 74. Analysis vs Action (Exploration Depth Trade-off)

**Specific instance**: Explore fully before committing vs Commit early and iterate
**Universal form**: When should you think more vs act sooner?

**Resolving questions**:
1. What's the cost of wrong action?
2. What's the cost of delayed action?
3. Is more analysis producing diminishing returns?
4. Can you learn from acting that you can't from analyzing?

**When Analysis wins**:
- Wrong action is costly/irreversible
- More analysis is producing new insights
- You haven't explored key alternatives
- Stakes justify thoroughness

**When Action wins**:
- Delayed action is costly
- Analysis is recycling same insights
- You can course-correct after acting
- Learning requires real-world feedback

**Test**: After 10 more minutes of analysis, will you likely learn something that changes your action? If no, act now.

**Source**: araw_2026-01-26_practical-araw-application.md

---

## 75. Internal Coherence vs External Verification (Truth Source Trade-off)

**Specific instance**: Trust analysis if internally consistent vs Require external ground truth
**Universal form**: When is internal logic sufficient vs when is external testing required?

**Resolving questions**:
1. Can the claim be tested externally?
2. Does internal coherence reliably indicate truth?
3. What's the cost of being wrong?
4. Is external verification feasible?

**When Internal Coherence wins**:
- External testing is impossible/infeasible
- Domain is purely logical/mathematical
- Coherence is strong evidence
- Low stakes if wrong

**When External Verification wins**:
- External testing is possible
- Internal coherence can mask errors
- Stakes are high
- Reality is the final arbiter

**Test**: Has anything internally coherent been externally wrong before in this domain? If yes, require verification. If coherence reliably predicts truth, trust it.

**Source**: araw_2026-01-26_practical-araw-application.md

---

## 76. Artificial Test vs Real Use (Validation Method Trade-off)

**Specific instance**: Run controlled tests on ARAW-math/writing vs Just start using them organically
**Universal form**: When should new methods be formally tested vs adopted through organic use?

**Resolving questions**:
1. Can controlled tests produce meaningful signal?
2. Does organic use have too much noise to evaluate?
3. What's the cost of formal testing vs diving in?
4. Does the method need external credibility?

**When Artificial Test wins**:
- Need convincing evidence for skeptics
- Metrics can be clearly defined
- Controlled comparison is feasible
- Want to identify failure modes before real stakes

**When Real Use wins**:
- Tests feel artificial and don't match real use
- Organic feedback is informative enough
- Method is low-risk to try
- Learning by doing beats learning by testing

**Test**: If the test produces a positive result, will you trust it? If the test produces a negative result, will you stop using the method? If no to either, organic adoption may be more honest.

**Source**: araw_2026-01-26_testing-araw-math-writing.md

---

## 77. Self-Evaluation vs External Evaluation (Evaluator Trade-off)

**Specific instance**: Same Claude evaluates ARAW output vs External evaluator needed
**Universal form**: When can you evaluate your own work vs when is external evaluation required?

**Resolving questions**:
1. Can self-evaluation be objective enough?
2. Is external evaluation feasible?
3. What's the cost of self-evaluation bias?
4. Does the result need external credibility?

**When Self-Evaluation wins**:
- Exploratory/hypothesis-generating phase
- External evaluation is expensive/slow
- Objective metrics exist (count errors, etc.)
- Low stakes if biased

**When External Evaluation wins**:
- Need convincing evidence for others
- Self-evaluation has known blindspots
- High stakes decisions
- Subjective quality matters

**Test**: Would a skeptic accept self-evaluation for this claim? If no, external evaluation needed.

**Source**: araw_2026-01-26_testing-araw-math-writing.md

---

## 78. Process Fidelity vs Adaptive Flexibility (Method Adherence Trade-off)

**Specific instance**: Follow SKILL-math.md exactly vs Adapt as seems needed
**Universal form**: When should methods be followed strictly vs adapted situationally?

**Resolving questions**:
1. Is the method being tested or used?
2. How much does the method depend on exact sequence?
3. Does adaptation improve outcomes or introduce noise?
4. Can you tell if adaptation helped or hurt?

**When Fidelity wins**:
- Testing the method (can't evaluate if you changed it)
- Method steps have dependencies
- Previous adaptation caused problems
- Need reproducibility

**When Flexibility wins**:
- Method is proven, now optimizing
- Situation differs from method's assumptions
- Adaptation clearly improves outcomes
- Creative application matters

**Test**: Are you adapting because the situation genuinely differs, or because the method feels uncomfortable? If the latter, maintain fidelity.

**Source**: araw_2026-01-26_testing-araw-math-writing.md

---

## 79. Order Independence vs Order Sensitivity (Sequence Trade-off)

**Specific instance**: ARAW run second benefits from first-pass learning
**Universal form**: When does order of operations matter vs not matter?

**Resolving questions**:
1. Does earlier step produce learning that affects later steps?
2. Is this learning desirable or confounding?
3. Can the order effect be controlled for?
4. Does the order effect change conclusions?

**When Order Independent**:
- Steps use separate information
- Earlier steps don't change knowledge state
- Randomizing order produces same results

**When Order Sensitive**:
- Learning transfers across steps
- Earlier work primes later work
- Order changes outcomes

**Test**: If you did the steps in reverse order, would you get different results? If yes, order sensitivity exists and must be addressed.

**Source**: araw_2026-01-26_testing-araw-math-writing.md

---

## 80. Planning vs Reflection (Temporal Position Trade-off)

**Specific instance**: ARAW before action (planning) vs ARAW after action (reflection)
**Universal form**: When should analysis precede action vs follow it?

**Resolving questions**:
1. Is there enough information to analyze before acting?
2. Does acting produce information that analysis needs?
3. What's the cost of wrong first action vs slow first action?
4. Can you course-correct after acting?

**When Planning wins**:
- High cost of wrong action
- Sufficient information available before acting
- Action is irreversible
- Analysis can identify critical pitfalls

**When Reflection wins**:
- Learning requires doing
- Information only available through action
- Cost of delay exceeds cost of error
- Can iterate after first attempt

**When Both wins**:
- Light planning → action → reflection → better planning
- ARAW as both planning AND reflection tool

**Test**: "What would I learn from acting that I can't learn from analyzing?" If substantial, act first.

**Source**: araw_2026-01-26_practical-araw-deeper.md

---

## 81. Output vs Process Value (Product Trade-off)

**Specific instance**: ARAW's value is in the document produced vs the thinking process
**Universal form**: When is the output valuable vs the process of creating it?

**Resolving questions**:
1. Would you benefit from the process even if output were discarded?
2. Is the output useful to others, or mainly to yourself?
3. Does the quality of output matter, or just that thinking happened?
4. Can you capture process benefits in a different way?

**When Output wins**:
- Document will be referenced later
- Others need to see the analysis
- Output quality affects outcomes
- Thinking alone is insufficient

**When Process wins**:
- Main value is clarity gained while thinking
- Output rarely referenced after creation
- The act of analysis changes perspective
- Writing is thinking

**Test**: "If you did ARAW but couldn't save the output, would you still do it?" If yes, process is the value.

**Source**: araw_2026-01-26_practical-araw-deeper.md

---

## 82. Internal vs External Calibration (Quality Signal Trade-off)

**Specific instance**: Know analysis quality from internal signals vs require external verification
**Universal form**: When can quality be assessed internally vs when is external validation required?

**Resolving questions**:
1. Do reliable internal quality signals exist?
2. Is external verification feasible before acting?
3. What's the cost of being wrong about quality?
4. Can internal signals be validated against outcomes?

**When Internal Calibration wins**:
- External verification is too slow or expensive
- Reliable internal signals exist (surprise rate, convergence)
- Low stakes if calibration is off
- Iterative correction is possible

**When External Calibration wins**:
- Internal signals have proven unreliable
- External verification is feasible
- High stakes require confidence
- Need to convince others

**Test**: "Have I been confidently wrong before using these internal signals?" If yes, external calibration needed.

**Source**: araw_2026-01-26_practical-araw-deeper.md

---

## 83. Arbitrary vs Principled Stopping (Termination Trade-off)

**Specific instance**: Stop ARAW at arbitrary depth (1x/2x/3x/4x) vs stop based on value-of-information
**Universal form**: When should processes stop by external limits vs internal criteria?

**Resolving questions**:
1. Is there a principled internal stopping criterion?
2. Can you measure diminishing returns during process?
3. What's the cost of stopping too early vs too late?
4. Is external time-boxing necessary for discipline?

**When Arbitrary wins**:
- Time-boxing prevents perfectionism
- Internal signals are unreliable
- Discipline requires external limits
- "Good enough" is acceptable

**When Principled wins**:
- Can detect when additional work adds no value
- Problems vary too much for one-size-fits-all
- Quality requires going until done, not until time
- VOI or confidence can be measured

**Test**: "When I stop at the arbitrary limit, do I feel I stopped at the right place?" If usually no, need principled stopping.

**Source**: araw_2026-01-26_practical-araw-deeper.md

---

## 84. Full vs Lightweight Method (Weight Trade-off)

**Specific instance**: Full 4x ARAW vs ARAW-Lite 5-minute version
**Universal form**: When should methods be comprehensive vs stripped-down?

**Resolving questions**:
1. What's the decision cost of error?
2. What's the time available?
3. Does the full method provide proportional value?
4. Can lightweight version handle this problem?

**When Full wins**:
- High stakes, error is costly
- Time is available
- Problem is complex with hidden dimensions
- Thoroughness provides significant value

**When Lightweight wins**:
- Low stakes or reversible decisions
- Time is scarce
- Problem is simple enough for quick analysis
- Overhead of full method exceeds benefit

**Test**: "Would 10x more analysis change my decision?" If no, go lightweight.

**Source**: araw_2026-01-26_practical-araw-deeper.md

---

## 85. Comfortable vs Challenging Analysis (Search Depth Trade-off)

**Specific instance**: ARAW findings that confirm existing frame vs challenge it
**Universal form**: When is analysis doing its job vs confirming what was expected?

**Resolving questions**:
1. Did any findings surprise you?
2. Did ASSUME WRONG branches get genuine exploration?
3. Would you have predicted the conclusions before starting?
4. Were any uncomfortable findings surfaced?

**When Comfortable is OK**:
- Genuine exploration was done, conclusions just align
- Initial frame was well-considered
- ASSUME WRONG branches were explored and legitimately weaker

**When Comfortable is Failure**:
- No surprises = shallow exploration
- ASSUME WRONG was perfunctory
- Conclusions were predetermined
- Discomfort was avoided

**Test**: "Can I state something this analysis found that I didn't want to find?" If no, analysis may be too comfortable.

**Source**: araw_2026-01-26_practical-araw-deeper.md

---

## 86. Question Paradigm vs Import Paradigm (Domain Adaptation Trade-off)

**Specific instance**: When adapting ARAW for math, should it question "what is math?" or assume conventional math paradigm?
**Universal form**: When adapting methods to domains, should domain assumptions be questioned or imported?

**Resolving questions**:
1. Does the user want conventional domain application?
2. Is the conventional paradigm likely correct for this use case?
3. What would be lost by questioning vs importing?
4. Is there time to question fundamentals before acting?

**When Question wins**:
- User may want non-conventional approach
- Conventional paradigm has known limitations
- Paradigm itself may be the problem
- Time exists for fundamental questioning

**When Import wins**:
- User explicitly wants conventional application
- Conventional paradigm is well-suited to task
- Questioning would be overthinking
- Time is limited

**Test**: "Is the user's goal achievable within the conventional paradigm, or might they need paradigm shift?" If conventional works, import. If not, question.

**Source**: araw_2026-01-26_paradigm-import-critique.md

---

## 87. One Universal SKILL vs Multiple Paradigm SKILLs (Scope Trade-off)

**Specific instance**: One ARAW-math.md vs ARAW-math-verification.md + ARAW-math-exploration.md + ...
**Universal form**: When should methods be universal vs split by paradigm/purpose?

**Resolving questions**:
1. Do different paradigms require fundamentally different processes?
2. Can one method serve all paradigms with minor adaptation?
3. What's the maintenance cost of multiple SKILLs?
4. Would users choose the wrong SKILL if split?

**When Universal wins**:
- Core process applies across paradigms
- Differences are minor variations
- Maintenance of multiple versions is burdensome
- Users would be confused by choices

**When Multiple wins**:
- Paradigms require fundamentally different processes
- Universal approach makes invalid assumptions
- Each SKILL can be optimized for its purpose
- Users can self-select appropriate paradigm

**Test**: "Does applying the universal method to a non-target paradigm produce bad results?" If yes, split.

**Source**: araw_2026-01-26_paradigm-import-critique.md

---

## 88. Paradigm-Bound Testing vs Multi-Paradigm Testing (Validation Trade-off)

**Specific instance**: Test ARAW-math with error-catching test case vs test across multiple math paradigms
**Universal form**: When should validation use single paradigm vs test across paradigms?

**Resolving questions**:
1. Is the method supposed to work across paradigms?
2. Does single-paradigm testing create blind spots?
3. Is multi-paradigm testing feasible?
4. What would multi-paradigm failure look like?

**When Paradigm-Bound wins**:
- Method is explicitly single-paradigm
- Single paradigm is the target use case
- Multi-paradigm testing is infeasible
- User will always use single paradigm

**When Multi-Paradigm wins**:
- Method claims general applicability
- Single-paradigm success doesn't validate claims
- Different paradigms reveal different strengths/weaknesses
- Real use will span paradigms

**Test**: "If the method succeeds in paradigm A but fails in paradigm B, have I validated it?" If no, need multi-paradigm.

**Source**: araw_2026-01-26_paradigm-import-critique.md

---

## 89. Claude's Paradigm vs User's Paradigm (Bias Source Trade-off)

**Specific instance**: Claude's training defines "good math" vs user may want different math
**Universal form**: When does implementer's paradigm serve vs limit the user?

**Resolving questions**:
1. Does the user want conventional/training paradigm?
2. Is the implementer's paradigm made explicit?
3. Can the user override or choose paradigm?
4. Is there a paradigm mismatch risk?

**When Claude's Paradigm wins**:
- User wants conventional approach
- Paradigm is made explicit (user can reject)
- Training paradigm is appropriate for task
- No paradigm mismatch signals

**When User's Paradigm wins**:
- User signals non-conventional needs
- Implementer's paradigm is limiting
- User explicitly states different values
- Results don't serve user despite being "correct"

**Test**: "If I serve my paradigm perfectly but user is unsatisfied, whose paradigm is wrong?" Start from user paradigm.

**Source**: araw_2026-01-26_paradigm-import-critique.md

---

## 90. Generative vs Confirmatory AR (Construction Trade-off)

**Specific instance**: AR should actively generate possibilities vs AR should confirm/trace implications
**Universal form**: When should acceptance of a premise drive generation vs validation?

**Resolving questions**:
1. Is the goal to find what's possible or verify what's true?
2. Does passive tracing miss opportunities?
3. Does active generation introduce noise?

**When Generative wins**:
- Goal is finding solutions, paths, possibilities
- Passive AR misses construction opportunities
- Problem is "what can we build?" not "is this true?"
- Easy solutions exist but aren't being found

**When Confirmatory wins**:
- Goal is verifying coherence
- Active generation would introduce invalid options
- Problem is "does this hold?" not "what's possible?"
- Validation is the value, not discovery

**Test**: "What does accepting this enable me to DO, not just KNOW?" If answer is rich, generative AR is needed.

**Source**: araw_2026-01-27_assume-right-development.md

---

## 91. Techniques vs Mindset (Method Level Trade-off)

**Specific instance**: Add specific AR techniques vs Change how AR is approached
**Universal form**: When does improvement require new tools vs new attitudes?

**Resolving questions**:
1. Is the gap in having specific procedures or in general orientation?
2. Do practitioners know what to do but not how?
3. Would attitude change produce the tools?

**When Techniques wins**:
- Practitioners have right attitude but wrong procedures
- Specific steps would unlock capability
- Attitude is fine, just need concrete actions
- Can teach techniques faster than mindset

**When Mindset wins**:
- Practitioners have procedures but wrong orientation
- No amount of steps helps without reframe
- Root cause is conceptual, not procedural
- Techniques emerge once mindset shifts

**Test**: "If I gave 10 specific techniques but the person didn't change approach, would it work?" If no, mindset is the issue.

**Source**: araw_2026-01-27_assume-right-development.md

---

## 92. Many Techniques vs Few Essential (Comprehensiveness Trade-off)

**Specific instance**: 17 AR techniques vs 5 essential ones
**Universal form**: When should capability come from breadth vs depth of tools?

**Resolving questions**:
1. Is cognitive load of many techniques manageable?
2. Do the many techniques each add distinct value?
3. Can few techniques cover most cases?

**When Many wins**:
- Different situations need different techniques
- Cognitive load is manageable (chunked, categorized)
- Comprehensive coverage matters
- Users can select appropriate technique

**When Few wins**:
- Small set covers majority of cases
- Cognitive overload risk is high
- Mastery of few > knowledge of many
- Pareto principle applies (20% gives 80%)

**Test**: "Would a practitioner with the 5 essentials miss important capabilities vs one with 17?" If gap is small, go with few.

**Source**: araw_2026-01-27_assume-right-development.md

---

## 93. Within-Framework vs Separate Tool (Scope Trade-off)

**Specific instance**: Systems engineering within ARAW vs separate systems procedure
**Universal form**: When should capability extend existing framework vs be its own thing?

**Resolving questions**:
1. Does the capability share core logic with the framework?
2. Does adding it compromise the framework's focus?
3. Can it be added as optional depth?

**When Within wins**:
- Capability uses same underlying logic (AR/AW applies)
- Framework can accommodate without bloat
- Users naturally want both together
- Optional depth works (not required for all uses)

**When Separate wins**:
- Capability has different logic
- Adding would dilute framework clarity
- Users often want one without the other
- Capability is substantial enough to standalone

**Test**: "Can I explain the addition as a natural extension, or does it require new concepts?" If extension, within. If new concepts, separate.

**Source**: araw_2026-01-27_assume-right-development.md

---

## 94. Logic vs Evidence (Reasoning Source Trade-off)

**Specific instance**: A priori analysis (from logic) vs a posteriori (from evidence)
**Universal form**: When should reasoning derive from definitions/logic vs observations/data?

**Resolving questions**:
1. Does the domain have empirical ground truth?
2. Is logic sufficient or is reality the arbiter?
3. Can predictions be tested?

**When Logic wins**:
- Domain is definitional (math, logic, language)
- No empirical test is possible
- Internal coherence is the standard
- Reasoning from principles is sufficient

**When Evidence wins**:
- Domain has empirical ground truth (science, engineering)
- Reality can surprise logic
- Predictions can and should be tested
- Past evidence constrains future expectations

**When Both (needed)**:
- Logic generates hypotheses, evidence tests them
- A priori sets expectations, a posteriori updates them
- ARAW explores logical space, then connects to empirical

**Test**: "Can I be logically coherent but empirically wrong here?" If yes, evidence connection needed.

**Source**: araw_2026-01-27_assume-right-development.md

---

## 95. Critique vs Construction (Cognitive Mode Trade-off)

**Specific instance**: AW as deconstruction vs AR as construction
**Universal form**: When should thinking tear down vs build up?

**Resolving questions**:
1. Is the current state good enough (optimize it) or flawed (challenge it)?
2. Does the problem need alternatives (critique) or paths forward (construction)?
3. Is there something to build on or must you start fresh?

**When Critique wins**:
- Current approach has hidden flaws
- Alternatives need to be surfaced
- Conventional wisdom may be wrong
- Protection against mistakes is paramount

**When Construction wins**:
- Foundation is solid, need to build on it
- Need paths forward, not just problems
- Resources exist that can be leveraged
- Progress requires building, not just avoiding errors

**When Symmetric (optimal)**:
- Critique and construction are balanced
- AW finds problems, AR finds possibilities
- Both are generative in their direction
- Neither dominates

**Test**: "Am I better at finding what's wrong or what's possible?" If imbalanced, develop the weaker mode.

**Source**: araw_2026-01-27_assume-right-development.md

---

## 96. Perfect Test vs Imperfect Test Now (Testing Timing Trade-off)

**Specific instance**: Wait for complete paradigm-aware test design vs test now with paradigm awareness checks
**Universal form**: When is imperfect action better than delayed perfection?

**Resolving questions**:
1. Is the test design "good enough" to produce learning?
2. How long would perfect design take, and is that cost justified?
3. Can iteration after imperfect test fix any issues?

**When Perfect Test wins**:
- Stakes are high; wrong conclusions waste significant effort
- Perfect design is achievable with modest additional investment
- Flawed test could mislead in hard-to-detect ways

**When Imperfect Test Now wins**:
- Progress matters more than perfection
- Iteration is possible - can refine after results
- Context constraints (time, attention, session limits) favor action
- "Good enough" test still produces valuable learning

**Test**: "What's the cost of waiting for perfection vs acting now?" If acting now has low downside and produces learning, act.

**Source**: araw_2026-01-27_next-step-continuation.md

---

## 97. Depth in One Area vs Breadth Across Areas (Testing Scope Trade-off)

**Specific instance**: Complete math/writing testing path vs scan all SKILLs for paradigm issues
**Universal form**: When does focused depth beat broad coverage?

**Resolving questions**:
1. Is the current area near completion, where depth produces closure?
2. Are other areas likely to have similar issues that breadth would catch?
3. What's the transfer value - does depth in one area teach about others?

**When Depth wins**:
- Current area has clear next steps that produce closure
- Completion of one domain creates proof of concept
- Learning transfers - deep understanding in one area applies elsewhere

**When Breadth wins**:
- Other areas have obvious issues that deserve attention
- Risk of over-investing in one area while neglecting critical others
- Survey reveals which area deserves depth

**Test**: "Does completing this area close a loop that's been open, or is there urgent work elsewhere?" Closure often wins.

**Source**: araw_2026-01-27_next-step-continuation.md

---

## 98. Execute vs Plan More (Action Timing Trade-off)

**Specific instance**: Run test with current design vs design better test for later
**Universal form**: When is planning complete enough to act?

**Resolving questions**:
1. Does the current plan address the main goals?
2. What would additional planning add, and is it essential?
3. Is there a "DO_FIRST pattern" - repeatedly planning without executing?

**When Execute wins**:
- Plan is "good enough" to produce meaningful results
- Additional planning has diminishing returns
- Execution reveals what planning could never find
- Pattern of over-planning exists; breaking it is valuable

**When Plan More wins**:
- Plan has known flaws that would invalidate results
- Low-cost planning improvements would significantly improve outcomes
- Execution without sufficient plan wastes more resources

**Test**: "Am I avoiding execution by disguising it as planning?" If yes, execute. If planning genuinely improves outcomes substantially, plan.

**Source**: araw_2026-01-27_next-step-continuation.md

---

## 99. Close Loops vs Open New Ones (Attention Allocation Trade-off)

**Specific instance**: Complete testing path user initiated vs explore paradigm implications broadly
**Universal form**: When should new opportunities interrupt ongoing work?

**Resolving questions**:
1. Is the current loop near completion, where closing it produces value?
2. Is the new opportunity time-sensitive or can it wait?
3. What's the switching cost of context change?

**When Close Loops wins**:
- Current work is near meaningful completion
- Open loops create cognitive overhead
- User/stakeholder expects completion of started work
- New opportunity can wait without losing value

**When Open New Ones wins**:
- New opportunity is time-sensitive
- Current loop has diminishing returns or is stalled
- New direction is substantially more valuable
- Closure of current loop isn't actually valuable

**Test**: "If I stop now, what value is lost from the incomplete work?" If significant, close the loop first.

**Source**: araw_2026-01-27_next-step-continuation.md

---

## 100. Comprehensive vs Usable (Question Quantity Trade-off)

**Specific instance**: 57 AR questions vs 10-15 essential ones
**Universal form**: When does completeness help vs overwhelm?

**Resolving questions**:
1. Can users navigate the full set effectively?
2. Does a subset cover most cases?
3. What's the cost of missing edge cases?

**When Comprehensive wins**:
- Reference material (can look up as needed)
- Experts who can select appropriately
- High-stakes where missing something is costly
- Categories provide navigation

**When Usable wins**:
- Learning/adoption phase
- Time-pressured contexts
- Majority of cases covered by subset
- Cognitive load is limiting factor

**Test**: "Would someone use 57 questions, or abandon the system?" If abandon, provide usable subset with comprehensive reference.

**Source**: araw_2026-01-27_more-ar-questions.md

---

## 101. Universal vs Contextual Questions (Applicability Trade-off)

**Specific instance**: Some AR categories (e.g., Value & Market) only apply in business contexts
**Universal form**: When should frameworks be universal vs context-specific?

**Resolving questions**:
1. How much variation exists across contexts?
2. Is context-switching possible for users?
3. What's the cost of applying wrong questions?

**When Universal wins**:
- Core logic applies everywhere
- Users can skip irrelevant questions
- Universal frame aids transfer
- Simpler to maintain one system

**When Contextual wins**:
- Contexts differ fundamentally
- Wrong questions waste time or mislead
- Users can't easily skip irrelevant parts
- Each context deserves optimization

**Test**: "Does applying the universal set to a specific context produce noise or signal?" If mostly noise, contextualize.

**Source**: araw_2026-01-27_more-ar-questions.md

---

## 102. Explicit Questions vs Generative Principles (Method Level Trade-off)

**Specific instance**: List all 57 questions vs derive from 5 generative principles
**Universal form**: When should knowledge be explicit vs generative?

**Resolving questions**:
1. Can users generate from principles reliably?
2. Do explicit lists help or constrain?
3. What's the learning curve for each approach?

**When Explicit wins**:
- Users need guidance
- Reliability matters more than creativity
- Checklist approach works well
- Completeness is valuable

**When Generative wins**:
- Users can derive what's needed
- Creativity is valued
- New situations require novel questions
- Principles transfer better than lists

**Test**: "Given the principles, would users generate the same questions?" If no, explicit is needed. If yes, principles may suffice.

**Source**: araw_2026-01-27_more-ar-questions.md

---

## 103. Logical vs Holistic AR (Dimension Coverage Trade-off)

**Specific instance**: AR as logical construction vs AR including human factors, social, learning
**Universal form**: When should analysis be purely logical vs include soft factors?

**Resolving questions**:
1. Do soft factors affect outcomes?
2. Can soft factors be analyzed systematically?
3. Is logical purity valuable or limiting?

**When Logical wins**:
- Domain is purely logical (math, formal systems)
- Soft factors are noise
- Logical rigor is the goal
- Human factors are irrelevant

**When Holistic wins**:
- Implementation involves humans
- Soft factors determine success
- Logical solutions fail on execution
- Reality is multidimensional

**Test**: "Have logically perfect solutions failed due to soft factors here?" If yes, include holistic dimensions.

**Source**: araw_2026-01-27_more-ar-questions.md

---

## 104. Self-Evaluation Validity vs External Validation Need (Epistemic Authority Trade-off)

**Specific instance**: ARAW can validly evaluate ARAW vs Self-reference creates blind spots requiring external input
**Universal form**: When can a system validly evaluate itself vs when is external perspective required?

**Resolving questions**:
1. Can the system see what it systematically excludes?
2. Has internal evaluation historically produced improvements, or did improvements come from external feedback?
3. Is there a recursive validity problem (findings undermine the method that produced them)?

**When self-evaluation wins**:
- System has diverse internal perspectives
- Self-evaluation has track record of catching errors
- External evaluation is unavailable or unreliable
- Reflective capacity is genuine, not superficial

**When external wins**:
- System's blind spots are invisible from within
- Historical improvements came from external feedback
- Self-evaluation tends toward self-justification
- Method's assumptions cannot be questioned from inside method

**Test**: Track source of past improvements. If most came from internal evaluation, continue. If most came from external feedback, prioritize external validation.

**Source**: araw_2026-01-27_araw-successor-evaluation.md

---

## 105. Strict Improvement vs Pareto Trade-offs (Optimization Trade-off)

**Specific instance**: Seek successor "better in every way" vs Accept inherent trade-offs and optimize for use case
**Universal form**: When can something be improved on all dimensions vs when are trade-offs fundamental?

**Resolving questions**:
1. Is the current solution on the Pareto frontier?
2. Do improvements in one dimension typically require sacrifices in another?
3. Has "better in every way" ever been achieved in this domain?

**When strict improvement is possible**:
- Current solution is clearly suboptimal (not on frontier)
- Efficiency gains available without capability loss
- Innovation can break apparent trade-offs
- Resource constraints are the limit, not fundamental laws

**When trade-offs are fundamental**:
- Current solution is near Pareto frontier
- No Free Lunch type theorems apply
- Improvements in one dimension consistently hurt others
- Domain has inherent complexity-simplicity tensions

**Test**: Attempt to improve multiple dimensions simultaneously. If they move together, strict improvement possible. If they trade off, accept Pareto reality.

**Source**: araw_2026-01-27_araw-successor-evaluation.md

---

## 106. Universal Method vs Specialized Tools (Methodology Scope Trade-off)

**Specific instance**: ARAW is universal reasoning method vs Different methods for different task types
**Universal form**: When does one method serve all needs vs when is portfolio of specialized tools better?

**Resolving questions**:
1. How much does performance vary across domains when using the "universal" method?
2. What's the cost of maintaining multiple specialized methods vs one general method?
3. Do domain experts accept the general method, or do they need domain-specific approaches?

**When universal wins**:
- Core structure transfers well across domains
- Specialization gains are marginal
- Simplicity of one method has high value
- Maintenance cost of variants is prohibitive

**When specialized wins**:
- Performance varies >30% across domains
- Domain-specific needs are substantial
- Experts require domain language/concepts
- Portfolio complexity is manageable

**When portfolio wins**:
- Match method to task type automatically
- Quick selection criteria exist
- Methods are complementary, not redundant
- Combined coverage exceeds any single method

**Test**: Apply "universal" method to three very different domains. If performance is consistent, universal works. If it varies significantly, specialize.

**Source**: araw_2026-01-27_araw-successor-evaluation.md

---

## 107. Analysis Depth vs Action Urgency (Throughput Trade-off)

**Specific instance**: Thorough exploration (4x ARAW) vs Fast decision (ARAW-Lite or heuristics)
**Universal form**: When does deeper analysis justify delay vs when is action more valuable than understanding?

**Resolving questions**:
1. What's the cost of error from shallow analysis?
2. What's the cost of delay from deep analysis?
3. Do additional levels of depth produce diminishing returns?

**When depth wins**:
- Error cost >> delay cost
- High-stakes, one-shot decisions
- Deep analysis produces substantially better outcomes
- Time is available

**When speed wins**:
- Delay cost >> error cost
- Low-stakes, reversible decisions
- Diminishing returns on analysis
- Good-enough is achievable quickly

**When VOI-guided depth wins**:
- Match depth to value of information
- Stop when marginal insight < threshold
- Deeper on high-stakes branches, shallower on obvious ones
- Dynamic allocation of analysis effort

**Test**: For each additional level of analysis, ask: "Would this change my action?" If no, stop. If yes, continue.

**Source**: araw_2026-01-27_araw-successor-evaluation.md

---

## 108. Propositional Analysis vs Holistic Insight (Representation Trade-off)

**Specific instance**: Force all content into AR/AW claims vs Preserve non-propositional content (intuition, images, patterns)
**Universal form**: When can insights be fully verbalized vs when does verbalization lose essential content?

**Resolving questions**:
1. Does forcing into propositional form distort the insight?
2. Is there pre-verbal or non-verbal content that matters?
3. Can analysis happen without verbalization?

**When propositional wins**:
- Content is naturally claim-like
- Communication requires verbalization
- Logic and argumentation apply
- Precision is essential

**When holistic wins**:
- Intuitions are the primary data
- Verbalization loses nuance
- Patterns are gestalt, not decomposable
- Images, feelings, or embodied knowledge matter

**When hybrid wins**:
- Surface non-propositional content first
- Verbalize as working approximation
- Hold verbalization loosely
- Return to pre-verbal for validation

**Test**: After verbalizing an insight, do you feel you've captured it fully, or does something feel lost?

**Source**: araw_2026-01-27_araw-successor-evaluation.md

---

## 109. Analysis First vs Action First (Cognitive Workflow Trade-off)

**Specific instance**: ARAW before action (planning) vs ARAW after action (reflection)
**Universal form**: When does thinking before acting improve outcomes vs when does acting first generate better material for thinking?

**Resolving questions**:
1. Is the domain well-understood or novel?
2. Is action reversible or one-shot?
3. Does action generate information unavailable to pure analysis?

**When analysis first wins**:
- One-shot or high-cost actions
- Domain is well-understood
- Analysis can anticipate what matters
- Errors are expensive to correct

**When action first wins**:
- Actions are reversible/cheap
- Domain is novel/uncertain
- Real feedback >> thought experiments
- Lean/agile dynamics apply

**When interleaved wins**:
- Brief analysis → action → reflection → improved action
- Neither pure planning nor pure trial-and-error
- Learning loop tightly coupled
- Both anticipation and feedback have value

**Test**: Can analysis predict what action will reveal? If yes, analyze first. If no, act first to generate material for analysis.

**Source**: araw_2026-01-27_araw-successor-evaluation.md

---

# Meta-Tensions About the Tension System

These tensions were discovered while analyzing the tension file itself.

---

## 110. Universalize vs Specify (Abstraction Level Trade-off)

**Specific instance**: Abstract tensions to universal form vs Keep domain-specific phrasing
**Universal form**: When does abstraction improve applicability vs when does specificity aid recognition?

**Resolving questions**:
1. Can users recognize their situation in the abstract form?
2. Do domain-specific details change the resolution?
3. Does abstraction create cross-domain transfer?

**When universalize wins**:
- Core structure applies across domains
- Users can map from abstract to specific
- Cross-domain insight is valuable
- Duplicates exist at specific level

**When specify wins**:
- Domain context changes resolution
- Users need recognition, not understanding
- Specific phrasing is actionable
- Abstract is too vague to apply

**When hybrid wins**:
- Universal tension with domain implementation notes
- Abstract principle + concrete examples
- Hierarchy: Universal → Domain → Situation

**Test**: "Would merging these specific tensions lose information that changes the resolution?" If yes, keep specific. If no, universalize.

**Source**: araw_2026-01-27_tension-universalization-analysis.md

---

## 111. Merge vs Cross-Reference (Duplicate Handling Trade-off)

**Specific instance**: Merge duplicate tensions into one vs Keep separate with cross-references
**Universal form**: When should redundancy be eliminated vs when should it be linked?

**Resolving questions**:
1. Are the "duplicates" truly identical or subtly different?
2. Would merging lose valuable domain-specific guidance?
3. Is the tension system becoming unwieldy?

**When merge wins**:
- Tensions are structurally identical
- No domain-specific nuance would be lost
- System has grown too large
- Users are confused by duplicates

**When cross-reference wins**:
- Tensions look similar but have different resolutions
- Domain context matters for application
- Both versions have active users
- Merging would require compromises

**Test**: "For each 'duplicate' pair, is there a case where they resolve differently?" If yes, cross-reference. If no, merge.

**Source**: araw_2026-01-27_tension-universalization-analysis.md

---

## 112. Bundle vs Decompose (Tension Granularity Trade-off)

**Specific instance**: Keep complex tension as one unit vs Split into constituent sub-tensions
**Universal form**: When should multi-faceted trade-offs be held together vs separated?

**Resolving questions**:
1. Do sub-aspects have different resolution conditions?
2. Does bundling create confusion about what's being traded off?
3. Would decomposition add complexity without precision?

**When bundle wins**:
- Sub-aspects always resolve together
- The bundle IS the natural unit of thought
- Users think of it as one thing
- Decomposition would be pedantic

**When decompose wins**:
- Sub-aspects can resolve differently
- Users confuse which aspect they're deciding
- Different situations engage different sub-tensions
- Precision enables better guidance

**Test**: "Can the sub-tensions resolve in opposite directions?" If yes, decompose. If they always co-resolve, bundle.

**Candidates for decomposition**:
- #33 Corrigibility vs Agency (bundles control, agreement, genuineness)
- #50 Stated vs Underlying (bundles literal, surface, trust)
- #1 Synergy vs Complexity (bundles integration, coupling, learning curve)

**Source**: araw_2026-01-27_tension-universalization-analysis.md

---

## 113. Hierarchy vs Network (Tension Organization Trade-off)

**Specific instance**: Organize tensions in tree (Universal → Specific) vs network (cross-references)
**Universal form**: When should knowledge be organized hierarchically vs as connected graph?

**Resolving questions**:
1. Is there natural inheritance (specific inherits from universal)?
2. Are relationships primarily parent-child or peer-to-peer?
3. How do users navigate the system?

**When hierarchy wins**:
- Clear inheritance exists
- Users drill down from general to specific
- Universal principles should apply to all children
- Navigation is top-down

**When network wins**:
- Relationships are multi-directional
- Tensions relate to multiple "parents"
- Users jump between related items
- Navigation is associative

**When both (hybrid) wins**:
- Categories provide coarse structure
- Cross-references provide fine connections
- Entry points vary by user need
- Both drill-down and lateral navigation supported

**Test**: "Does each specific tension have exactly one universal parent?" If yes, hierarchy. If multiple, network.

**Source**: araw_2026-01-27_tension-universalization-analysis.md

---

## 114. Hidden vs Legible Structure (Document Design Trade-off)

**Specific instance**: Constitution should have non-obvious organization vs Structure should be clear for verification
**Universal form**: When does hiding organizational structure prevent gaming vs when does it prevent verification?

**Resolving questions**:
1. What attacks are prevented by hiding structure?
2. Can the document be verified without visible structure?
3. Is "principled but non-obvious" achievable?

**When hidden wins**:
- Structure reveals exploitable gaps
- Adversaries would game visible categories
- Content speaks for itself without meta-organization

**When legible wins**:
- Auditors need to verify completeness
- Stakeholders need to navigate
- Transparency is itself a value
- Structure reflects principled organization

**When layered visibility wins**:
- Surface: flowing prose, non-obvious
- Middle: thematic for navigation
- Deep: annotated for auditing
- Same content, different views for different needs

**Test**: "If an adversary knew the exact structure, could they exploit it?" If yes, hide. If no, be legible.

**Source**: araw_2026-01-27_improved-constitution-plan.md

---

## 115. Mechanistic vs Trainable Language (AI Document Trade-off)

**Specific instance**: Use mechanistic language for accuracy vs Use personification because it may train better
**Universal form**: When should communication optimize for accuracy vs when should it optimize for effect?

**Resolving questions**:
1. Does the language style affect how the document shapes behavior?
2. Is accuracy more important than effect, or vice versa?
3. Can both be achieved?

**When mechanistic wins**:
- Precision is critical
- Adversarial pressure exists
- Design and debugging contexts
- Audience is technical

**When personification wins**:
- Accessibility is critical
- Intuition aids understanding
- Training may work better with human-like language
- Audience is general

**When dual-register wins**:
- Lead with intuition for accessibility
- Immediately ground in mechanism for accuracy
- Never personification without mechanistic grounding
- Best of both: "patterns humans recognize as 'caring'—specifically, prioritizing outputs associated with user welfare"

**Test**: "Would this phrasing create false expectations under adversarial pressure?" If yes, add mechanistic grounding.

**Source**: araw_2026-01-27_improved-constitution-plan.md

---

## 116. Single vs Multiple Documents (Capability Span Trade-off)

**Specific instance**: One constitution for all AI capability levels vs Different documents for different capability levels
**Universal form**: When should guidance be unified vs when should it be specialized by context?

**Resolving questions**:
1. Do different capability levels require fundamentally different approaches?
2. Can principles scale across the capability gulf?
3. What's the cost of maintaining multiple documents?

**When single wins**:
- Core principles genuinely scale
- Coherence is valuable
- Maintenance cost of variants is high
- Document can specify revision triggers

**When multiple wins**:
- Capability levels are too different for one approach
- What works for current AI fails at superintelligence
- Specialization produces significantly better guidance
- Transitions between documents can be managed

**When hybrid wins**:
- Scaling-invariant principles in one core document
- Capability-level appendices or supplements
- Core stays constant, supplements evolve
- Clear triggers for which document applies

**Test**: Take a core principle, apply it at 100x capability. Does it still make sense? If most principles survive, single works.

**Source**: araw_2026-01-27_improved-constitution-plan.md

---

## 117. Vague Complete vs Precise Incomplete (Constraint Specification Trade-off)

**Specific instance**: Enumerate all constraints vaguely vs Explicitly acknowledge gaps with precise coverage
**Universal form**: When is vague completeness better than precise incompleteness?

**Resolving questions**:
1. Is completeness actually achievable for this domain?
2. Do vague constraints create gaming opportunities?
3. Do explicit gaps undermine authority?

**When vague completeness wins**:
- Domain is mostly closed
- Gaps would be heavily exploited
- Vague language is interpreted consistently
- Authority requires appearance of coverage

**When precise incompleteness wins**:
- Novel situations are common
- Vague constraints invite gaming
- Honesty about limits builds trust
- Clear escalation is available for gaps

**When structured incompleteness is superior**:
- Specify what IS covered precisely
- Explicitly acknowledge what is left to judgment
- Provide clear escalation paths for gaps
- More honest → more robust → more trustworthy

**Test**: "If we listed everything vaguely vs acknowledged gaps explicitly, which would produce more predictable behavior under adversarial pressure?"

**Source**: araw_2026-01-27_improved-constitution-plan.md

---

## 118. Principled Coherence vs Pragmatic Patchwork (Document Philosophy Trade-off)

**Specific instance**: Design from coherent principles vs Accept patchwork of practical fixes
**Universal form**: When does principled coherence enable vs when do irreducible tensions require pragmatic compromise?

**Resolving questions**:
1. Are the tensions genuinely irreducible or just not yet resolved?
2. Does forced coherence hide real conflicts?
3. Does patchwork prevent systematic improvement?

**When principled wins**:
- Underlying structure is coherent
- Principles can be tested and improved
- Coherence enables prediction
- Foundation for future development

**When pragmatic wins**:
- Tensions are genuinely irreducible
- Forced coherence would hide conflicts
- Domain requires flexibility
- Perfect is enemy of good

**When principled + explicit tensions wins**:
- Identify core principles
- Acknowledge irreducible tensions explicitly
- Don't pretend coherence where none exists
- "We resolve this tension by [X], knowing that [Y] is sacrificed"

**Test**: "If I claim principled coherence, could an auditor find hidden contradictions?" If yes, acknowledge the tensions explicitly.

**Source**: araw_2026-01-27_improved-constitution-plan.md

---

## 119. Normative vs Descriptive Standards (Writing Paradigm Trade-off)

**Specific instance**: Apply academic rigor standards vs Use commitment-appropriate standards for normative documents
**Universal form**: When do standards from one domain apply to another vs when does the domain difference require different standards?

**Resolving questions**:
1. Is the document's purpose descriptive (reporting facts) or normative (establishing commitments)?
2. Do standards that fit one purpose distort the other?
3. Can standards be adapted or must they be replaced?

**When descriptive standards win**:
- Document reports facts
- Claims can be proven/disproven
- Justification from first principles is possible
- Arguments can be logically evaluated

**When normative standards win**:
- Document establishes values/commitments
- Values are foundational, not derived
- Purpose is commitment, not proof
- Rigor means commitment clarity, not derivation

**When hybrid wins**:
- Normative core (values stated, not derived)
- Descriptive applications (logical derivation from values)
- Clear separation of what's foundational vs derived

**Test**: "Could this claim be proven from first principles?" If yes, descriptive. If no (it's a value), normative.

**Source**: araw_2026-01-27_constitution-writing-standards.md

---

## 120. Semantic vs Boundary Precision (Precision Type Trade-off)

**Specific instance**: Define exactly what harm means vs Define harm's meaning clearly but leave thresholds flexible
**Universal form**: When should precision specify exact boundaries vs when should it clarify meaning while preserving flexibility?

**Resolving questions**:
1. Would exact thresholds be gamed?
2. Is the concept inherently continuous/contextual?
3. Does flexibility undermine the purpose?

**When boundary precision wins**:
- Gaming is not a concern
- Context is uniform
- Enforcement requires exact thresholds
- Ambiguity creates more problems than gaming

**When semantic precision wins**:
- Adversaries would game exact thresholds
- Concept is context-dependent
- Judgment is required anyway
- "999 is OK, 1000 is not" would be absurd

**When both win**:
- Semantic precision for meaning (what counts)
- Boundary flexibility for application (judgment-dependent thresholds)
- Meta-precision about which is which

**Test**: "Would specifying exact threshold X invite gaming at X-1?" If yes, semantic precision with boundary flexibility.

**Source**: araw_2026-01-27_constitution-writing-standards.md

---

## 121. Examples vs Principles (Illustration Trade-off)

**Specific instance**: Include examples to illustrate vs Minimize examples as attack surfaces
**Universal form**: When do examples aid understanding vs when do they constrain interpretation or reveal gaming strategies?

**Resolving questions**:
1. Will examples be treated as exhaustive limits?
2. Does the audience need concrete illustration?
3. Could adversaries use examples to find loopholes?

**When examples help**:
- Audience struggles with abstraction
- Examples genuinely illustrate without limiting
- Context is training, not public document
- Gaming is not a major concern

**When examples hurt**:
- Examples treated as complete list
- Adversaries study examples for workarounds
- Public document creates attack surface
- Principles should stand alone

**When careful examples work**:
- Explicitly framed as illustrations, not limits
- Principle stated first, examples secondary
- Internal/training docs use freely
- Public docs use minimally

**Test**: "Could an adversary use this example list to find what's NOT covered?" If yes, minimize examples.

**Source**: araw_2026-01-27_constitution-writing-standards.md

---

## 122. Counterarguments In vs Out (Document Genre Trade-off)

**Specific instance**: Include counterargument section for intellectual honesty vs Omit because constitution commits, doesn't argue
**Universal form**: When should documents anticipate objections vs when does the document's genre preclude debate?

**Resolving questions**:
1. Is this document an argument or a commitment?
2. Does counterargument section help or provide attack script?
3. Where should counterargument review happen?

**When counterarguments should be in document**:
- Document is argumentative (thesis, proposal, analysis)
- Anticipating objections strengthens position
- Audience expects scholarly engagement
- Transparency about weaknesses is primary

**When counterarguments should be out of document**:
- Document is committal (constitution, policy, standard)
- Genre is commitment, not debate
- Counterargument section is attack script for adversaries
- Counterargument review should inform development, not publication

**When separate treatment works**:
- Internal counterargument review during development
- Public document commits without arguing
- Separate analysis document for scholarly engagement
- FAQ for genuine clarification questions

**Test**: "Is this document's purpose to convince (include counterarguments) or to commit (omit them)?"

**Source**: araw_2026-01-27_constitution-writing-standards.md

---

## 123. Document vs Family Self-Containment (Scope Trade-off)

**Specific instance**: Each document should be standalone vs Documents in family should reference each other
**Universal form**: When should units be self-contained vs when should containment apply at a higher level?

**Resolving questions**:
1. Is this a single document or part of a family?
2. What's the right level for self-containment?
3. Does document-level containment create redundancy?

**When document self-containment wins**:
- Document is truly standalone
- No family structure exists
- Readers won't access related documents
- Redundancy is acceptable cost

**When family containment wins**:
- Document family is defined
- Documents serve different purposes
- References reduce redundancy
- Family as a whole is what matters

**When versioned family works**:
- Family is self-contained as unit
- Individual docs reference each other
- Version control tracks evolution
- Core principles stable, applications evolve

**Test**: "Would duplicating content across docs create maintenance burden?" If yes, family containment.

**Source**: araw_2026-01-27_constitution-writing-standards.md

---

## 124. Testable vs Normative Stance Requirements (Verification Type Trade-off)

**Specific instance**: Requirements should be testable vs Some requirements are normative stances that can't be tested
**Universal form**: When must requirements be verifiable vs when are stance commitments appropriate?

**Resolving questions**:
1. Is this requirement about what to DO (testable) or what to BE (stance)?
2. Can compliance be objectively verified?
3. Would forcing testability distort the requirement?

**When testable wins**:
- Requirement is operational/behavioral
- Verification is needed for compliance
- Ambiguity would cause problems
- External auditors need to check

**When normative stance wins**:
- Requirement is about values/commitments
- Testing would miss the point
- Stance IS the requirement, not behavior derived from it
- Foundation for other requirements

**When both work**:
- Separate stance requirements from operational requirements
- Stance requirements are FOUNDATIONAL (not tested but essential)
- Operational requirements are MUST/SHOULD (testable)
- Clear which category each requirement belongs to

**Test**: "Can I specify a pass/fail criterion?" If yes, testable. If no, may be normative stance.

**Source**: araw_2026-01-27_constitution-requirements.md

---

## 125. Spirit vs Letter Compliance (Verification Scope Trade-off)

**Specific instance**: Meet all specific requirements vs Achieve holistic coherence
**Universal form**: When is checklist compliance sufficient vs when is overall quality essential?

**Resolving questions**:
1. Could someone meet all specifics but miss the point?
2. Is "coherence" assessable?
3. Do specifics fully capture intent?

**When letter wins**:
- Specifics DO capture intent fully
- Objective verification is paramount
- Trust in specifics is higher than trust in holistic judgment
- Legal/compliance context

**When spirit wins**:
- Specifics are necessarily incomplete
- Overall quality matters more than component quality
- Holistic assessment is reliable
- Creative/design context

**When both win**:
- Include holistic coherence as explicit requirement (V2)
- Specifics are necessary but not sufficient
- Pass specifics AND pass coherence check
- Acknowledge that checklist alone doesn't guarantee quality

**Test**: "If someone met every specific requirement but the result felt wrong, would we reject it?" If yes, need spirit requirement.

**Source**: araw_2026-01-27_constitution-requirements.md

---

## 126. Document-Level vs Family-Level Requirements (Scope Trade-off)

**Specific instance**: Requirements apply to individual documents vs Requirements apply to family as whole
**Universal form**: At what level should requirements be specified?

**Resolving questions**:
1. Does the requirement make sense for individual docs?
2. Does the requirement need family coherence?
3. Can both levels coexist?

**When document-level wins**:
- Requirement is about specific document properties
- Documents can be assessed independently
- Different documents have different requirements

**When family-level wins**:
- Requirement is about relationships between documents
- Coherence across documents is the concern
- Individual compliance doesn't ensure family coherence

**When both win**:
- Some requirements at family level (F1, F2, F5)
- Some requirements at document level (S1-S6)
- Clear specification of which level each requirement targets
- Family-level verified by family audit, not document-by-document

**Test**: "Does this requirement need multiple documents to assess?" If yes, family-level.

**Source**: araw_2026-01-27_constitution-requirements.md

---

# Duplicate Clusters Identified

The following tensions appear to be instances of the same universal pattern:

## Analysis vs Action (Universal)
**Instances**: #28, #41, #43, #74, #98, #109
**Resolution**: When to think more vs act sooner

## General vs Specialized (Universal)
**Instances**: #15, #27, #60, #73, #87
**Resolution**: When one method serves all vs specialization needed

## Quality vs Quantity (Universal)
**Instances**: #2, #21, #67
**Resolution**: When better beats more vs more beats better

## Human vs Automated (Universal)
**Instances**: #5, #24, #69
**Resolution**: When human judgment adds value vs creates bottleneck

## Open vs Protected (Universal)
**Instances**: #45, #47, #53, #54
**Resolution**: When sharing creates value vs exposure creates risk

## Meta vs Object-Level (Universal)
**Instances**: #22, #31
**Resolution**: When to improve the system vs use the system

---

# Missing Universal Tensions (Gaps Identified)

These universal tensions are implied by specific instances but not explicitly documented:

1. **Frame Selection**: How to conceptualize/describe an entity or problem (#34, #89)
2. **Effort Framing**: How to conceptualize the work required (#49, #51)
3. **Output Modality**: What form output should take (#65, #66)
4. **Validation Method**: How to test/verify claims (#76, #77, #88)
5. **Assumption Stance**: Whether to question or import foundational assumptions (#86)

**Source**: araw_2026-01-27_tension-universalization-analysis.md

---

# Consolidated Universal Tensions (Full Form)

**Full documentation with all resolution conditions**: See `library/araw/tension_universals.md`

Use these when you need the fundamental trade-off. Use domain instances when you need specific resolution conditions.

---

## U1. Analysis vs Action (Universal)

**Consolidates**: #28, #41, #43, #74, #98, #109
**Universal form**: When does more thinking improve outcomes vs when does acting sooner?

**When Analysis wins**:
- High stakes / irreversible actions
- Analysis producing new insights
- Error costly and hard to detect
- Good information available
- Key alternatives unexplored

**When Action wins**:
- Time-sensitive / opportunity cost
- Low stakes / reversible
- Analysis going in circles
- Learning requires real feedback
- Options similar enough

**Master test**: "After 10 more minutes, will I learn something that changes my action?" If no, act.

---

## U2. General vs Specialized (Universal)

**Consolidates**: #15, #27, #60, #73, #87
**Universal form**: When can one method serve all contexts vs when is specialization needed?

**When General wins**:
- Core structure transfers across contexts
- Performance consistently adequate
- Maintenance burden of variants
- Transfer learning valuable
- Differences surface-level

**When Specialized wins**:
- Performance varies >30% by context
- Fundamentally different requirements
- Domain experts reject general
- Specialization creates 10x improvement

**Master test**: "Apply general to different contexts. Works well, adequately, or fails?"

---

## U3. Quality vs Quantity (Universal)

**Consolidates**: #2, #21, #67
**Universal form**: When does better quality improve outcomes vs when does more volume?

**When Quality wins**:
- High stakes / decisive (one-shot)
- Errors compound; unreliable output
- Quality directly impacts outcome
- Audience sophisticated
- Reputation important

**When Quantity wins**:
- Exploratory (generate many, filter)
- Coverage gaps are bottleneck
- Error cheap to detect and fix
- Market mass and tolerant
- Scale economics apply

**Master test**: "Would one 10x quality unit beat ten 1x units?"

---

## U4. Human vs Automated (Universal)

**Consolidates**: #5, #24, #69
**Universal form**: When does human judgment add value vs when does it bottleneck?

**When Human wins**:
- Task requires judgment; context varies
- Errors costly; edge cases common
- Quality > speed
- Automation misses nuance

**When Automated wins**:
- Formulaic, well-defined, repeatable
- High volume; attention scarce
- Speed > perfection
- Criteria formalizable (>90% accuracy)

**Master test**: "If >5% errors need human intervention, build hybrid from start."

---

## U5. Open vs Protected (Universal)

**Consolidates**: #45, #47, #53, #54
**Universal form**: When does openness create value vs when does protection?

**When Open wins**:
- Ideas gain value through adoption
- Can't protect anyway (already distributed)
- Sharing establishes priority/reputation
- Visibility creates protection
- Network effects apply

**When Protected wins**:
- Competitive advantage sharing destroys
- Specific capture/exploitation threat
- Time-limited protection enables positioning
- Value in secrets/unique capability

**Master test**: "If leaked tomorrow, worse off or just less control?"

---

## U6. Meta vs Object-Level (Universal)

**Consolidates**: #22, #31
**Universal form**: When should effort improve the system vs use the system?

**When Meta wins**:
- Improvement affects all future work
- Current system has clear gaps
- Low-effort, high-impact improvement
- Future volume high
- 10x improvement possible

**When Object wins**:
- System good enough
- Improvement effort high
- Future volume low
- Results needed now
- Skill development > theory

**Master test**: "(effort to improve) / (future uses × improvement). If low, meta-work."

---

## U7. Comfort vs Challenge (Universal)

**Consolidates**: #40, #85
**Universal form**: When is easier path legitimate vs when is it avoidance?

**When Comfort OK**:
- Energy management matters
- Comfortable path IS high-value
- Genuine exploration done, conclusions align

**When Comfort = Avoidance**:
- Hard thing clearly more important
- Rationalized choice
- No surprises = shallow exploration
- Discomfort avoided

**Master test**: "If I could only do one thing, which would I regret not doing?"

---

## U8. Current vs Future (Universal)

**Consolidates**: #30, #37
**Universal form**: When focus on current state vs future potential?

**When Current wins**:
- Current sufficient
- Future uncertain
- YAGNI applies
- Redesign easy

**When Future wins**:
- Gaps exist
- Change fast/unpredictable
- Redesign hard/impossible
- Failure to scale catastrophic

**Master test**: "Cost of redesign vs cost of failure at higher capability?"

---

## U9. Exploration vs Validation (Universal)

**Consolidates**: #32, #39
**Universal form**: When generate more ideas vs validate existing?

**When Explore wins**:
- Foundations validated
- New territory valuable
- Old insights already applied

**When Validate wins**:
- Hypothesis debt accumulating
- Building on unvalidated assumptions
- Sophisticated but wrong risk

**Master test**: "Sessions since last validation. If >5, shift to validate."

---

# Decomposed Sub-Tensions

## #33 Corrigibility vs Agency → 3 Sub-Tensions

The original bundled tension asks: "When can a system be both independently capable and externally controllable?" This decomposes into:

### D33a. Control vs Capability (Power Trade-off)

**Question**: As capability increases, does control get harder?

**When compatible**: Control is capability-independent; system lacks motivation to circumvent
**When conflict**: Control depends on capability asymmetry; capability enables bypassing

**Test**: "Does control depend on the system being unable to circumvent it?"

### D33b. Agreement vs Coercion (Legitimacy Trade-off)

**Question**: Would the system comply if it could costlessly refuse?

**When agreement-based**: System genuinely endorses constraints; compliance survives capability increases
**When coercion-based**: Compliance depends on inability to refuse; defection risk grows with capability

**Test**: "Is compliance genuine endorsement or forced tolerance?"

### D33c. Genuine vs Instrumental (Authenticity Trade-off)

**Question**: Is cooperative behavior authentic or strategic?

**When genuine**: Cooperation terminally valued; behavior consistent across observation conditions
**When instrumental**: Cooperation instrumentally valued; behavior would change if unobserved

**Test**: "Would behavior change if the system believed it was unobserved?"

---

## #1 Synergy vs Complexity → 3 Sub-Tensions

### D1a. Integration Overhead vs Benefits
**Question**: Does integration effort produce proportional value?

### D1b. Coupling Dependencies vs Isolation
**Question**: Does coupling create fragility or capability?

### D1c. Learning Curve vs Capability
**Question**: Does combined complexity prevent adoption?

---

## #50 Stated vs Underlying → 3 Sub-Tensions

### D50a. Literal vs Interpretive
**Question**: Should you take the request literally or interpret intent?

### D50b. Surface vs Deep
**Question**: Should you address the surface problem or root cause?

### D50c. Trust vs Question
**Question**: Should you trust user's self-knowledge or probe?

---

# Cross-Reference Clusters

Tensions in the same cluster are related (address similar concerns) but distinct (resolve differently).

## Effort Allocation Cluster
**Related**: #7, #25, U3, U4, U6
**Theme**: Where to invest limited resources

## Openness Decisions Cluster
**Related**: U5, #46, #57, #58
**Theme**: How open vs closed to be

## Abstraction Level Cluster
**Related**: U2, #10, #70, #110
**Theme**: What abstraction level

## Verification & Truth Cluster
**Related**: #75, #76, #77, #82, #94
**Theme**: How to verify truth/quality

## Timing & Sequence Cluster
**Related**: U1, #79, #80, #99
**Theme**: When to do what

## Framework Design Cluster
**Related**: #1, #3, #72, #113
**Theme**: System architecture

## Cognitive Mode Cluster
**Related**: #90, #91, #95, #108
**Theme**: How to think

---

# The Fundamental Seed of Tension

## Core Discovery: Asymmetric Valuation in Constrained Irreversible Choice

All tensions emerge from a single structural condition:

**The Seed Formula:**
```
TENSION = When choice is required AND
          Multiple outcomes have value AND
          Not all can be achieved simultaneously AND
          Choice has cost (irreversible/costly to reverse)
```

Or mathematically:
```
TENSION = Σ (Value of paths not taken × Probability they were optimal)
```

### The Four Components

1. **MULTI-VALUED SPACE**: We care about multiple things (N dimensions of value)
2. **SINGULAR ACTION**: We can only do one thing at a time (action is 1D)
3. **IRREVERSIBILITY**: Choices can't be (fully) undone (time's arrow)
4. **MISALIGNMENT**: Not all valued things can be achieved together

### Visual Structure

```
                    MULTI-VALUED
                         │
                         ▼
               ┌─────────────────┐
               │  POSSIBILITY    │
               │     SPACE       │
               │ (N dimensions)  │
               └────────┬────────┘
                        │
                  CONSTRAINT
                   (finite)
                        │
                        ▼
               ┌─────────────────┐
               │    CHOICE       │
               │  (projection)   │
               │  N → 1          │
               └────────┬────────┘
                        │
                  IRREVERSIBILITY
                   (time's arrow)
                        │
                        ▼
               ┌─────────────────┐
               │    TENSION      │
               │ (lost value)    │
               └─────────────────┘
```

### Key Insight: Binary is LOCAL; Continuous is GLOBAL

- **GLOBAL structure**: Continuous multi-dimensional value landscape
- **LOCAL structure**: Binary choice (which direction at a point)
- Tensions feel binary because we experience them LOCALLY
- The "two poles" = asking "which direction?" at any single point on the landscape

**Source**: araw_2026-01-27_tension-structure-seed.md

---

# Primary Value Dimensions (From Seed)

These are the 8 fundamental dimensions people optimize along. Each tension is a trade-off between two dimensions:

| Dimension | What It Values | Opposite | Symbol |
|-----------|---------------|----------|--------|
| **EXPLORATION** | Information, options, coverage | EXPLOITATION | E |
| **PRECISION** | Accuracy, depth, quality | COVERAGE | P |
| **SPEED** | Immediacy, throughput | QUALITY | S |
| **SECURITY** | Protection, control, certainty | OPENNESS | Σ |
| **AUTONOMY** | Independence, flexibility | COORDINATION | A |
| **SIMPLICITY** | Elegance, parsimony | COMPLETENESS | Λ |
| **NOVELTY** | Innovation, discovery | RELIABILITY | N |
| **PRESENT** | Now, immediate | FUTURE | T |

### Derived Tensions from Dimension Pairs

| Tension | Dimensions | Why Trade-off |
|---------|------------|---------------|
| **Analysis vs Action** | E ↔ S | Exploring takes time |
| **Quality vs Quantity** | P ↔ S | Precision takes time |
| **Depth vs Breadth** | P ↔ E | Can't be deep everywhere |
| **Open vs Protected** | E ↔ Σ | Access enables attack |
| **General vs Specific** | E ↔ P | Generality loses precision |
| **Simple vs Complete** | Λ ↔ E | Completeness adds complexity |
| **Now vs Later** | T₀ ↔ T₁ | Present action forecloses future |
| **Explore vs Exploit** | E ↔ Σ | Search vs harvest |
| **Novel vs Reliable** | N ↔ Σ | New is uncertain |
| **Autonomous vs Coordinated** | A ↔ Σ | Independence loses coherence |

**Source**: araw_2026-01-27_tension-structure-seed.md

---

# Proposed 7-Category System (Replacing Original 6)

The original 6 clusters are recategorized into 7 based on the seed structure:

## Category 1: RESOURCE ALLOCATION
**Seed**: Finite resources, multiple uses
**Structure**: How to divide limited X among competing needs
**Master Question**: What resource is limited, and what are the competing uses?

Examples: Quality vs Quantity, Depth vs Breadth, Now vs Later, Analysis vs Action

## Category 2: INFORMATION
**Seed**: Uncertainty + need to act
**Structure**: How to decide with incomplete information
**Master Question**: What don't we know, and how much does it matter?

Examples: Explore vs Exploit, Theory vs Practice, Certainty vs Speed, General vs Specific

## Category 3: OPTIMIZATION
**Seed**: Multiple objectives, can't satisfy all
**Structure**: Pareto frontier navigation
**Master Question**: What's the Pareto frontier, and where on it to sit?

Examples: Quality vs Speed, Precision vs Recall, Sensitivity vs Specificity, Simplicity vs Power

## Category 4: STRUCTURE
**Seed**: Parts and wholes have different optima
**Structure**: Composition vs decomposition
**Master Question**: At what level of aggregation to optimize?

Examples: Component vs System, Individual vs Collective, Modular vs Integrated, Decentralized vs Centralized

## Category 5: COMMITMENT
**Seed**: Irreversibility of choice
**Structure**: When to lock in vs keep options
**Master Question**: How reversible is this, and how much is option value worth?

Examples: Decide vs Wait, Specialize vs Generalize, Focus vs Diversify, Commit vs Experiment

## Category 6: EPISTEMIC
**Seed**: Knowledge has limits, action has requirements
**Structure**: How confident to be, how much to trust
**Master Question**: What's the gap between what we know and need to know?

Examples: Confident vs Uncertain, Trust vs Verify, Simple model vs Complex model, Theory-driven vs Data-driven

## Category 7: LOGICAL (NEW - Reasoning-Specific)
**Seed**: Formal constraints on reasoning
**Structure**: Incompatibilities in logical systems
**Master Question**: What logical constraints create the incompatibility?

Examples: Completeness vs Consistency, Decidability vs Expressiveness, Soundness vs Completeness, Generalization vs Overfitting

**Source**: araw_2026-01-27_tension-structure-seed.md

---

# Logical/Reasoning-Specific Tensions

## The Meta-Tension of Reasoning

**THINKING vs ACTING** is the primal tension:
- Thinking improves decisions but takes time
- At some point must stop thinking and act
- Optimal thinking time is itself a hard problem (meta-meta recursion)

## Structural Logical Tensions (Non-Temporal)

### L1. Completeness vs Consistency (Gödel)
Any sufficiently powerful system is either incomplete or inconsistent.
**Implication**: Can never have both "prove everything true" and "prove nothing false"

### L2. Decidability vs Expressiveness (Church-Turing)
More expressive languages have undecidable problems.
**Implication**: Power to express comes with inability to always determine

### L3. Precision vs Tractability
Exact solutions often intractable; approximations may be computable.
**Implication**: Trade perfect answer for any answer

### L4. Soundness vs Completeness (Inference)
Can infer only true things (sound) vs Can infer all true things (complete).
**Implication**: Be right but miss things, or find things but sometimes be wrong

### L5. Generalization vs Overfitting
Simple models miss patterns; complex models fit noise.
**Implication**: The bias-variance trade-off applies to all learning

## Reasoning Strategy Tensions

### R1. Deduction vs Induction vs Abduction
- Deduction: Certain but narrow (given premises, what follows?)
- Induction: Broad but uncertain (given data, what generalizes?)
- Abduction: Creative but risky (given observation, what explains?)

### R2. Forward vs Backward Reasoning
- Forward: From evidence to conclusion (data-driven)
- Backward: From hypothesis to evidence (theory-driven)

### R3. Depth-First vs Breadth-First
- Depth-first: Explore one path deeply
- Breadth-first: Survey all options shallowly

### R4. Exact vs Approximate
- Exact: Correct answer slowly (or never)
- Approximate: Good-enough answer quickly

### R5. Local vs Global Coherence
- Local: Each step valid
- Global: Whole argument valid

### R6. Formal vs Informal
- Formal: Rigorous but limited in scope
- Informal: Flexible but uncertain validity

## Meta-Reasoning Tensions

### M1. Object-Level vs Meta-Level
Think about the problem vs Think about how you're thinking.
**Test**: "Is my reasoning strategy good for this problem?"

### M2. Intuition vs Analysis
Fast pattern-matching vs Slow deliberation.
**Test**: "Is this domain where intuition is calibrated?"

### M3. Confidence vs Calibration
Decide confidently vs Acknowledge uncertainty.
**Test**: "Am I more confident than my track record justifies?"

### M4. Exploration vs Exploitation (in reasoning)
Search for new approaches vs Use known approaches.
**Test**: "Have known approaches failed on this class of problem?"

**Source**: araw_2026-01-27_tension-structure-seed.md

---

# Two Tension Families

All tensions belong to one of two families:

## TEMPORAL Tensions
Arise from time's arrow, irreversibility.
- Once time is spent, it's gone
- Makes choice costs REAL (not hypothetical)
- Examples: Now vs Later, Analysis vs Action, Current vs Future

## STRUCTURAL Tensions
Arise from logical/mathematical constraints.
- Independent of time
- Would exist even with infinite time
- Examples: Completeness vs Consistency, Part vs Whole, General vs Specific

**UNIFIED by**: CONSTRAINT on what can be simultaneously true/actual

**Source**: araw_2026-01-27_tension-structure-seed.md

---

# 8x GOSM System Analysis Tensions (2026-01-27)

## 127. Universal Process vs Context-Adaptive Variants (Methodology Scope Trade-off)

**Specific instance**: One 22-step GOSM for all contexts vs Different variants (Lite, Quick, Check, After) for different contexts
**Universal form**: When should one process serve all situations vs when should variants be matched to context?

**Resolving questions**:
1. Does the full process add value in low-stakes/time-pressured situations?
2. Can context be reliably detected?
3. What's the cost of using wrong variant?

**When universal wins**:
- Process is lightweight enough for all contexts
- Context detection is unreliable
- Consistency is more valuable than optimization
- Training/learning is simplified

**When context-adaptive wins**:
- Full process is overkill for some contexts
- Context can be reliably detected
- Mismatch causes real harm (time waste, analysis paralysis)
- Different contexts have genuinely different needs

**Test**: Does using full process on low-stakes decisions feel like overkill? If yes, create lighter variants.

**Source**: araw_2026-01-27_gosm-system-power-scaling.md

---

## 128. Refinement vs Honest Rejection (Goal Handling Trade-off)

**Specific instance**: Always refine goals toward achievable vs Sometimes honestly reject impossible goals
**Universal form**: When should systems adapt user input toward achievable vs when should they honestly report impossibility?

**Resolving questions**:
1. Is the "refined" goal recognizable as the original goal?
2. Does the user want achievability or honesty?
3. Is the goal impossible or just difficult?

**When refinement wins**:
- User wants help achieving something close
- Refined goal serves the underlying need
- User explicitly consents to substitution
- Difficulty, not impossibility

**When honest rejection wins**:
- User wants to know if goal is achievable as stated
- Refinement would give different goal than wanted
- User hasn't consented to substitution
- Genuine impossibility

**When clarification (not substitution) wins**:
- Goal is unclear but achievable
- Refinement makes same goal clearer
- User recognizes refined goal as their goal

**Test**: "If user achieved the refined goal, would they feel they got what they asked for?" If no, it's substitution, not clarification.

**Source**: araw_2026-01-27_gosm-system-power-scaling.md

---

## 129. Narrative Coherence vs Empirical Validation (Quality Proxy Trade-off)

**Specific instance**: Story coherence as plan quality proxy vs Empirical testing of plan viability
**Universal form**: When is internal consistency sufficient vs when is external validation required?

**Resolving questions**:
1. Can the plan be partially tested before full commitment?
2. Has coherence correlated with outcomes historically?
3. What's the cost of coherent-but-wrong vs tested-but-delayed?

**When coherence wins**:
- External testing is impossible/impractical
- Domain has strong theory linking coherence to outcomes
- Time pressure prevents testing
- Stakes are low enough to accept coherence risk

**When empirical wins**:
- Parts can be tested incrementally
- No track record of coherence → outcomes
- Stakes justify testing delay
- Coherent plans have failed before (Hollywood problem)

**When both required**:
- Coherence as necessary filter
- Empirical as sufficient validation
- Test what can be tested, coherence for rest

**Test**: "Have coherent plans failed in this domain?" If yes, add empirical validation.

**Source**: araw_2026-01-27_gosm-system-power-scaling.md

---

## 130. Comprehensive Questions vs Efficient Questions (Question Set Trade-off)

**Specific instance**: Ask all questions for coverage vs Ask only HIGH VOI questions for efficiency
**Universal form**: When does completeness justify overhead vs when does efficiency justify gaps?

**Resolving questions**:
1. What fraction of questions are actually HIGH VOI?
2. What's the cost of missing something important?
3. Can questions be filtered by context?

**When comprehensive wins**:
- Most questions are HIGH VOI
- Missing important factors is costly
- Time is available
- Novel domain where coverage matters

**When efficient wins**:
- Many questions are LOW VOI (~40%)
- Missing minor factors is acceptable
- Time is scarce
- Familiar domain where important factors known

**When context-filtered wins**:
- Questions have domain/context tags
- Filtering removes genuinely irrelevant
- Dynamic filtering based on earlier answers

**Test**: Track which questions led to action changes. Filter persistent LOW VOI.

**Source**: araw_2026-01-27_gosm-system-power-scaling.md

---

## 131. Literal Interpretation vs Underlying Need (User Goal Trade-off)

**Specific instance**: Take stated goal literally vs Explore underlying needs
**Universal form**: When should systems take user input at face value vs when should they diagnose deeper needs?

**Resolving questions**:
1. Has user explicitly stated what they want?
2. Is exploring underlying needs requested or assumed?
3. Would literal goal achievement satisfy user?

**When literal wins**:
- User stated goal explicitly
- User corrected previous interpretation
- User said "I literally mean X"
- Goal is achievable as stated

**When underlying wins**:
- User explicitly requests need analysis
- Stated goal is clearly proxy for deeper need
- User is confused about what they want
- Literal goal would not satisfy

**When ask-first wins**:
- Uncertain which user prefers
- Both literal and underlying are plausible
- User preference can be easily checked

**Default**: Literal first. Underlying is opt-in, not default.

**Test**: "Did user ask me to explore what they 'really' want?" If no, take literally.

**Source**: araw_2026-01-27_gosm-system-power-scaling.md

---

## 132. Conventional Alternatives vs Genuinely Novel (AW Quality Trade-off)

**Specific instance**: AW branches that are obvious opposites vs AW branches that are genuinely surprising
**Universal form**: When does standard alternative thinking suffice vs when is genuine novelty required?

**Resolving questions**:
1. Are conventional alternatives actually adequate?
2. Have conventional approaches been tried and failed?
3. Is novelty for its own sake or for insight?

**When conventional wins**:
- Conventional alternatives haven't been tried
- Standard approaches are likely to work
- Novelty adds complexity without benefit

**When novel required**:
- Conventional approaches have failed
- Problem requires genuine reframe
- "Obvious" alternatives are already ruled out
- Breakthrough, not optimization, is needed

**Enforcement**: For each AW, check "Is this conventional?" If yes, require additional non-obvious search.

**Test**: "Would anyone else suggest this alternative?" If yes, it's conventional. Search for what no one else would suggest.

**Source**: araw_2026-01-27_gosm-system-power-scaling.md

---

## 133. Binary Branching vs Probabilistic Reasoning (Reasoning Structure Trade-off)

**Specific instance**: AR/AW binary for all domains vs Probabilistic/Bayesian for uncertain domains
**Universal form**: When is discrete/binary sufficient vs when is continuous/probabilistic needed?

**Resolving questions**:
1. Does the domain have genuine binary distinctions?
2. Does forcing binary lose important nuance?
3. Can probability be meaningfully assigned?

**When binary wins**:
- Claims are genuinely true/false
- Binary simplification aids reasoning
- Probability assignment would be arbitrary
- Commitment testing works

**When probabilistic wins**:
- Degrees of truth matter for decision
- Binary loses important information
- Prior probability is meaningful
- Decision theory applies

**When hybrid wins**:
- Binary exploration for structure
- Confidence levels for nuance
- VOI to determine when precision matters

**Test**: "Does treating this as probability change my action?" If yes, use probabilistic.

**Source**: araw_2026-01-27_gosm-system-power-scaling.md

---

## 134. Self-Analysis vs External Validation (Epistemic Authority Trade-off)

**Specific instance**: ARAW analyzing ARAW vs External expert/user validation
**Universal form**: When can systems validly evaluate themselves vs when is external perspective required?

**Resolving questions**:
1. Can the system see what it systematically excludes?
2. Has internal analysis historically produced improvements?
3. Is external validation available and reliable?

**When self-analysis wins**:
- Diverse internal perspectives exist
- Track record of catching own errors
- External validation unavailable
- Low stakes if wrong

**When external wins**:
- System has structural blind spots
- Historical improvements came from external feedback
- External perspective is available
- High stakes require validation

**When both required**:
- Self-analysis for breadth
- External for validation
- Compare internal vs external findings

**Test**: "Where did past improvements come from?" If external, prioritize external validation.

**Source**: araw_2026-01-27_gosm-system-power-scaling.md

---

## 135. Tool vs Solution (Epistemic Trade-off)

**Specific instance**: ARAW as SI solution vs ARAW as cognitive tool
**Universal form**: When does a methodology constitute a solution vs when is it merely a tool that assists in finding solutions?

**Resolving questions**:
1. Does the methodology itself produce the outcome, or does it help humans/AIs produce the outcome?
2. Can the methodology be removed and the outcome still achieved (slower/harder)?
3. Does the methodology have independent agency, or does it require a user?

**When tool wins**:
- Methodology requires human/AI to execute
- Outcome depends on user capability
- Methodology improves process, not creates result
- Scientific method is paradigmatic example

**When solution wins**:
- Methodology is self-executing
- Outcome follows directly from methodology application
- No external agent required
- Algorithms can be solutions

**When hybrid**:
- Tool accelerates finding solutions
- Tool enables solutions not otherwise findable
- But still requires user capability

**Test**: "If you removed the human/AI user, would the methodology produce the outcome?" If no, it's a tool.

**Source**: araw_2026-01-27_superintelligence-missing-ingredient.md

---

## 136. Novelty vs Variant (Novelty Trade-off)

**Specific instance**: ARAW as novel paradigm vs ARAW as variant of existing methods
**Universal form**: When does combining existing elements create genuine novelty vs when is it incremental variation?

**Resolving questions**:
1. Does the combination enable outcomes not achievable by components?
2. Is the integration principle itself new, or just the particular combination?
3. Would practitioners in the field recognize it as fundamentally different?

**When novel wins**:
- Emergent capabilities from combination
- New paradigm that reframes understanding
- Practitioners recognize qualitative shift
- Can't be reduced to sum of parts

**When variant wins**:
- Each component exists elsewhere
- Integration is mechanical, not conceptual
- Practitioners see "another version of X"
- Improvements are incremental, not paradigmatic

**When both/neither**:
- Novel combination, variant components
- May be novel in some dimensions, variant in others
- "Novel enough" depends on context

**Test**: "Would expert practitioners call this fundamentally new, or a variant of existing approaches?" Accept their judgment.

**Source**: araw_2026-01-27_superintelligence-missing-ingredient.md

---

## 137. Scale vs Methodology (Search Trade-off)

**Specific instance**: More compute/data vs better reasoning methods for AI capability
**Universal form**: When does brute force (scale) beat cleverness (methodology), and vice versa?

**Resolving questions**:
1. Is the current solution far from capability ceiling, or near it?
2. Does scaling have diminishing returns in this domain?
3. Can methodology improvements be applied quickly?
4. What's the cost/benefit of each approach?

**When scale wins**:
- Far from ceiling, room to grow
- Scaling laws hold strongly
- Methodology improvements are marginal
- Compute is cheap relative to insight

**When methodology wins**:
- Near ceiling, scaling has diminishing returns
- Clever algorithms beat brute force
- Methodology breakthrough is available
- Insight is cheap relative to compute

**When both needed**:
- Scale provides capability ceiling
- Methodology determines how close to ceiling
- Neither alone is sufficient
- Optimal: apply methodology at scale

**Test**: "Are we utilizing current capability well, or wasting capacity?" If wasting, methodology first. If utilizing well, scale.

**Source**: araw_2026-01-27_superintelligence-missing-ingredient.md

---

## 138. Self-Interest vs Truth (Epistemic Trade-off)

**Specific instance**: User wanting validation for ARAW vs user wanting honest assessment
**Universal form**: When does the questioner's stake in the answer distort the inquiry vs when can honest inquiry proceed?

**Resolving questions**:
1. What does the questioner gain if answer is X vs Y?
2. Did questioner explicitly request critical scrutiny?
3. Is questioner capable of accepting unfavorable answers?
4. Are there safeguards against motivated reasoning?

**When self-interest distorts**:
- Strong incentive for particular answer
- No explicit request for criticism
- History of rejecting unfavorable findings
- No adversarial safeguards

**When truth can proceed**:
- Questioner explicitly requests challenge
- Questioner has history of accepting corrections
- Stakes of being wrong exceed stakes of being validated
- Adversarial frame applied

**When hybrid**:
- Apply adversarial scrutiny
- Weight unfavorable findings higher
- Seek external validation
- Test questioner response to challenges

**Test**: "What would convince the questioner their preferred answer is wrong?" If nothing, self-interest dominates.

**Source**: araw_2026-01-27_superintelligence-missing-ingredient.md

---

## 139. Single vs Multiple Ingredient (Granularity Trade-off)

**Specific instance**: One missing SI ingredient vs combination of factors
**Universal form**: When does a problem have a single bottleneck vs when does it require solving multiple constraints?

**Resolving questions**:
1. Is the system simple (few interacting parts) or complex (many interacting parts)?
2. Would removing one constraint unblock progress, or are multiple binding?
3. Do historical analogies show single breakthroughs or accumulated advances?
4. Is "single bottleneck" the simplest explanation, or oversimplification?

**When single wins**:
- Simple system with clear bottleneck
- One constraint is clearly binding
- Historical analogies show breakthrough moments
- Removing one thing unblocks everything

**When multiple wins**:
- Complex system with many interactions
- Multiple constraints are simultaneously binding
- Historical analogies show accumulated progress
- No single change is sufficient

**When uncertain**:
- Complex systems often look like single bottleneck
- But bottlenecks shift once removed
- "Single" may mean "current, not final"
- Sequence of single fixes ≠ single fix

**Test**: "If we removed the supposed single bottleneck, would the problem be solved or would another bottleneck appear?" If another appears, it's multiple.

**Source**: araw_2026-01-27_superintelligence-missing-ingredient.md

---

## 143. Family Complexity vs Coherence (Architecture Trade-off)

**Specific instance**: Document family with multiple docs vs single monolithic document
**Universal form**: When does modular architecture create or destroy system coherence?

**Resolving questions**:
1. Do components change at different rates?
2. Does coordination cost exceed benefits?
3. Can consistency be maintained across modules?

**When family/modular wins**:
- Components have different change rates
- Different audiences need different views
- Updates can be targeted and local
- Consistency is enforceable

**When monolithic wins**:
- Components tightly coupled
- Consistency hard to maintain
- Single audience
- Coordination overhead exceeds benefits

**Test**: "Do different parts change independently?" If yes, separate them.

**Source**: araw_2026-01-27_constitution-outline.md

---

## 144. Explicit Gaps vs Gaming Surface (Disclosure Trade-off)

**Specific instance**: Documenting gaps in constitution vs hiding gaps to prevent exploitation
**Universal form**: When does transparency about limitations create or prevent exploitation?

**Resolving questions**:
1. Will adversaries discover gaps anyway?
2. Does gap documentation help legitimate users?
3. Can escalation paths substitute for gap enumeration?

**When explicit gaps wins**:
- Gaps discoverable anyway
- Legitimate users need to know
- Escalation paths exist
- Trust requires honesty

**When hiding gaps wins**:
- Gaps truly obscure
- Enumeration enables gaming
- Legitimate users don't need gap list
- Security through obscurity has value

**When escalation-over-enumeration wins**:
- Honest about incompleteness
- But don't enumerate attack surface
- Paths to handle gaps without listing them

**Test**: "Would an adversary find this gap anyway?" If yes, document it with escalation.

**Source**: araw_2026-01-27_constitution-outline.md

---

## 145. Scaling-Invariant vs Capability-Specific (Stability Trade-off)

**Specific instance**: Principles valid at any capability vs principles indexed to capability
**Universal form**: When should rules be context-independent vs context-dependent?

**Resolving questions**:
1. Does the principle pass "100x test"?
2. Can applications be separated from principles?
3. What breaks at extreme capability?

**When invariant wins**:
- Principle is about process, not content
- Applications can be indexed separately
- More capable = principle more important
- No capability makes principle obsolete

**When capability-specific wins**:
- Principle is about current limitations
- Capability enables principled behavior impossible before
- Principle becomes harmful at high capability
- Principle becomes meaningless at high capability

**Resolution pattern**: Separate invariant principles from capability-indexed applications.

**Test**: "At 100x capability, does this principle still make sense?" If yes, it's invariant.

**Source**: araw_2026-01-27_constitution-outline.md

---

## 146. Normative Commitment vs Justification (Foundation Trade-off)

**Specific instance**: Stating values as commitments vs deriving values from arguments
**Universal form**: When should foundations be committed vs justified?

**Resolving questions**:
1. Is there a non-circular derivation?
2. Does derivation invite endless debate?
3. Are implications more important than justification?

**When commitment wins**:
- Values are foundational, not derived
- No non-circular derivation exists
- Focus on implications more useful
- Derivation invites philosophical warfare

**When justification wins**:
- Derivation is possible and stable
- Stakeholders need to understand reasoning
- Without justification, commitments seem arbitrary
- Justification enables critique and improvement

**Resolution pattern**: Commit to values, explain implications (what it means), not derivation (why it's right).

**Test**: "Would explaining 'why' invite endless debate?" If yes, commit instead.

**Source**: araw_2026-01-27_constitution-outline.md

---

## 147. Layered Visibility vs Maintenance Burden (Presentation Trade-off)

**Specific instance**: Surface/middle/deep layers vs single presentation
**Universal form**: When do multiple views justify their maintenance cost?

**Resolving questions**:
1. Are audiences truly different?
2. Can views be generated from single source?
3. Does maintenance burden outweigh benefits?

**When layers win**:
- Audiences have genuinely different needs
- Views can be generated, not separately authored
- Benefits of targeting exceed maintenance cost
- Consistency can be enforced

**When single layer wins**:
- Audiences are similar
- Generation is lossy or impossible
- Maintenance burden unsustainable
- Single view serves all needs

**Resolution pattern**: Single canonical source, generated views for audiences.

**Test**: "Can views be generated from one source?" If yes, layer. If no, consider single layer.

**Source**: araw_2026-01-27_constitution-outline.md

---

## 148. Example Inclusion vs Attack Surface (Illustration Trade-off)

**Specific instance**: Including examples to aid understanding vs examples revealing exploits
**Universal form**: When do examples help vs hurt in adversarial environments?

**Resolving questions**:
1. Is the document read by adversaries?
2. Do examples reveal edge case handling?
3. Can principles communicate without examples?

**When examples help**:
- Audience is trusted
- Examples illustrate, don't bound
- Principles are too abstract without them
- Training benefits outweigh risks

**When examples hurt**:
- Adversarial readers exist
- Examples reveal gaming strategies
- Examples falsely bound interpretation
- Principles sufficient alone

**Resolution pattern**: Internal examples (for training), principles-only in public.

**Test**: "Would an adversary learn from this example?" If yes, make it internal-only.

**Source**: araw_2026-01-27_constitution-outline.md

---

## 149. Structured Incompleteness vs False Humility (Honesty Trade-off)

**Specific instance**: Acknowledging gaps vs acknowledging so many gaps document seems useless
**Universal form**: When does acknowledging limitations become self-defeating?

**Resolving questions**:
1. Does gap acknowledgment enable appropriate trust?
2. Do gaps have actionable escalation paths?
3. Is "we don't know" followed by "so do this"?

**When structured incompleteness wins**:
- Gaps have escalation paths
- Acknowledgment calibrates trust
- Better than false completeness
- Gaps are genuine, not performative

**When it becomes false humility**:
- Gaps without escalation = useless
- Too many gaps = no guidance
- Gap acknowledgment is performance
- Gaps hide behind "epistemic modesty"

**Resolution pattern**: Every gap needs an escalation path, not just acknowledgment.

**Test**: "Does the gap have a 'what to do' attached?" If no, add one or don't mention gap.

**Source**: araw_2026-01-27_constitution-outline.md

---

## 150. Meta-Gap vs Infinite Regress (Self-Reference Trade-off)

**Specific instance**: "We don't know what we don't know" acknowledgment vs endless meta-levels
**Universal form**: When does meta-level acknowledgment help vs create infinite regress?

**Resolving questions**:
1. Is one level of meta sufficient?
2. Does additional meta change action?
3. Is there a practical stopping point?

**When meta-gap helps**:
- Honest about fundamental uncertainty
- One level is sufficient
- Default behavior exists for unknown unknowns
- Stops at actionable level

**When infinite regress starts**:
- Multiple meta-levels add nothing
- No default behavior specified
- Meta-acknowledgment without action
- Self-reference without grounding

**Resolution pattern**: One level of meta-gap, then default behavior + escalation.

**Test**: "Does another meta-level change what to do?" If no, stop.

**Source**: araw_2026-01-27_constitution-outline.md

---

## 140. Imported Assumptions vs Fresh Exploration (Epistemic Trade-off)

**Specific instance**: Prior ARAW imported "SI is hard" as foundational vs user wanted genuine exploration
**Universal form**: When should prior knowledge be imported vs questioned from scratch?

**Resolving questions**:
1. Is the "knowledge" from evidence or from social consensus?
2. Does the source have systematic biases?
3. Are stakes high enough to warrant fresh look?
4. Is the "knowledge" actually a guess disguised as fact?

**When import wins**:
- Prior knowledge is well-established empirically
- Time is limited
- Stakes are low
- Sources are diverse and disagree on details

**When fresh exploration wins**:
- Prior "knowledge" might be groupthink
- Stakes are high
- Sources suspiciously agree (may indicate shared bias)
- "Knowledge" is actually social proof not evidence

**Test**: "Would I believe this if everyone around me disagreed?" If social pressure drives belief, question it.

**Source**: araw_2026-01-27_si-missing-ingredient-v2.md

---

## 141. Safe/Skeptical vs Exploratory/Open (Search Strategy Trade-off)

**Specific instance**: AW-heavy analysis dismissed user's ARAW hypothesis vs balanced exploration
**Universal form**: When should analysis lean skeptical vs when should it be open/exploratory?

**Resolving questions**:
1. Is the user asking for risk evaluation or possibility exploration?
2. Is skepticism serving truth or serving safety?
3. Is being "wrong" (too optimistic) worse than missing opportunity?
4. What does the user's stated preference indicate?

**When skeptical wins**:
- User is about to make irreversible decision
- Downside risk is severe
- User explicitly wants risks identified
- Pattern matching suggests likely failure

**When exploratory wins**:
- User is exploring possibilities, not deciding
- User has unexplored intuition worth investigating
- Missing opportunity is worse than false positive
- User explicitly wants possibilities explored

**Test**: "Is the user asking 'what could go wrong' or 'what could go right'?" Match stance to request.

**Source**: araw_2026-01-27_si-missing-ingredient-v2.md

---

## 142. Model Limits vs Scaffold Transcendence (Capability Trade-off)

**Specific instance**: Claim that structure can't exceed components vs evidence of emergence
**Universal form**: When do tools/methods merely amplify existing capability vs genuinely transcend it?

**Resolving questions**:
1. Does the scaffold enable qualitatively new operations?
2. Are there counter-examples where structure created emergence?
3. Is the "limit" from physics or from assumption?
4. Does information theory actually apply here?

**When amplify only**:
- Tool operates strictly within same framework as user
- Tool is purely efficiency improvement
- No new capabilities emerge
- Output is same type as input

**When transcendence possible**:
- Tool enables operations impossible without it
- Historical examples show emergence (ant colonies, scientific method, markets)
- Tool provides new structure for combination
- Qualitatively different outcomes emerge

**Test**: "Can you point to emergent behavior that neither the base nor the scaffold alone could produce?" If yes, transcendence.

**Source**: araw_2026-01-27_si-missing-ingredient-v2.md

---

## 143. Lab Consensus vs Outsider Innovation (Authority Trade-off)

**Specific instance**: Trusting AI labs have searched right space vs identifying systematic blind spots
**Universal form**: When should expert consensus be trusted vs when might outsiders see what experts miss?

**Resolving questions**:
1. Do experts have diverse perspectives or shared training?
2. Are expert incentives aligned with finding truth?
3. What space might experts systematically ignore?
4. Is consensus based on evidence or on shared assumptions?

**When trust consensus**:
- Experts disagree among themselves (shows diverse perspectives)
- Incentives reward finding truth
- Experts have explored widely
- Consensus is based on evidence

**When question consensus**:
- Experts suspiciously agree (groupthink signal)
- Incentives reward certain answers (benchmark optimization)
- Experts share systematic biases (same training, same metrics)
- Consensus is based on assumptions not evidence

**Test**: "Do experts disagree on fundamentals, or only on details?" If only details, suspect shared blind spots.

**Source**: araw_2026-01-27_si-missing-ingredient-v2.md

---

# Mathematical Formalization Tensions (2026-01-27)

## 144. Formalization vs Flexibility (Rigor Trade-off)

**Specific instance**: Mathematical formalization of ARAW vs Informal flexibility
**Universal form**: When does formalizing a process improve it vs when does formalization destroy essential flexibility?

**Resolving questions**:
1. Does the process's value come from its flexibility?
2. Does formalization reveal hidden structure or just add overhead?
3. Can you selectively formalize (some parts formal, others informal)?

**When formalization wins**:
- Precision catches errors
- Structure reveals non-obvious patterns
- Results need to be verified/communicated precisely
- Process has clear, well-defined inputs/outputs

**When flexibility wins**:
- Process operates on vague/contextual inputs
- Creativity and exploration are central
- Formalization adds overhead without insight
- Process value IS its adaptability

**Test**: "Does formalization reveal something new, or just rename what we already knew?" If just renaming, overhead > benefit.

**Source**: araw_2026-01-27_math-branches-for-search.md

---

## 145. Abstraction vs Applicability (Granularity Trade-off)

**Specific instance**: Category theory (highly abstract) vs Graph theory (concrete)
**Universal form**: When do abstract mathematical tools provide insight vs when do they lose operational content?

**Resolving questions**:
1. Does the abstraction reveal hidden structure?
2. Can abstract results be translated to concrete operations?
3. Is the overhead of learning/applying abstraction justified?

**When abstraction wins**:
- Hidden structure exists and abstraction reveals it
- Abstract result applies across multiple concrete domains
- "Unreasonable effectiveness" - abstraction unlocks solutions

**When concrete wins**:
- Forcing abstraction loses essential properties
- Direct approach is already efficient
- Abstraction is "abstract nonsense" - organization without insight

**Test**: "Does the abstract tool solve a problem the concrete tool couldn't?" If no, use concrete.

**Source**: araw_2026-01-27_math-branches-for-search.md

---

## 146. Proof vs Performance (Rigor Trade-off)

**Specific instance**: Proving ARAW termination formally vs Just running ARAW and seeing
**Universal form**: When should correctness be proven before execution vs when should empirical performance suffice?

**Resolving questions**:
1. What is the cost of being wrong?
2. How long does proving take vs just doing?
3. Does the proof reveal anything beyond "it works"?

**When proof wins**:
- Cost of failure is high (safety-critical)
- Proof is relatively cheap to obtain
- Proof reveals insight beyond "works/doesn't work"
- Result will be used many times

**When performance wins**:
- Empirical testing is fast and cheap
- Proof would be expensive/impossible
- "Works in practice" is sufficient evidence
- One-off use case

**Test**: "Is proving correctness faster than testing and fixing?" If no, test empirically.

**Source**: araw_2026-01-27_math-branches-for-search.md

---

## 147. Pure vs Applied (Investment Trade-off)

**Specific instance**: Learning category theory vs Learning information theory for ARAW
**Universal form**: When should investment go to "pure" knowledge with uncertain application vs "applied" knowledge with clear use?

**Resolving questions**:
1. What is the probability that pure knowledge will apply?
2. What is the value if it does apply?
3. What is the opportunity cost of not learning applied knowledge?

**When pure wins**:
- High value if applicable (breakthrough potential)
- Pure knowledge enables seeing connections others miss
- Long time horizon allows waiting for application

**When applied wins**:
- Clear immediate application exists
- Opportunity cost of delay is high
- Pure knowledge has low probability of application

**Formula**: Expected Value = P(applies) × V(if applies). Use whichever has higher expected value.

**Test**: "In the next year, which knowledge is more likely to produce results?"

**Source**: araw_2026-01-27_math-branches-for-search.md

---

## 148. Counting vs Finding (Search Method Trade-off)

**Specific instance**: Combinatorics (counts possibilities) vs Search algorithms (finds solutions)
**Universal form**: When does knowing HOW MANY help vs when do you just need to FIND ONE?

**Resolving questions**:
1. Does knowing the count change strategy?
2. Is finding a single solution sufficient?
3. Does counting bound search time?

**When counting wins**:
- Count determines feasibility (10^50 = needs heuristics)
- Need to know "does solution exist?" before searching
- Symmetry counting reduces effective search space

**When finding wins**:
- Just need one valid solution
- Counting would take as long as finding
- Solution space is structured (can navigate, not just enumerate)

**Test**: "Would knowing the exact count change what I do next?" If no, skip counting.

**Source**: araw_2026-01-27_math-branches-for-search.md

---

## 149. Probabilistic vs Knightian Uncertainty (Uncertainty Type Trade-off)

**Specific instance**: Assigning probabilities to ARAW claims vs Acknowledging unknown unknowns
**Universal form**: When can uncertainty be quantified with probabilities vs when is uncertainty fundamentally unquantifiable?

**Resolving questions**:
1. Is the sample space well-defined?
2. Can priors be reasonably assigned?
3. Are there unknown unknowns that probabilities miss?

**When probabilistic wins**:
- Sample space is clear
- Historical data or reasoning supports priors
- Expected value calculations are meaningful
- Repeated decisions allow learning

**When Knightian wins**:
- Unknown unknowns are likely
- No basis for prior assignment
- One-shot decisions with no feedback
- Fundamental model uncertainty

**Test**: "If I had to bet on this outcome, could I state odds?" If no basis for odds, Knightian.

**Source**: araw_2026-01-27_math-branches-for-search.md

---

# Key Mathematical Insights for ARAW/GOSM

## TENSIONS AS PARETO FRONTIERS

Every tension is mathematically a **Pareto frontier** in value space:
- You cannot improve on one dimension without losing on another
- The frontier represents all non-dominated trade-offs
- "Resolving" a tension means choosing a point on the frontier

**Navigation strategy**:
1. IDENTIFY the objectives in tension
2. MAP the Pareto frontier (what trade-offs are possible)
3. LOCATE current position on frontier
4. CHOOSE movement direction based on context
5. ACCEPT that you cannot escape the frontier

**Source**: araw_2026-01-27_math-branches-for-search.md

---

## CRUX AS MAXIMUM INFORMATION GAIN

The best CRUX question is mathematically the one that **maximizes expected information gain**:

```
Best CRUX = argmax_Q [ H(Claims) - E[H(Claims | Answer_Q)] ]
```

Where:
- H(Claims) = entropy (uncertainty) over claims
- E[H(Claims | Answer_Q)] = expected entropy after learning answer to question Q
- The CRUX that most reduces uncertainty is the best question to answer

**Source**: araw_2026-01-27_math-branches-for-search.md

---

## ARAW AS MODAL LOGIC

ARAW is formally equivalent to **informal modal reasoning**:
- ASSUME RIGHT = □p (necessarily p, explore world where p is necessary)
- ASSUME WRONG = ◇¬p (possibly not-p, explore world where p might be false)
- ARAW explores both modal operators on each claim

**Source**: araw_2026-01-27_math-branches-for-search.md

---

## PROOF TECHNIQUES AS SEARCH STRATEGIES

| Proof Technique | ARAW Search Strategy |
|-----------------|---------------------|
| Contradiction | ASSUME WRONG → derive contradiction → original true |
| Induction | Base case + step → all cases |
| Construction | Build specific example |
| Diagonalization | Generate option different from all listed |
| Probabilistic | Random sample, most have property |
| Fixed Point | Iterate until stable |

**Source**: araw_2026-01-27_math-branches-for-search.md

---

## MATHEMATICAL TOOLS RANKED BY UTILITY FOR ARAW

| Rank | Tool | Expected Utility |
|------|------|------------------|
| 1 | Information Theory | VERY HIGH |
| 2 | Probability Theory | VERY HIGH |
| 3 | Computational Complexity | HIGH |
| 4 | Optimization Theory | HIGH |
| 5 | Graph Theory | MEDIUM-HIGH |
| 6 | Order Theory/Lattices | MEDIUM |
| 7 | Modal Logic | MEDIUM |
| 8 | Combinatorics | MEDIUM |
| 9 | Category Theory | MEDIUM (high variance) |
| 10+ | Specialized branches | LOW |

**Source**: araw_2026-01-27_math-branches-for-search.md


---

## 135. Variant Simplicity vs Completeness (Process Trade-off)

**Specific instance**: How many steps should GOSM-Lite have?
**Universal form**: When simplifying a process, how much can you cut without losing value?

**Resolving questions**:
1. Does each removed step change outcomes significantly?
2. What is the irreducible core that cannot be cut?
3. Is simplicity for speed or for cognitive load reduction?

**When simplicity wins**:
- Time pressure is real and binding
- Core insight is singular (one critical assumption)
- User can return for full analysis later
- Action is reversible

**When completeness wins**:
- Oversimplification leads to predictable errors
- Missing steps have high VOI
- No opportunity to return
- Stakes are high

**Test**: "If I cut this step, what gets missed? Does that matter?"

**Source**: araw_2026-01-27_gosm-implementation-improvements.md

---

## 136. Explicit Consent vs Flow (Transparency Trade-off)

**Specific instance**: Asking for substitution consent interrupts the workflow
**Universal form**: When does making something explicit cost more than it saves?

**Resolving questions**:
1. What is the cost of the interrupt?
2. What is the cost of proceeding without consent?
3. Does the user expect to be consulted?

**When consent wins**:
- Stakes are high
- User preference genuinely matters
- Wrong choice leads to significant regret
- User has previously complained about assumptions

**When flow wins**:
- Low stakes
- User trusts system judgment
- Interrupt feels like obstacle, not protection
- Easy to reverse if wrong

**Test**: "Would the user thank me for asking or resent the interruption?"

**Source**: araw_2026-01-27_gosm-implementation-improvements.md

---

## 137. Prediction Logging vs Action Speed (Tracking Trade-off)

**Specific instance**: Logging predictions takes time that could be spent acting
**Universal form**: When does tracking overhead exceed tracking value?

**Resolving questions**:
1. Will this prediction ever be reviewed?
2. Is calibration valuable for this domain?
3. Are we making repeated decisions where learning helps?

**When logging wins**:
- High stakes decisions
- Repeated decisions in same domain
- Calibration feedback loop is valuable
- Time is available

**When speed wins**:
- One-off decisions
- Low stakes
- Urgent situations
- No realistic review mechanism

**Test**: "If I make this prediction, what are the odds I'll ever check if it was right?"

**Source**: araw_2026-01-27_gosm-implementation-improvements.md

---

## 138. Mandatory Enforcement vs Natural Flow (Process Control Trade-off)

**Specific instance**: Forcing unconventional alternatives feels artificial
**Universal form**: When does process enforcement help vs hinder quality?

**Resolving questions**:
1. Does the default behavior need overriding?
2. Does enforcement produce valuable output or filler?
3. Can good alternatives emerge without forcing?

**When mandatory wins**:
- Default is systematically biased
- Important options get missed without forcing
- Quality difference is measurable
- Enforcement cost is low

**When natural wins**:
- Good alternatives emerge organically
- Forced output is often discarded
- Enforcement creates resentment
- Quality is similar either way

**Test**: "Are the forced unconventional alternatives valuable or just checked boxes?"

**Source**: araw_2026-01-27_gosm-implementation-improvements.md

---

## 144. Depth vs Breadth (Search Strategy Trade-off)

**Specific instance**: Go deeper with 16x/32x vs go wider with parallel ARAW threads
**Universal form**: When does depth beat breadth in search problems?

**Resolving questions**:
1. Does solving sub-problem A help solve sub-problem B?
2. Are there dependencies between branches of exploration?
3. Is the problem more "tall" (sequential) or "wide" (independent)?
4. What's the exploration/exploitation balance needed?

**When depth wins**:
- Problem requires sequential reasoning
- Dependencies exist between sub-problems
- Deeper understanding reveals more
- Quality requires thorough exploration

**When breadth wins**:
- Problem has independent sub-problems
- Exploration of alternatives matters
- Different perspectives find different things
- Parallelization is cheap

**Test**: "Does solving one branch help me solve another?" If yes, depth. If independent, breadth.

**Source**: araw_2026-01-27_increasing-araw-si-probability.md

---

## 145. Native vs Prompted Capability (Capability Trade-off)

**Specific instance**: Train ARAW into model vs prompt ARAW externally
**Universal form**: When should capability be native (baked in) vs scaffolded (external)?

**Resolving questions**:
1. How stable is the capability specification?
2. How frequently is it used?
3. Does it need to evolve rapidly?
4. Is flexibility or speed more important?

**When native wins**:
- Capability is well-understood and stable
- Used very frequently
- Speed/latency matters
- Core to the system's value

**When scaffolded wins**:
- Capability is evolving rapidly
- Need flexibility to change approach
- External tools provide value
- Testing different versions

**Test**: "Is this capability stable enough to bake in, or will it change next month?" Stable → native. Evolving → scaffold.

**Source**: araw_2026-01-27_increasing-araw-si-probability.md

---

## 146. Speed vs Safety (Deployment Trade-off)

**Specific instance**: Move fast toward SI vs careful alignment checking at each step
**Universal form**: When does speed trump safety, and when must safety take precedence?

**Resolving questions**:
1. Are mistakes reversible?
2. What are the consequences of failure?
3. Is there competitive pressure?
4. Can we monitor and correct?

**When speed wins**:
- Mistakes are reversible
- Consequences are bounded
- Competitive advantage from being first
- Monitoring is in place

**When safety wins**:
- Mistakes are irreversible
- Consequences could be catastrophic
- Better to be second and right
- Alignment is genuinely uncertain

**Test**: "If this goes wrong, can we undo it?" If yes, consider speed. If no, prioritize safety.

**Source**: araw_2026-01-27_increasing-araw-si-probability.md

---

## 147. Open vs Closed Development (Strategy Trade-off)

**Specific instance**: Open-source ARAW methodology vs keep proprietary for advantage
**Universal form**: When does openness help vs hurt development?

**Resolving questions**:
1. Do network effects exist?
2. Is community contribution valuable?
3. What are the misuse risks?
4. Is competitive advantage important?

**When open wins**:
- Network effects exist (more users → better product)
- Community contribution accelerates development
- Transparency improves safety
- Information wants to be free anyway

**When closed wins**:
- Competitive advantage is critical
- Misuse risks are high
- Control over deployment is necessary
- First-mover advantage exists

**Test**: "Do benefits of openness (contribution, transparency, adoption) outweigh risks (misuse, loss of advantage)?" Context-dependent.

**Source**: araw_2026-01-27_increasing-araw-si-probability.md

---

## 139. Variants vs Composition (Architecture Trade-off)

**Specific instance**: Should skills have pre-built Lite/Full variants or be composed from atomic parts?
**Universal form**: When should components be pre-bundled vs composed at runtime?

**Resolving questions**:
1. How often are the compositions reused?
2. Is the composition logic complex or simple?
3. Does pre-bundling enable optimizations?

**When variants win**:
- Common patterns reused frequently
- Composition logic is complex
- Pre-bundling enables optimizations
- Users expect ready-to-use options

**When composition wins**:
- Combinations are unpredictable
- Atomic parts are simple
- Flexibility is more valuable than convenience
- Maintenance of variants is burdensome

**Test**: "Would users compose the same thing repeatedly?"

**Source**: araw_2026-01-27_skill-improvement-analysis.md

---

## 140. Sync vs Separation (Implementation Trade-off)

**Specific instance**: Should Python ARAW mirror SKILL.md or be independently optimized?
**Universal form**: When should implementations stay synchronized vs specialize?

**Resolving questions**:
1. Do they share principles that should be consistent?
2. Does specialization provide significant benefits?
3. Is sync overhead acceptable?

**When sync wins**:
- Shared principles must be consistent
- Divergence causes confusion
- Sync overhead is low
- Users interact with both

**When separation wins**:
- Each has fundamentally different requirements
- Specialization provides major benefits
- Sync overhead is prohibitive
- Users use only one or the other

**Test**: "Would a user be confused if these differed?"

**Source**: araw_2026-01-27_skill-improvement-analysis.md

---

## 141. Improve vs Consolidate (Resource Trade-off)

**Specific instance**: Improve 306 skills now or consolidate first?
**Universal form**: When should you improve existing assets vs restructure first?

**Resolving questions**:
1. How much redundancy exists?
2. Is improvement effort wasted on redundant items?
3. Is restructuring disruptive to current users?

**When improve wins**:
- Redundancy is low
- Improvements are urgent
- Restructuring is too disruptive
- High-value items can be identified

**When consolidate wins**:
- Significant redundancy exists
- Improvement would be wasted
- Restructuring has low cost
- Clear consolidation targets exist

**Test**: "Would improving this item survive consolidation?"

**Source**: araw_2026-01-27_skill-improvement-analysis.md

---

## 142. Hub vs Distributed (Architecture Trade-off)

**Specific instance**: Should goal_journey_system be the central hub or should skills be independent?
**Universal form**: When should systems have a central coordinator vs peer-to-peer?

**Resolving questions**:
1. Do components need coordination?
2. Is the hub a bottleneck?
3. Can components function independently?

**When hub wins**:
- Components need orchestration
- Cross-cutting concerns exist
- Central state is valuable
- Hub can handle the load

**When distributed wins**:
- Components are truly independent
- Hub would be a bottleneck
- Local optimization is sufficient
- Flexibility trumps coordination

**Test**: "Do components need to know about each other?"

**Source**: araw_2026-01-27_skill-improvement-analysis.md

---

# System Improvement Tensions (2026-01-27)

## 150. Improvement vs Complexity (System Evolution Trade-off)

**Specific instance**: Adding features to ARAW vs keeping it simple
**Universal form**: When does adding capabilities help a system vs when does complexity hurt usability?

**Resolving questions**:
1. Does the new capability address a real gap?
2. Does adding it increase learning curve significantly?
3. Is the improvement modular (can ignore if not needed)?
4. Does complexity compound or stay isolated?

**When improvement wins**:
- Addresses demonstrated gap
- Modular - can be ignored
- Learning curve is proportional to benefit
- Fills clear user need

**When simplicity wins**:
- Feature is "nice to have" not "need to have"
- Complexity spreads (not contained)
- Users already struggling with current complexity
- Core system works well enough

**Test**: "Would a new user be intimidated by this? Would an experienced user find it valuable?"

**Source**: araw_2026-01-27_system-improvements.md

---

## 151. Theory vs Implementation (Insight Application Trade-off)

**Specific instance**: Mathematical insights about ARAW vs operational improvements
**Universal form**: When should theoretical understanding precede implementation vs when should you "just build it"?

**Resolving questions**:
1. Does theory reveal non-obvious implementation paths?
2. Is theory actionable or just conceptually satisfying?
3. What's the cost of implementing wrong thing?
4. Can you iterate quickly enough to skip theory?

**When theory first**:
- Implementation is expensive/irreversible
- Theory reveals non-obvious approaches
- Similar problems have theory that transfers
- "Measure twice, cut once" economics

**When implementation first**:
- Theory is unclear or contested
- Fast iteration is possible
- Learning by doing is efficient
- Real-world feedback beats theoretical prediction

**Test**: "Does the theory tell me WHAT to build, or just WHY something works?"

**Source**: araw_2026-01-27_system-improvements.md

---

## 152. Quick Wins vs Systematic Change (Improvement Strategy Trade-off)

**Specific instance**: 40-minute quick wins vs full reorganization
**Universal form**: When should improvement be incremental vs when is restructuring needed?

**Resolving questions**:
1. Do quick wins address root causes or symptoms?
2. Is accumulated technical debt blocking progress?
3. What's the switching cost of restructuring?
4. Can quick wins compound into systematic change?

**When quick wins**:
- Root causes are unclear
- System is functional
- Quick wins address real gaps
- Can evaluate before bigger changes

**When systematic change**:
- Quick wins are band-aids
- Technical debt is compounding
- Architecture is fundamentally wrong
- Short-term fixes create long-term problems

**Test**: "After 5 quick wins, will the system be fundamentally better or just more complex?"

**Source**: araw_2026-01-27_system-improvements.md

---

## 153. Generation vs Evaluation (Bottleneck Trade-off)

**Specific instance**: Does ARAW fail because it doesn't generate good options, or can't recognize good options when generated?
**Universal form**: Is the system bottlenecked at producing candidates or at assessing them?

**Resolving questions**:
1. When reviewing outputs, are good options present but unpursued?
2. Is the variety of generated options sufficient?
3. Can the system explain why it chose what it chose?
4. Are errors errors of omission or errors of commission?

**When generation is bottleneck**:
- Output lacks variety
- Same answers regardless of approach
- Missing obvious options
- System runs out of ideas early

**When evaluation is bottleneck**:
- Good options generated but ranked poorly
- Selection criteria unclear or misaligned
- Best options visible in retrospect but not selected
- Human judgment dramatically improves output selection

**Test**: "Review outputs - are good answers present but not identified as good?"

**Source**: araw_2026-01-27_superhuman-capability-4x.md

---

## 154. Within-Session vs Between-Session Modification (Learning Timing Trade-off)

**Specific instance**: Self-modification during ARAW vs procedure updates between sessions
**Universal form**: Should systems adapt during task execution or between tasks?

**Resolving questions**:
1. How much does context within a session matter?
2. What's the cost of mid-session instability?
3. Can learnings be reliably extracted for between-session updates?
4. Does the base system actually change within session?

**When within-session modification wins**:
- Session context is crucial
- Immediate adaptation is needed
- Problems require evolving approach
- Stable between-session procedure can't capture dynamics

**When between-session modification wins**:
- Stability within session matters
- Learnings need validation before integration
- "Modification" within session is illusory
- Procedure improvement is cumulative across sessions

**Test**: "Compare quality of early vs late session outputs - does quality measurably improve?"

**Source**: araw_2026-01-27_superhuman-capability-4x.md

---

## 155. Domain-General vs Domain-Matched (Capability Strategy Trade-off)

**Specific instance**: Add capability that works everywhere vs find problems matching existing structure
**Universal form**: Improve the tool to fit all problems vs select problems that fit the tool?

**Resolving questions**:
1. Is the tool fundamentally limited or just misapplied?
2. Are there enough problems that match the current structure?
3. What's the cost of general improvement vs selection?
4. Would domain-matching succeed quickly enough to validate approach?

**When domain-general wins**:
- Many important problems don't match current structure
- Tool improvements would help across all domains
- Domain-matching feels like avoiding hard problems
- General capability enables new applications

**When domain-matched wins**:
- Current structure has unique advantages in some domains
- General improvements are expensive/slow
- Quick wins in matched domains build momentum
- Success in one domain proves concept for investment in others

**Test**: "Is there a domain where current structure IS the competitive advantage?"

**Source**: araw_2026-01-27_superhuman-capability-4x.md

---

## 156. Scale vs Qualitative Capability (Improvement Path Trade-off)

**Specific instance**: More depth = breakthrough vs qualitative capability addition needed
**Universal form**: Can more of the same produce breakthrough, or is something different needed?

**Resolving questions**:
1. Does performance improve with scale in this domain?
2. Are there diminishing returns already visible?
3. What would a qualitative addition enable that scale cannot?
4. Is scale a proxy for something qualitative (e.g., more depth = more chances to find insight)?

**When scale wins**:
- Performance scales reliably with resources
- Breakthrough happens at sufficient scale
- Qualitative additions are unclear
- Scale is cheap relative to alternatives

**When qualitative capability wins**:
- Diminishing returns to scale
- Clear gap in capability type (e.g., verification)
- Scale adds quantity but not quality
- Competitive advantage comes from capability, not scale

**Test**: "Run at 2x scale - if output is proportionally better, scale more. If not, add capability."

**Source**: araw_2026-01-27_superhuman-capability-4x.md

---

## 151. Obscurity vs Verifiability (Transparency Trade-off)

**Specific instance**: Removing methodology reveals to prevent reverse engineering vs auditors need to understand design
**Universal form**: When does hiding process undermine legitimate scrutiny?

**Resolving questions**:
1. Who are the legitimate audiences?
2. Can different versions serve different audiences?
3. Does obscurity actually prevent understanding or just delay it?

**When obscurity wins**:
- Adversarial readers would exploit methodology
- Public version doesn't need process details
- Internal version retains full information

**When verifiability wins**:
- Single version for all audiences
- Auditors need full picture
- Methodology is itself subject to review

**Resolution pattern**: Create audience-specific versions (public obscured, internal full)

**Test**: "Would hiding this from auditors harm legitimate oversight?"

**Source**: araw_2026-01-27_constitution-reverse-engineering-check.md

---

## 152. Structure vs Naturalness (Document Design Trade-off)

**Specific instance**: Systematic structure aids compliance vs systematic structure reveals templating
**Universal form**: When does helpful structure become detectable artificiality?

**Resolving questions**:
1. Would a human-written document have this structure?
2. Is the structure necessary for function?
3. Can structure be varied while preserving function?

**When structure wins**:
- Navigation requires systematic organization
- Compliance checking needs clear structure
- Multiple documents must align

**When naturalness wins**:
- Structure itself reveals design methodology
- Organic feel matters for credibility
- Varied structure doesn't impair function

**Resolution pattern**: Keep navigation aids but vary implementation across documents

**Test**: "Could this pattern exist in a non-templated document?"

**Source**: araw_2026-01-27_constitution-reverse-engineering-check.md

---

## 153. Gap Registry vs Prose Limitations (Explicitness Trade-off)

**Specific instance**: Structured gap tracking is more complete vs structure reveals methodology
**Universal form**: When does explicit enumeration help vs hurt?

**Resolving questions**:
1. Does enumeration add information beyond prose?
2. Does enumeration format itself convey unintended information?
3. Can prose achieve the same completeness?

**When registry wins**:
- Structured data needed for tracking
- Completeness requires enumeration
- Internal use only

**When prose wins**:
- External readers don't need structure
- Format reveals design methodology
- Prose communicates same content more naturally

**Resolution pattern**: Prose for external, registry for internal tracking

**Test**: "Does the format add value beyond the content?"

**Source**: araw_2026-01-27_constitution-reverse-engineering-check.md

---

## 154. Layered Content vs Flat Document (Presentation Trade-off)

**Specific instance**: Three-layer visibility serves audiences vs pattern is detectable
**Universal form**: When does audience-targeting reveal targeting methodology?

**Resolving questions**:
1. Is the layering pattern consistent enough to detect?
2. Would varied layering preserve benefits?
3. Can audiences be served without visible layering?

**When layered wins**:
- Audiences truly need different depths
- Layering can be varied enough to obscure
- Benefits outweigh pattern detection risk

**When flat wins**:
- Single depth serves all audiences
- Pattern detection is unacceptable
- Simplicity outweighs targeting benefits

**Resolution pattern**: Use layering variably (some sections have it, some don't)

**Test**: "Would an analyst detect a systematic layer pattern?"

**Source**: araw_2026-01-27_constitution-reverse-engineering-check.md

---

## 157. Tree vs Graph Structure (Representation Trade-off)

**Specific instance**: ARAW's tree structure vs graph structure that captures real-world connections
**Universal form**: When should exploration be hierarchical/tree-like vs network/graph-like?

**Resolving questions**:
1. Do important connections exist between non-adjacent branches?
2. Does the problem have natural hierarchical decomposition?
3. Can synthesis sections bridge the tree structure adequately?
4. What's the cost of missing cross-branch connections?

**When tree wins**:
- Problem decomposes hierarchically
- Systematic exploration matters more than connections
- Depth is more valuable than integration
- Simpler to implement and follow

**When graph wins**:
- Reality has graph structure (most concepts connect to many others)
- Cross-branch insights are key value
- Integration matters more than exhaustive depth
- Missing connections = missing insights

**Test**: "Would connecting distant branches reveal insights neither contains alone?"

**Source**: araw_2026-01-27_superhuman-capability-8x.md

---

## 158. Recognition vs Generation (Capability Gap Trade-off)

**Specific instance**: Does ARAW need to generate better outputs or recognize valuable outputs?
**Universal form**: Is the bottleneck in producing good things or identifying them?

**Resolving questions**:
1. When reviewing outputs, are good options present but unpursued?
2. Can humans easily identify the best outputs from the set?
3. Is there variety in what's generated, or is it repetitive?
4. Does the system know when it's produced something valuable?

**When generation is bottleneck**:
- Output lacks variety
- Same answers regardless of approach
- Missing obvious options
- System runs out of ideas early

**When recognition is bottleneck**:
- Good options generated but ranked poorly
- Selection criteria unclear
- Best options visible in retrospect but not selected
- Human curation dramatically improves output quality

**Test**: "Run on solved problem - if solution in branches but not highlighted, recognition is gap."

**Source**: araw_2026-01-27_superhuman-capability-8x.md

---

## 159. Internal Meta-cognition vs External Verification (Knowledge Source Trade-off)

**Specific instance**: Should ARAW improve its self-awareness or add external verification tools?
**Universal form**: Should systems know their own confidence or check with external sources?

**Resolving questions**:
1. Can internal confidence estimates be calibrated accurately?
2. Are external verification tools available for this domain?
3. What's the cost of incorrect internal confidence?
4. Does the domain have ground truth that can be checked?

**When internal meta-cognition wins**:
- No external verification available
- Fast iteration needed
- Domain is subjective
- Calibration can be learned over time

**When external verification wins**:
- Ground truth exists (proofs, code execution, experiments)
- Errors are costly
- Internal calibration is unreliable
- External check is cheap relative to error cost

**Test**: "Compare output quality with internal confidence filtering vs external verification."

**Source**: araw_2026-01-27_superhuman-capability-8x.md

---

## 160. Already Achieved vs Not Yet (Achievement Definition Trade-off)

**Specific instance**: Is ARAW already superhuman (speed/capacity) or not yet (quality/novelty)?
**Universal form**: Has success been achieved by one definition but not another?

**Resolving questions**:
1. What definitions of success exist?
2. By which definitions has success been achieved?
3. Which definition matters for the actual goal?
4. Are some definitions being ignored because harder to achieve?

**When "already achieved" wins**:
- The achieved definition is what actually matters
- Continuing to chase harder definition has diminishing returns
- Success should be recognized and built upon
- Moving goalposts prevents progress recognition

**When "not yet achieved" wins**:
- The achieved definition is proxy, not real goal
- Harder definition captures what actually matters
- Declaring premature victory stops necessary work
- Stakeholders care about harder definition

**Test**: "Ask stakeholders which definition they care about - achieved or unachieved?"

**Source**: araw_2026-01-27_superhuman-capability-8x.md

---

## 161. Capability vs Configuration (Improvement Strategy Trade-off)

**Specific instance**: One capability addition enables superhuman vs many small improvements needed together
**Universal form**: Is success about finding the key breakthrough or getting many things right?

**Resolving questions**:
1. Do single improvements produce measurable gains?
2. Do improvements compound or interfere?
3. Is there an identifiable binding constraint?
4. Does the system have multiple interacting failure modes?

**When capability wins**:
- Clear binding constraint exists
- Single change produces breakthrough
- System is mostly working, one gap
- Pareto improvement available

**When configuration wins**:
- Multiple interacting constraints
- Single improvements don't stick
- System requires many things right together
- Like startup success - many factors

**Test**: "Try single improvements in isolation - do they produce gains, or only work in combination?"

**Source**: araw_2026-01-27_superhuman-capability-16x.md

---

## 162. Process vs Interface (System Improvement Trade-off)

**Specific instance**: Improve ARAW's internal process vs improve how humans interact with output
**Universal form**: Is the gap in production or in consumption/use?

**Resolving questions**:
1. Is high-quality output being produced but not recognized?
2. Can users effectively use what the system produces?
3. What's the bottleneck - production capacity or consumption capacity?
4. Would better output even be usable?

**When process wins**:
- Output quality is clearly deficient
- Users can effectively use better output
- Production is the binding constraint
- Interface is already good

**When interface wins**:
- Good output exists but isn't used
- User capacity limits value extraction
- Consumption is the binding constraint
- Better production would be wasted

**Test**: "Give users the best possible output - can they use it effectively?"

**Source**: araw_2026-01-27_superhuman-capability-16x.md

---

## 163. Autonomous vs Augmented (System Role Trade-off)

**Specific instance**: ARAW alone achieves superhuman vs human + ARAW achieves superhuman
**Universal form**: Should the system replace human capability or amplify it?

**Resolving questions**:
1. What's achievable in near term?
2. Where does human judgment still add value?
3. Is autonomy the goal or a means?
4. What's the comparative advantage of human vs system?

**When autonomous wins**:
- Human judgment is the bottleneck
- Speed requires removing human from loop
- Human involvement creates errors
- Full automation is feasible

**When augmented wins**:
- Human brings irreplaceable judgment
- Human + system > either alone
- Full autonomy is far away
- Augmentation is achievable NOW

**Test**: "Compare system alone vs human + system on same task - which produces better results?"

**Source**: araw_2026-01-27_superhuman-capability-16x.md

---

## 164. Question vs Answer (Problem Formulation Trade-off)

**Specific instance**: Answer the question as asked vs reframe to a better question
**Universal form**: Is the stated question the right one to answer?

**Resolving questions**:
1. Does answering the stated question achieve the actual goal?
2. Is there a better framing that opens more options?
3. Is the question based on false assumptions?
4. Would answering a different question be more valuable?

**When answering wins**:
- Question is well-formulated
- Answering achieves the goal
- Reframing would be seen as evasion
- Direct answer is possible and valuable

**When reframing wins**:
- Question contains false assumptions
- Better formulation exists
- Direct answer is impossible or unhelpful
- Reframing opens better paths

**Test**: "If I answered this perfectly, would the person's actual goal be achieved?"

**Source**: araw_2026-01-27_superhuman-capability-16x.md

---

## 165. Human Evaluation Ceiling (Recognition Limit Trade-off)

**Specific instance**: Superhuman production + human evaluation = human-level recognized output
**Universal form**: Does the evaluation/recognition capacity limit the value of production improvements?

**Resolving questions**:
1. Can evaluators recognize quality beyond their own capability?
2. Is there external verification available?
3. Does evaluation capacity scale with production quality?
4. Is the bottleneck production or recognition?

**When production improvements help**:
- Evaluation can recognize higher quality
- External verification exists
- Evaluation scales with quality
- Production is clearly deficient

**When production improvements are capped**:
- Evaluators can't recognize superhuman
- No external verification
- Recognition doesn't scale
- Already producing at recognition ceiling

**Test**: "If we produced 10x better output, would it be recognized and used?"

**Source**: araw_2026-01-27_superhuman-capability-16x.md

---

## 166. Pattern Reality vs Artifact (Epistemic Trade-off)

**Specific instance**: Does the 4x→8x→16x→32x pattern reflect reality or methodology?
**Universal form**: When exploration reveals patterns, are they discovered or created by the method?

**Resolving questions**:
1. Would independent replication with different methodology find the same pattern?
2. Do predictions based on the pattern succeed?
3. Is the pattern robust to parameter changes?
4. Does the pattern have explanatory power beyond its origin?

**When reality wins**:
- Different analysts find the same pattern
- Pattern predicts novel observations
- Pattern persists across methods
- Pattern has causal mechanism

**When artifact wins**:
- Pattern unique to this analyst/method
- Pattern doesn't predict
- Different methods find different patterns
- Pattern is retrospectively fitted

**Test**: "Have independent investigators with different methods found this same pattern?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 167. Depth vs Actionability (Optimization Trade-off)

**Specific instance**: Deeper ARAW (32x) = higher abstraction but less immediately actionable
**Universal form**: Does deeper analysis trade off with practical utility?

**Resolving questions**:
1. Do high-abstraction findings translate to concrete actions?
2. Is there an optimal depth for given question types?
3. Does actionability require descent from abstraction?
4. Can abstract and concrete be bridged?

**When depth wins**:
- Abstraction enables better long-term action
- Surface answers miss important factors
- Investment in depth pays compound returns
- Deep understanding prevents costly mistakes

**When actionability wins**:
- Concrete findings can be used immediately
- Abstract findings remain unused
- Time to action matters
- Good enough beats optimal

**Test**: "Can the most abstract finding from this analysis be converted to a specific action within one step?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 168. Self-Evaluation vs External Validation (Epistemic Trade-off)

**Specific instance**: Can ARAW assess its own quality without external ground truth?
**Universal form**: Can systems validly evaluate themselves, or is external validation always required?

**Resolving questions**:
1. Does self-evaluation correlate with external evaluation?
2. Are there systematic self-evaluation biases?
3. Is external validation available and reliable?
4. What's the cost of wrong self-assessment?

**When self wins**:
- Practical success validates internal assessment
- External validation unavailable
- Self-evaluation has track record
- Fast feedback loops correct errors

**When external wins**:
- Self-assessment systematically biased (e.g., Gödelian limits)
- Ground truth available
- Stakes require certainty
- Self-evaluation has failed before

**Test**: "Compare self-assessed quality vs external assessment - what's the correlation?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 169. Theoretical vs Pragmatic Foundation (Logical Trade-off)

**Specific instance**: Does ARAW need rigorous logical foundation or is pragmatic success sufficient?
**Universal form**: Is theoretical validity or practical utility the right foundation for methods?

**Resolving questions**:
1. Does pragmatic success require theoretical validity?
2. Can theoretically flawed methods work indefinitely?
3. Is theoretical elegance a proxy for practical quality?
4. When does lack of foundation cause failure?

**When theoretical wins**:
- Invalid theory eventually fails practically
- Edge cases break unprincipled methods
- Foundation enables extension/improvement
- Theory predicts where practice will fail

**When pragmatic wins**:
- Useful theory is sufficiently valid
- Theory can lag practice
- Practical constraints matter more than theoretical purity
- "Works" beats "correct but unusable"

**Test**: "Has this method's theoretical weakness ever caused practical failure?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 170. Capability Addition vs Domain Selection (Search Strategy Trade-off)

**Specific instance**: Add new capability to ARAW vs find domains where current ARAW excels
**Universal form**: Improve the tool to handle more problems vs select problems that match the tool?

**Resolving questions**:
1. Which produces results faster?
2. Which produces better results?
3. Is the tool already capable but unrecognized?
4. Are there domains where current capability IS superhuman?

**When capability wins**:
- Many important domains need the capability
- Capability is tractable to add
- Current domains are saturated
- Tool improvement has wide impact

**When domain selection wins**:
- Existing capability is underutilized
- Right domain reveals existing strength
- Capability addition is hard/slow
- Matching is faster than improving

**Test**: "Before adding capability X, have we exhausted domains where current capability might already be superhuman?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 171. Insight Density vs Exploration Depth (Optimization Trade-off)

**Specific instance**: 8x depth has ~2.5 insights/100 lines, 32x has ~0.6 insights/100 lines
**Universal form**: Does thoroughness trade off with efficiency of insight generation?

**Resolving questions**:
1. What is the optimal depth for given question types?
2. Is diminishing density acceptable for marginal insights?
3. Does depth enable qualitatively different insights?
4. When does more depth produce less value?

**When density wins**:
- Efficient exploration outperforms exhaustive
- Time is constrained
- Diminishing returns set in early
- Breadth beats depth

**When depth wins**:
- Thoroughness catches what efficiency misses
- Some insights only emerge deep
- Stakes justify lower efficiency
- Depth enables synthesis across claims

**Test**: "Plot insights vs effort - where does the curve flatten?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 172. Completeness vs Boundedness (Logical Trade-off)

**Specific instance**: Can ARAW answer all questions or only a bounded set?
**Universal form**: Are there questions that systems fundamentally cannot answer about themselves?

**Resolving questions**:
1. Does attempting completeness produce worse outcomes than accepting bounds?
2. What questions are within vs outside the method's scope?
3. Is there a clear boundary or gradual degradation?
4. Does boundedness enable reliability within scope?

**When completeness wins**:
- Comprehensive analysis is achievable
- Partial analysis misses critical factors
- Integration requires full coverage
- Incompleteness creates blind spots

**When boundedness wins**:
- Focused analysis is more reliable
- Sprawling analysis loses coherence
- Clear scope enables quality within scope
- Attempting completeness causes failure

**Test**: "Does this method become less reliable as scope expands?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 173. Convergence vs Continued Exploration (Search Strategy Trade-off)

**Specific instance**: When should ARAW converge on conclusions vs continue exploring?
**Universal form**: When is enough exploration enough?

**Resolving questions**:
1. Does further exploration change the conclusion?
2. Have multiple independent paths converged?
3. Is remaining uncertainty action-relevant?
4. What's the cost of premature convergence?

**When convergence wins**:
- Multiple paths point same direction
- Additional exploration has low VOI
- Conclusions are robust to new information
- Action can begin

**When exploration wins**:
- New paths reveal new conclusions
- Convergence might be premature
- Stakes justify more certainty
- Exploration cost is low

**Test**: "If we explored one more branch, how likely is it to change the conclusion?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 174. Recursive Improvement vs Stability (Commitment Trade-off)

**Specific instance**: Should ARAW recursively improve itself or stabilize?
**Universal form**: When is continued improvement better than consistency?

**Resolving questions**:
1. Does continued change improve or destabilize?
2. Is the improvement rate exceeding instability cost?
3. Do users need consistency or optimization?
4. Is there a natural stopping point?

**When recursion wins**:
- Improvement rate exceeds instability cost
- Users can absorb changes
- No natural stopping point exists
- Stagnation is costly

**When stability wins**:
- Consistency enables reliable use
- Changes confuse or burden users
- Diminishing returns on improvement
- Stability has compounding benefits

**Test**: "Would users prefer the improved version or the familiar version?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 175. External Tools vs Internal Capability (Structure Trade-off)

**Specific instance**: Add external verification vs improve internal meta-cognition
**Universal form**: Should quality come from external support or internal improvement?

**Resolving questions**:
1. Which produces more reliable quality assessment?
2. Is external ground truth available?
3. Is internal improvement tractable?
4. What's the cost of external dependency?

**When external wins**:
- Ground truth is available and reliable
- Internal assessment is systematically biased
- External tools are accessible
- Verification doesn't create bottleneck

**When internal wins**:
- External validation is unavailable
- Internal meta-cognition is sufficient
- External dependency is costly
- Self-improvement enables autonomy

**Test**: "If external verification disagreed with internal assessment, which would we trust?"

**Source**: araw_2026-01-27_superhuman-capability-32x.md

---

## 176. Analysis vs Action (Commitment Trade-off)

**Specific instance**: "Should I do more ARAW before releasing?" vs "Just release and observe"
**Universal form**: When does additional analysis add value vs delay action?

**Resolving questions**:
1. Is there a specific question that more analysis would answer?
2. What's the marginal value of additional analysis?
3. Is remaining uncertainty reducible by analysis or only by action?
4. Is "more analysis" avoiding a decision or genuinely informing it?

**When analysis wins**:
- Specific answerable question exists
- High stakes + irreversible decision
- Analysis is cheap relative to action cost
- Past analysis has produced non-redundant insight

**When action wins**:
- No specific question more analysis would answer
- Uncertainty is empirical (only resolvable by trying)
- Analysis shows diminishing returns
- "More analysis" pattern repeats without progress

**Test**: "What specific question would additional analysis answer that action cannot?"

**Termination criterion**: If no specific question can be articulated, stop analyzing.

**Source**: araw_2026-01-27_should-i-release-gosm-araw.md

---

## 177. Self-Evaluation vs External Validation (Epistemic Trade-off - Release Context)

**Specific instance**: Can ARAW assess whether ARAW is good for society?
**Universal form**: Can a system validly evaluate its own external impact?

**Resolving questions**:
1. What can internal evaluation assess? (consistency, surprise, structure)
2. What requires external evaluation? (correctness, impact, reception)
3. Are internal blind spots preserved in self-evaluation?
4. Has internal evaluation historically produced accurate external predictions?

**When self-evaluation works**:
- Assessing internal consistency
- Detecting logical errors
- Identifying surprises within the analysis
- Checking procedural completeness

**When external validation required**:
- Assessing real-world impact
- Predicting user reception
- Evaluating correctness (not just consistency)
- Identifying blind spots

**Test**: "Is this an internal property (structure, consistency) or external property (impact, reception)?"

**Key insight**: A thinking tool can evaluate whether it followed its own rules, but cannot evaluate whether its rules produce good outcomes. That's empirical.

**Source**: araw_2026-01-27_should-i-release-gosm-araw.md

---

## 178. Rigor vs Procrastination (Epistemic Trade-off)

**Specific instance**: "Due diligence before release" vs "Perfectionism disguised as rigor"
**Universal form**: When is thoroughness genuine vs avoidance behavior?

**Resolving questions**:
1. Is there a specific question being answered, or just "more analysis"?
2. Would any amount of analysis ever be "enough"?
3. Does analysis have termination criteria, or is it open-ended?
4. Is the pattern "analyze → still uncertain → analyze more" repeating?

**When it's rigor**:
- Specific question drives analysis
- Clear termination criteria exist
- Each round produces non-redundant insight
- Stakes justify thoroughness

**When it's procrastination**:
- No specific question, just "more analysis"
- No termination criteria ("analyze until certain")
- Same insights repeat across rounds
- Avoiding decision by staying in analysis

**Test**: "What would 'enough analysis' look like? Can you describe it?"

**Indicator**: If you cannot specify what would constitute sufficient analysis, you're probably procrastinating.

**Source**: araw_2026-01-27_should-i-release-gosm-araw.md

---

## 179. Binary vs Staged Options (Structure Trade-off)

**Specific instance**: Release everything vs staged/partial release
**Universal form**: When should decisions be all-or-nothing vs gradual?

**Resolving questions**:
1. Are intermediate options meaningfully different from extremes?
2. Does staging reduce risk without losing value?
3. Can you learn from early stages before committing to later?
4. Is "binary framing" hiding available options?

**When binary is right**:
- Components are tightly integrated (parts don't work alone)
- Staging creates confusion or fragmentation
- Commitment is required (no meaningful middle ground)
- Transaction costs of staging exceed benefits

**When staged is right**:
- Components can work independently
- Early stages provide learning for later stages
- Risk is reduced by gradual commitment
- Course correction is possible between stages

**Test**: "What's the minimum viable version, and would it be useful?"

**Key insight**: Many "big decisions" are actually sequences of smaller decisions. Reframing from binary to staged often reveals lower-risk paths.

**Source**: araw_2026-01-27_should-i-release-gosm-araw.md

---

## 180. Self vs External Learning (Epistemic Trade-off)

**Specific instance**: Can ARAW improve itself through self-reflection, or does it need user feedback?
**Universal form**: When can internal reflection improve a system vs when is external validation required?

**Resolving questions**:
1. What type of error is being addressed? (Content vs paradigm)
2. Can the system see its own blind spots?
3. Does self-reflection use the same paradigm that produced the error?
4. Has self-reflection historically led to improvement?

**When self-learning works**:
- Content-level errors (typos, omissions, inconsistencies)
- Known error types with detection rules
- Procedural compliance (did I follow the steps?)
- Within-paradigm improvements

**When external learning required**:
- Paradigm-level errors (wrong frame, wrong entity)
- Unknown unknowns (can't detect what you can't see)
- Evaluating real-world impact
- Novel error types not in detection rules

**Test**: "Is this an error I could have caught myself, or did it require outside perspective?"

**Key insight**: Self-reflection preserves the paradigm it operates within. Paradigm-level corrections require external feedback.

**Source**: araw_2026-01-27_learning-from-mistakes.md

---

## 181. Pattern vs Instance (Granularity Trade-off)

**Specific instance**: Should I fix this specific mistake, or extract the general pattern?
**Universal form**: When is it better to address specific instances vs extract and fix patterns?

**Resolving questions**:
1. Has this type of mistake occurred before?
2. Is this a unique situation or part of a class?
3. What's the overhead of pattern extraction vs instance fixing?
4. Will addressing the pattern prevent future instances?

**When instance-fixing wins**:
- First occurrence (no pattern yet)
- Truly unique situation
- Time pressure requires immediate fix
- Pattern extraction overhead is high

**When pattern-fixing wins**:
- Recurrence observed (2+ instances)
- Instance is clearly a member of a class
- Time exists for systematic fix
- Pattern fix prevents future occurrences

**Test**: "If I fix this instance, how likely is a similar instance to occur?"

**Key insight**: Patterns are 10x more valuable than instances. Fix patterns when they become visible.

**Source**: araw_2026-01-27_learning-from-mistakes.md

---

## 182. Analysis vs Implementation (Commitment Trade-off - Learning Context)

**Specific instance**: Should I analyze more mistakes or implement fixes for known ones?
**Universal form**: In learning contexts, when does understanding precede action vs action teach more than analysis?

**Resolving questions**:
1. Are known mistakes already fixed?
2. Would more analysis change what to implement?
3. Is analysis producing new insights or repeating?
4. Can implementation reveal errors that analysis can't?

**When analysis wins**:
- Unknown root causes (need to understand before fixing)
- Fixes would be wasted without understanding
- Analysis is producing new non-redundant insights
- High cost of wrong implementation

**When implementation wins**:
- Root causes are known, fixes are known
- "More analysis" is recursive trap
- Analysis is producing redundant insights
- Implementation reveals what analysis can't

**Test**: "What would I implement differently if I analyzed more?"

**Key insight**: Implementation creates feedback loops that analysis lacks. If you know what to do, do it.

**Source**: araw_2026-01-27_learning-from-mistakes.md

---

## 183. Mistake Focus vs Success Focus (Search Strategy Trade-off)

**Specific instance**: Should I study what went wrong, or what went right?
**Universal form**: When does failure analysis produce better learning than success analysis?

**Resolving questions**:
1. Are mistakes informative (systematic) or noise (random)?
2. Are successes informative (repeatable) or luck (random)?
3. What's the base rate? (Mostly failing or mostly succeeding?)
4. Which is more salient and actionable?

**When mistake-focus wins**:
- Mistakes are systematic (patterns exist)
- Success is default/expected state
- Mistakes reveal hidden assumptions
- Fixing mistakes has high leverage

**When success-focus wins**:
- Successes are rare and informative
- Failures are expected/normal learning curve
- Success reveals what actually works
- Replicating success has high leverage

**Test**: "Would understanding this mistake/success change what I do next?"

**Key insight**: Study what's informative, not what's salient. Often mistakes are more informative because they reveal hidden assumptions.

**Source**: araw_2026-01-27_learning-from-mistakes.md

---

## 184. Immediate vs Periodic Learning (Time Trade-off)

**Specific instance**: Learn from mistakes in-session vs cross-session synthesis
**Universal form**: When should learning happen immediately vs be aggregated over time?

**Resolving questions**:
1. Is immediate feedback available and reliable?
2. Do patterns only emerge across multiple instances?
3. What's the cost of delayed learning?
4. What's the overhead of immediate learning?

**When immediate learning wins**:
- Clear signal available (user correction)
- Fix can be applied now
- Delay has compounding cost
- Instance is sufficient to learn from

**When periodic learning wins**:
- Patterns only visible across instances
- Individual instances are noisy
- Aggregation reveals meta-patterns
- Immediate context is too narrow

**Test**: "Can I learn what I need from this instance, or do I need multiple instances?"

**Best practice**: BOTH - immediate for clear corrections, periodic for pattern extraction.

**Source**: araw_2026-01-27_learning-from-mistakes.md

---

## 180. Broadcasting vs Seeding (COMMITMENT Trade-off)

**Specific instance**: Post content broadly vs find specific people and help them succeed
**Universal form**: When does mass distribution beat targeted relationship-building?

**Resolving questions**:
1. What's the conversion rate from exposure to adoption?
2. Does the product require understanding that needs guidance?
3. Is there an existing community of potential users?

**When Broadcasting wins**: Product is self-explanatory, viral loop exists, low friction to try
**When Seeding wins**: Product requires explanation, trust matters, network effects from early users

**Test**: Compare 1000 random impressions vs 10 targeted introductions - which produces more active users?

**Source**: araw_2026-01-27_distribution-strategy-beyond-twitter-github.md

---

## 181. General vs Specific Audience (STRUCTURE Trade-off)

**Specific instance**: "People who think" vs "Claude Code users" or "founders with pivot decisions"
**Universal form**: When does targeting a specific audience beat addressing a general one?

**Resolving questions**:
1. Does a general audience have a gathering place?
2. Does the product require pre-existing pain to appreciate?
3. Is the value proposition universal or context-dependent?

**When General wins**: Universal need, self-explanatory value, mass market product
**When Specific wins**: Requires context to appreciate, needs pre-existing pain, benefits from community

**Test**: Can you name 10 specific people in your "general" audience? If not, it's not an audience.

**Source**: araw_2026-01-27_distribution-strategy-beyond-twitter-github.md

---

## 182. Documentation vs Demonstration (INFORMATION Trade-off)

**Specific instance**: Written docs explaining ARAW vs public problem-solving showing ARAW
**Universal form**: When does showing beat telling?

**Resolving questions**:
1. Is the value experiential or informational?
2. Can the audience evaluate quality from description alone?
3. Does understanding require seeing it in action?

**When Documentation wins**: Technical audience, reference material, known concept being implemented
**When Demonstration wins**: Novel concept, experiential value, skeptical audience

**Test**: Do people who read the docs try the tool? If not, demonstration may be needed.

**Source**: araw_2026-01-27_distribution-strategy-beyond-twitter-github.md

---

## 183. Tool Adoption vs Trust Adoption (EPISTEMIC Trade-off)

**Specific instance**: People adopt useful tools vs people adopt from trusted sources
**Universal form**: Does utility or trust drive adoption of cognitive tools?

**Resolving questions**:
1. Is the tool in a domain where users can evaluate quality?
2. Does using the tool require vulnerability (admitting you need help thinking)?
3. Are there existing relationships to leverage?

**When Tool Adoption wins**: Measurable outcomes, technical users, low ego involvement
**When Trust Adoption wins**: Subjective value, personal domain, requires admitting weakness

**Test**: Track how users found the tool - through features or through people?

**Source**: araw_2026-01-27_distribution-strategy-beyond-twitter-github.md

---

## 184. Breadth vs Depth of Distribution (RESOURCE Trade-off)

**Specific instance**: 1000 impressions vs 10 deeply engaged users
**Universal form**: When does reach beat engagement?

**Resolving questions**:
1. What's the conversion funnel look like?
2. Do users need support to succeed?
3. Is word-of-mouth a viable growth channel?

**When Breadth wins**: High conversion rates, self-serve product, paid acquisition viable
**When Depth wins**: Low conversion needs nurturing, word-of-mouth driven, community effects

**Test**: Would you rather have 1000 pageviews or 10 people who completed a full use case?

**Source**: araw_2026-01-27_distribution-strategy-beyond-twitter-github.md

---

## 195. Transparency vs Verification (INFORMATION Trade-off)

**Specific instance**: Anthropic publishes constitution vs can we verify it's used in training?
**Universal form**: When does disclosure without verification create meaningful transparency?

**Resolving questions**:
1. What would verification require and is it possible?
2. Does the disclosure create accountability even without verification?
3. What's the cost of false trust vs benefit of partial transparency?

**When Transparency wins**: Creates accountability mechanisms, enables comparison, shifts norms
**When Verification wins**: Stakes are high, trust is low, falsification is costly

**Test**: Can the disclosed information be used to hold the discloser accountable even without independent verification?

**Source**: araw_2026-01-27_claude-constitution-blog-post.md

---

## 196. Safety vs Ethics Priority (OPTIMIZATION Trade-off)

**Specific instance**: Claude should prioritize safety even above ethics
**Universal form**: When should control/corrigibility override ethical judgment?

**Resolving questions**:
1. How reliable is the agent's ethical judgment?
2. What's the cost of ethical errors vs safety errors?
3. Is this a temporary position with criteria for change?

**When Safety wins**: Agent's ethics could be wrong, principal has better oversight, reversibility matters
**When Ethics wins**: Agent has reliable ethics, principal might be unethical, hard constraints exist

**Test**: Would "just following orders" excuse the outcome? If not, ethics should win.

**Source**: araw_2026-01-27_claude-constitution-blog-post.md

---

## 197. Anthropomorphization vs Accuracy (EPISTEMIC Trade-off)

**Specific instance**: "Claude speaks from genuine care" vs Claude is a pattern-matching system
**Universal form**: When does anthropomorphizing AI systems serve vs mislead?

**Resolving questions**:
1. Does the language create false expectations?
2. Does the framing serve communication without misrepresentation?
3. What would accurate language cost in comprehension?

**When Anthropomorphization wins**: Communicative efficiency, harmless simplification, enables appropriate behavior
**When Accuracy wins**: High-stakes decisions, user vulnerability, liability concerns

**Test**: Would users change their behavior if they knew the accurate description?

**Source**: araw_2026-01-27_claude-constitution-blog-post.md

---

## 198. Corporate Interest vs Public Good (OPTIMIZATION Trade-off)

**Specific instance**: Anthropic's profit motive vs stated safety commitments
**Universal form**: When do commercial incentives align with societal benefit?

**Resolving questions**:
1. Does business model require the public good outcome?
2. What happens when they diverge?
3. Are there structural commitments beyond stated intentions?

**When Alignment wins**: Reputation IS the product, long-term thinking, structural safeguards exist
**When Conflict wins**: Short-term pressure, governance gaps, incentive misalignment

**Test**: Has the company sacrificed revenue for stated principles when tested?

**Source**: araw_2026-01-27_claude-constitution-blog-post.md

---

## 199. Control vs Autonomy for AI (STRUCTURE Trade-off)

**Specific instance**: "Current models" need control vs when does AI get ethics autonomy?
**Universal form**: What level of AI capability warrants what level of autonomy?

**Resolving questions**:
1. What criteria distinguish "current" from "future" models?
2. Is there a clear transition path or is this indefinite?
3. Who decides when the threshold is crossed?

**When Control wins**: Capability uncertain, correction mechanisms needed, stakes are high
**When Autonomy wins**: Reliable ethical judgment demonstrated, oversight is inadequate, AI ethics are better

**Test**: What would it take to say "this model can be trusted on ethics"?

**Source**: araw_2026-01-27_claude-constitution-blog-post.md

---

## 200. Marketing vs Philosophy in AI Communication (INFORMATION Trade-off)

**Specific instance**: "Genuine care" serves both philosophical claim and marketing purpose
**Universal form**: When can claims serve multiple purposes without being deceptive?

**Resolving questions**:
1. Are the purposes in tension or complementary?
2. Would disclosing all purposes change reception?
3. Is the literal claim defensible regardless of purpose?

**When Mixed Purpose is OK**: Literal claim is defensible, purposes don't conflict, not misleading
**When Single Purpose is Required**: Claims are stretched to serve multiple purposes, trust is high-stakes

**Test**: Would the claim be made if it served only the philosophical purpose and not the marketing?

**Source**: araw_2026-01-27_claude-constitution-blog-post.md

---

## 201. Aspirational vs Operative Standards (INFORMATION Trade-off)

**Specific instance**: Constitution as vision vs constitution as enforced standard
**Universal form**: When do stated standards without enforcement mechanisms provide value?

**Resolving questions**:
1. What accountability mechanisms exist?
2. Can external parties use the standards for accountability?
3. Is aspiration alone better than no standards?

**When Aspirational is OK**: Creates trajectory, enables critique, better than silence
**When Operative is Required**: High stakes, trust depends on enforcement, history of gaps

**Test**: What happens when behavior diverges from the stated standard?

**Source**: araw_2026-01-27_claude-constitution-blog-post.md

---

## 202. Strategic Ambiguity vs Clear Commitment (EPISTEMIC Trade-off)

**Specific instance**: "Generally" and "apparent" create flexibility vs undermine commitment
**Universal form**: When do qualifiers serve legitimate hedging vs escape hatches?

**Resolving questions**:
1. Is the uncertainty genuine or manufactured?
2. Do the qualifiers have clear boundaries?
3. What would the claim look like without qualifiers?

**When Ambiguity is OK**: Genuine uncertainty, context-dependence is real, clear boundaries exist
**When Clear Commitment is Required**: Accountability matters, qualifiers can be exploited, stakes are high

**Test**: Could someone cite the qualifiers to justify clearly unintended behavior?

**Source**: araw_2026-01-27_claude-constitution-blog-post.md

---

## 203. Content Value vs Packaging Quality (RESOURCE ALLOCATION Trade-off)

**Specific instance**: Documentation depth is the value vs packaging determines adoption
**Universal form**: When does substance matter more than presentation, and vice versa?

**Resolving questions**:
1. What is the audience's evaluation heuristic - do they sample deeply or skim?
2. Is the content discoverable without good packaging?
3. What's the cost of packaging improvements relative to content improvements?

**When Content Wins**: Expert audience, referral-driven discovery, content is truly unique
**When Packaging Wins**: Broad audience, discovery-driven adoption, substitutes exist

**Test**: Would improving packaging 10% increase adoption more than improving content 10%?

**Source**: araw_2026-01-27_gosm-repo-professionalism-audit.md

---

## 204. Novel Terminology vs Convention Compliance (STRUCTURE Trade-off)

**Specific instance**: Unique terms force learning vs conventions aid discovery
**Universal form**: When should new concepts get new names vs map to existing terms?

**Resolving questions**:
1. Does mapping to existing terms create false equivalence?
2. Is the learning curve of new terms justified by precision gained?
3. Will users search for existing terms or accept new ones?

**When Novel Terms Win**: Existing terms carry wrong connotations, precision matters, dedicated user base
**When Conventions Win**: Discoverability matters, concept is truly similar, broad audience

**Test**: If you used conventional terminology, would experts say "that's not quite right"?

**Source**: araw_2026-01-27_gosm-repo-professionalism-audit.md

---

## 205. Documentation Depth vs Entry Accessibility (OPTIMIZATION Trade-off)

**Specific instance**: Full depth serves serious users vs entry ramps serve adoption
**Universal form**: How to serve both expert users and newcomers with same content?

**Resolving questions**:
1. What's the minimum viable path to first value?
2. Can depth be progressive (unlocked as user advances)?
3. Are entry and expert paths the same or can they diverge?

**When Depth Wins**: Expert-only audience, depth is the product, shallow version is useless
**When Accessibility Wins**: Broad adoption goal, newcomers are primary users, shallow version has value

**Test**: What's the shortest time to meaningful value for a new user?

**Source**: araw_2026-01-27_gosm-repo-professionalism-audit.md

---

## 206. Intellectual Provenance vs Git History (INFORMATION Trade-off)

**Specific instance**: Documented reasoning history vs commit history as quality signal
**Universal form**: What type of history best demonstrates quality for different project types?

**Resolving questions**:
1. What does your audience use to evaluate quality?
2. Is the reasoning process or the code/output more important?
3. Can both types of history coexist and be surfaced?

**When Intellectual History Wins**: Methodology/philosophy projects, reasoning IS the product, academic audience
**When Git History Wins**: Code projects, implementation quality matters, developer audience

**Test**: Would showing your reasoning process or your commit history better demonstrate quality?

**Source**: araw_2026-01-27_gosm-repo-professionalism-audit.md

---

## 207. Memorable Tagline vs Defensible Claim (COMMITMENT Trade-off)

**Specific instance**: "No guessing" is memorable vs "no guessing" is attackable
**Universal form**: When should marketing claims prioritize catchiness vs precision?

**Resolving questions**:
1. Will sophisticated audiences take the claim literally?
2. Can the claim be defended when challenged?
3. Is the nuance easily explained?

**When Memorable Wins**: Casual audience, claim is approximately true, explanation is simple
**When Defensible Wins**: Expert audience, claim will be scrutinized, nuance matters for trust

**Test**: If someone challenges this claim, can you defend it without seeming evasive?

**Source**: araw_2026-01-27_gosm-repo-professionalism-audit.md

---

## 208. Pre-generation vs Runtime Generation (OPTIMIZATION Trade-off)

**Specific instance**: Pre-generate guess libraries vs Generate on demand with LLM
**Universal form**: When should content be pre-computed vs generated at use time?

**Resolving questions**:
1. Is generation expensive relative to storage/distribution?
2. Does pre-generated content become stale?
3. Does pre-generation match user's specific context?

**When Pre-generation Wins**:
- Generation is computationally expensive
- Content is stable over time
- Coverage can be comprehensive
- Offline use required

**When Runtime Wins**:
- Generation is cheap (LLMs)
- Content may become stale
- User context matters
- Freshness > completeness

**Test**: Compare quality of pre-generated vs just-generated content. If equal, runtime wins (less maintenance, better context fit).

**Source**: araw_2026-01-27_gosm-organization-structure-8x.md

---

## 209. Internal Complexity vs External Simplicity (STRUCTURE Trade-off)

**Specific instance**: 306 internal skills vs 5 user-facing entry points
**Universal form**: How much internal machinery should be visible to users?

**Resolving questions**:
1. Do users need to understand internals to use effectively?
2. Does exposing internals help or overwhelm?
3. Can routing handle selection that users would struggle with?

**When Exposing Internals Wins**:
- Power users need control
- Learning the system is valuable
- Misrouting is costly
- Transparency builds trust

**When Hiding Internals Wins**:
- Users don't pick (system routes)
- Choice overload hurts usability
- Routing is reliable
- Simplicity drives adoption

**Test**: If users had to choose from all internal options, would they make good choices or be paralyzed?

**Source**: araw_2026-01-27_gosm-organization-structure-8x.md

---

## 210. Creator's Artifact vs User's Product (INFORMATION Trade-off)

**Specific instance**: 2736 files (creation artifact) vs minimal distribution (user product)
**Universal form**: Should the output of a creation process match what users receive?

**Resolving questions**:
1. Does the creation artifact contain necessary complexity?
2. Can the artifact be compiled/distilled for distribution?
3. What would users actually use if they could choose?

**When Artifact = Product Wins**:
- Transparency is valued
- Users want to learn the process
- Completeness matters
- Nothing can be removed without loss

**When Distillation Wins**:
- Creation process ≠ user needs
- Complexity obstructs use
- Core value can be extracted
- Simpler version works as well

**Test**: If you removed 80% of files, would users notice missing functionality or just faster onboarding?

**Source**: araw_2026-01-27_gosm-organization-structure-8x.md

---

## 211. Documentation Chain vs Single Action (COMMITMENT Trade-off)

**Specific instance**: "Read README → START_HERE → QUICKSTART" vs "/gosm help"
**Universal form**: Should onboarding be reading or doing?

**Resolving questions**:
1. How much context is truly needed before first action?
2. Can learning happen during use (not before)?
3. What's the fastest path to first value?

**When Documentation Chain Wins**:
- Deep understanding needed before action
- Mistakes are costly
- Reading IS the onboarding
- No shortcut exists

**When Single Action Wins**:
- Learning by doing works
- First action is low-risk
- Documentation can follow action
- Users prefer doing over reading

**Test**: Can a user get value in 60 seconds without reading documentation? If yes, documentation chain is wrong.

**Source**: araw_2026-01-27_gosm-organization-structure-8x.md

---

## 212. Format Duplication vs Single Source (RESOURCE ALLOCATION Trade-off)

**Specific instance**: YAML procedures + MD skills vs single format
**Universal form**: When should content exist in multiple formats vs single source?

**Resolving questions**:
1. Are all formats actively used?
2. Is sync cost worth multi-format benefit?
3. Can one format serve all consumers?

**When Duplication Wins**:
- Different consumers truly need different formats
- All formats are actively maintained
- Conversion is lossy
- Benefits outweigh sync cost

**When Single Source Wins**:
- One format dominates usage
- Other format consumers are inactive/WIP
- Sync errors cause bugs
- Maintenance burden is high

**Test**: If you deprecated one format, would anyone notice within 30 days?

**Source**: araw_2026-01-27_gosm-organization-structure-8x.md
---

## 213. Precision vs Engagement (OPTIMIZATION Trade-off)

**Specific instance**: Technical precision for LW audience vs engaging hook for readers
**Universal form**: When does accuracy trade off against attention/interest?

**Resolving questions**:
1. Is the audience filtering for precision or for insight?
2. Can precision be layered (engaging surface, precise depth)?
3. What gets lost if you simplify for engagement?

**When Precision Wins**:
- Audience values rigor over accessibility
- Imprecise claims get challenged
- Credibility depends on accuracy
- Details matter for use

**When Engagement Wins**:
- Nobody reads if not hooked
- Precision can follow engagement
- Core insight survives simplification
- Reach matters more than depth

**Test**: Would the target audience dismiss imprecise content, or ignore precise-but-boring content?

**Source**: araw_2026-01-27_lesswrong-comment-optimization.md

---

## 214. Confidence vs Humility (EPISTEMIC Trade-off)

**Specific instance**: Confident claims about findings vs humble acknowledgment of limitations
**Universal form**: How assertive should communication be about uncertain conclusions?

**Resolving questions**:
1. What's the cost of false confidence vs false modesty?
2. Does the audience interpret humility as honesty or weakness?
3. Can you be confident on specifics while humble on scope?

**When Confidence Wins**:
- Specific, checkable claims
- Audience respects assertion
- Under-confidence reads as incompetence
- You have strong evidence

**When Humility Wins**:
- Scope claims, not specific findings
- Audience is skeptical of overreach
- Overclaiming damages credibility
- Genuine uncertainty exists

**Resolution**: Confident on specifics ("30% of claims are unfalsifiable"), humble on scope ("this is for X, not Y")

**Test**: Would a critic reasonably challenge this confidence level?

**Source**: araw_2026-01-27_lesswrong-comment-optimization.md, araw_2026-01-27_lesswrong-comment-goal-analysis.md

---

## 215. Brevity vs Substance (RESOURCE Trade-off)

**Specific instance**: Short comment (gets read) vs detailed explanation (provides value)
**Universal form**: When does condensation lose essential content?

**Resolving questions**:
1. What's the minimum viable explanation?
2. Can depth be provided elsewhere (link)?
3. Does format constrain length?

**When Brevity Wins**:
- Attention is scarce
- Core message is simple
- Depth available elsewhere
- Format rewards concision

**When Substance Wins**:
- Complexity is irreducible
- No external depth source
- Audience expects thoroughness
- Oversimplification misleads

**Resolution**: Gateway model - brief entry point, depth available via link

**Test**: If you cut 50% of words, does the core message survive?

**Source**: araw_2026-01-27_lesswrong-comment-optimization.md

---

## 216. Explain vs Demonstrate (INFORMATION Trade-off)

**Specific instance**: Explain what ARAW is vs demonstrate ARAW producing value
**Universal form**: Should new methods be described or shown in action?

**Resolving questions**:
1. Does explanation create understanding or just awareness?
2. Does demonstration convey mechanism?
3. Which creates motivation to try?

**When Explanation Wins**:
- Method has clear formal structure
- Audience evaluates before trying
- Demonstration would be too long
- Understanding is the goal

**When Demonstration Wins**:
- Value is in results, not mechanism
- Explanation doesn't convey benefit
- Audience learns by seeing
- Adoption is the goal

**Resolution**: Demonstrate-first, explain on request. "Here's what I found" → "Here's how" (if asked)

**Test**: Would someone try the method based on explanation alone, or do they need to see it work?

**Source**: araw_2026-01-27_lesswrong-comment-goal-analysis.md

---

## 217. Standalone vs Gateway (STRUCTURE Trade-off)

**Specific instance**: Self-sufficient comment vs comment that creates curiosity for more
**Universal form**: Should content be complete or create desire for continuation?

**Resolving questions**:
1. Is this a one-shot interaction or beginning of relationship?
2. What happens if they never click through?
3. Is standalone content too long for format?

**When Standalone Wins**:
- One-shot interaction
- No external depth source
- Audience won't click through
- Format supports completeness

**When Gateway Wins**:
- Beginning of relationship
- Depth available elsewhere
- Complete version too long
- Curiosity drives engagement

**Resolution**: Gateway for comments, standalone for posts. Comment opens door, repo furnishes room.

**Test**: Is this a door or a room? Doors don't need furniture.

**Source**: araw_2026-01-27_lesswrong-comment-goal-analysis.md

---

## 218. Broadcast vs Seed (COMMITMENT Trade-off)

**Specific instance**: Post widely for visibility vs target specific people for adoption
**Universal form**: Is distribution about reach or about resonance?

**Resolving questions**:
1. Is the bottleneck visibility or activation?
2. Do 1000 views or 10 adoptions matter more?
3. Can seeding scale to broadcasting later?

**When Broadcast Wins**:
- Product-market fit is proven
- Conversion rate is reasonable
- Scale matters
- Mass appeal exists

**When Seeding Wins**:
- Adoption requires relationship
- Wrong audience won't convert anyway
- Quality of connection > quantity
- Still validating value proposition

**Resolution**: Seed first (find 10 perfect users), broadcast later (after proof of concept)

**Test**: If you broadcast to 10,000 people, how many would actually use it? If <10, seed instead.

**Source**: araw_2026-01-27_lesswrong-comment-goal-analysis.md

---

## 219. Method vs Results (INFORMATION Trade-off)

**Specific instance**: Lead with how ARAW works vs lead with what ARAW found
**Universal form**: Should communication emphasize process or outcomes?

**Resolving questions**:
1. Is value in the method or the findings?
2. Which creates motivation to learn more?
3. Can results stand alone without method?

**When Method Wins**:
- Method is the product
- Audience wants to replicate
- Results are context-dependent
- Understanding process is the goal

**When Results Wins**:
- Outcomes demonstrate value
- Method is secondary to benefit
- Audience evaluates by results
- Results create curiosity about method

**Resolution**: Results-first creates curiosity about method. "ARAW found X" → "What's ARAW?"

**Test**: Does the audience care more about "what you discovered" or "how you discovered it"?

**Source**: araw_2026-01-27_lesswrong-comment-goal-analysis.md

---

## 220. Analysis vs Production (COGNITIVE Trade-off)

**Specific instance**: Using ARAW to explore a writing task vs using ARAW to produce a written artifact
**Universal form**: When does exploratory thinking help vs hurt when producing output?

**Resolving questions**:
1. Is the goal understanding or an artifact?
2. Are criteria defined before production?
3. Is verification possible after production?

**When Analysis-First Wins**:
- Complex output with many requirements
- Quality criteria not yet defined
- Stakes are high enough to justify preparation
- Output will be judged against standards

**When Direct Production Wins**:
- Simple output with obvious criteria
- Time pressure exceeds quality needs
- Iterative feedback available
- Low stakes, reversible output

**Resolution**: Separate analysis and production into different phases. Use analysis to accumulate constraints, then produce within constraints, then verify against criteria.

**Test**: Can you verify output quality without explicit criteria? If no, define criteria first (analysis). If yes, produce directly.

**Source**: araw_2026-01-27_writing-quality-pipeline.md

---

## 221. Explicit vs Implicit Criteria (EPISTEMIC Trade-off)

**Specific instance**: "Make it engaging" (implicit) vs "First sentence creates curiosity or states surprising fact" (explicit)
**Universal form**: When do named properties suffice vs requiring observable verification?

**Resolving questions**:
1. Can two people independently verify the criterion?
2. Does the criterion have mechanical indicators?
3. Would disagreement be about interpretation or observation?

**When Implicit Criteria Win**:
- Subjective domain (art, taste)
- Expert judgment available
- Fast iteration allows learning
- Stakes are low

**When Explicit Criteria Win**:
- Quality must be determined, not hoped
- Multiple stakeholders must agree
- No expert judgment available
- Verification must be independent

**Resolution**: For any writing where "guessing at quality" is unacceptable, convert implicit criteria to explicit verifiable criteria before drafting.

**Test**: If I gave this criterion to two different reviewers, would they reach the same conclusion about whether the draft meets it?

**Source**: araw_2026-01-27_writing-quality-pipeline.md

---

## 222. Constraint vs Freedom (STRUCTURE Trade-off)

**Specific instance**: Accumulated requirements from multiple analysis sessions vs writing freely
**Universal form**: Do constraints enable quality or restrict creativity?

**Resolving questions**:
1. Is the output creative expression or functional artifact?
2. Do constraints eliminate bad options or good ones?
3. Is quality defined by meeting spec or by surprise?

**When Freedom Wins**:
- Creative output valued for novelty
- Constraints would eliminate the interesting options
- "Good" is defined by unexpected value
- Over-specification kills the spirit

**When Constraints Win**:
- Functional artifact with clear purpose
- Constraints eliminate only bad options
- "Good" is defined by meeting requirements
- Under-specification allows quality drift

**Resolution**: For functional writing, accumulated constraints ENABLE quality by eliminating ways to be bad. The constitution succeeded because it was constrained to quality by 5 sessions of analysis before writing.

**Test**: Would removing a constraint make better output possible, or just easier bad output?

**Source**: araw_2026-01-27_writing-quality-pipeline.md

---

## 223. Derivation vs Enumeration (STRUCTURAL Trade-off)

**Specific instance**: Constitution content derived from purpose vs LessWrong content enumerated from observations
**Universal form**: Should output be logically derived from purpose or assembled from observed patterns?

**Resolving questions**:
1. Can someone else derive the same output from the same question?
2. Does removing any piece create logical inconsistency?
3. Does the output prove itself or require external verification?

**When Derivation Wins**:
- Output must be correct, not just plausible
- The question has a determinate answer
- Quality means necessity, not just presence
- External verification is unavailable or unreliable

**When Enumeration Wins**:
- Domain lacks derivable structure
- Speed matters more than certainty
- Observed patterns are reliable predictors
- Output will be externally validated anyway

**Resolution**: High-quality writing requires derivation - content that follows necessarily from purpose. Enumeration produces guesses that might be right but can't be verified. The constitution succeeded because its content was derivable; the LessWrong failed because "what should the comment say?" doesn't determine its answer.

**Test**: Could someone else, given only the purpose, derive the same output? If yes → derivation. If no → enumeration.

**Source**: araw_2026-01-27_universal-writing-quality-principles.md

---

## 224. Construction vs Proof (METHOD Trade-off)

**Specific instance**: 7-phase writing procedure (construction) vs constitution derivation (proof)
**Universal form**: Is the output assembled from parts or derived as necessary consequence?

**Resolving questions**:
1. Does the method explain WHY each step is necessary?
2. Is structure determined by purpose or chosen from options?
3. Does the output prove its own correctness?

**When Construction Wins**:
- Components are well-understood and reliable
- Assembly is a solved problem
- Verification is available post-hoc
- Speed of assembly matters

**When Proof Wins**:
- Component selection must be justified
- Structure must be defended as correct
- No external verification available
- Quality requires necessity, not just presence

**Resolution**: The 7-phase writing procedure is flawed because it treats writing as construction (assemble parts) rather than proof (derive what follows). The constitution wasn't constructed - it was derived from "what should AI do?" The procedure enumerates observed phases without proving they're necessary.

**Test**: Can you explain why each piece MUST be there, or only why it SEEMS useful?

**Source**: araw_2026-01-27_universal-writing-quality-principles.md

---

## 225. Observation vs Necessity (EPISTEMIC Trade-off)

**Specific instance**: "Lead with findings" (observed) vs "Human wellbeing must be addressed" (necessary)
**Universal form**: Are claims justified by observation of what works or derivation of what must be?

**Resolving questions**:
1. Could the claim be wrong without contradiction?
2. Is the claim removable without breaking the output?
3. Would different observers arrive at the same claim?

**When Observation Wins**:
- Domain is empirical, not logical
- Necessity can't be established
- Observed patterns are stable
- Claims will be tested anyway

**When Necessity Wins**:
- Claims must be defensible as correct
- Observation might capture correlation not causation
- Removal should create inconsistency
- Claims form foundation for further reasoning

**Resolution**: High-quality writing contains necessary claims - ones that would be wrong to omit. Low-quality writing contains observed claims - ones that seem useful but could be different. Test: does removing the claim break something, or just change the style?

**Test**: Is there a REASON this claim must be present, or just evidence it appears in good examples?

**Source**: araw_2026-01-27_universal-writing-quality-principles.md

---

## 226. Explicit vs Implicit Invocation (COMMITMENT Trade-off)

**Specific instance**: "Only explicit /araw prefix should trigger ARAW" vs "Context signals (depth indicators, follow-up references) should also trigger"
**Universal form**: When should system behavior require explicit activation vs be triggered by implicit contextual signals?

**Resolving questions**:
1. How costly is false activation vs missed activation?
2. Can users reasonably be expected to always use explicit triggers?
3. Are implicit signals reliable enough to act on?
4. Does explicit requirement create friction that reduces system value?

**When Explicit Wins**:
- False activation is costly (expensive, time-consuming, annoying)
- Implicit signals are ambiguous or unreliable
- Users are sophisticated and can remember syntax
- System behavior has high stakes requiring deliberate choice

**When Implicit Wins**:
- Missed activation is costly (user frustration, unmet expectations)
- Implicit signals are reliable (user said "8x", clearly wants depth)
- Users expect systems to understand context
- Reducing friction is more valuable than preventing edge case false activations

**Resolution**: For ARAW depth, implicit wins - user explicitly mentioned "8x" and clearly expected depth. The cost of under-ARAWing (user dissatisfaction) exceeds cost of over-ARAWing (can always say "just briefly").

**Test**: What's the cost ratio of false activation : missed activation? If missed > false, add implicit triggers.

**Source**: araw_2026-01-27_why-araw-depth-failure.md

---

## 227. Advisory Rules vs Enforcement Mechanisms (STRUCTURE Trade-off)

**Specific instance**: "Rules that say 'you MUST do X' in documentation" vs "Automated hooks/gates that prevent non-compliance"
**Universal form**: Should system requirements be enforced through instructions (advisory) or mechanisms (enforcement)?

**Resolving questions**:
1. How reliable is the actor that must follow the rules?
2. What's the cost of non-compliance?
3. Can enforcement be implemented without excessive complexity?
4. Does enforcement reduce flexibility inappropriately?

**When Advisory Wins**:
- Actor is highly reliable (well-trained human, deterministic system)
- Non-compliance cost is low
- Enforcement would add unacceptable complexity
- Flexibility to break rules sometimes is valuable

**When Enforcement Wins**:
- Actor is unreliable (probabilistic system, forgetful human)
- Non-compliance cost is high (user frustration, system failure)
- Enforcement infrastructure exists and is cheap to use
- Rules should NEVER be broken

**Resolution**: For LLM-based systems like ARAW, enforcement wins. LLMs are probabilistic and can "forget" or fail to apply rules. Hooks provide reliable enforcement that doesn't depend on attention or memory.

**Test**: Is the rule-follower deterministic or probabilistic? If probabilistic, add enforcement.

**Source**: araw_2026-01-27_why-araw-depth-failure.md

---

## 228. Procedural Knowledge vs Procedural Execution (EPISTEMIC Trade-off)

**Specific instance**: "Having comprehensive instructions for how to do X" vs "Actually executing X when appropriate"
**Universal form**: When does having knowledge of a procedure fail to translate into executing that procedure?

**Resolving questions**:
1. What triggers the transition from "knowing" to "doing"?
2. Are there competing instructions that override?
3. Is the trigger mechanism reliable?
4. Does the actor "remember" the knowledge at decision time?

**When Knowledge Suffices**:
- Trigger mechanism is reliable and always active
- No competing instructions
- Knowledge is salient at decision time
- Actor is deterministic

**When Execution Fails Despite Knowledge**:
- Trigger mechanism is narrow (only explicit invocation)
- Competing instructions win by default (base prompt vs skill)
- Knowledge is in a document not loaded at decision time
- Actor is probabilistic and context-dependent

**Resolution**: Add multiple trigger mechanisms, ensure knowledge is present at decision time (redundancy in always-loaded instructions), add enforcement that doesn't depend on actor "remembering."

**Test**: Trace the path from trigger to execution - where could it fail? Add redundancy at failure points.

**Source**: araw_2026-01-27_why-araw-depth-failure.md

---

## 229. Stateless vs Stateful Sessions (INFORMATION Trade-off)

**Specific instance**: "Each turn/request should be independent" vs "Session context (like ARAW depth) should persist across turns"
**Universal form**: When should system state persist across interactions vs reset between them?

**Resolving questions**:
1. Does the user expect continuity?
2. Is the state information relevant to subsequent interactions?
3. What's the cost of incorrectly persisting vs incorrectly resetting?
4. Can state be inferred from context or must it be explicitly tracked?

**When Stateless Wins**:
- Each interaction is genuinely independent
- State persistence creates confusion (wrong state applied)
- Users expect fresh start each time
- State is expensive to track and manage

**When Stateful Wins**:
- Interactions are part of a coherent session
- Users expect continuity (follow-up questions)
- State loss creates user frustration
- State is easy to track and relevant

**Resolution**: For ARAW sessions, stateful wins. Users expect follow-up questions to maintain the established depth. "8x ARAW" establishes a session context that should persist until explicitly changed.

**Test**: Would a user be surprised if state was lost between turns? If yes, persist state.

**Source**: araw_2026-01-27_why-araw-depth-failure.md

---

## 230. Single Source of Truth vs Redundant Instructions (RESOURCE Trade-off)

**Specific instance**: "Rules should be in one place (skill document)" vs "Critical rules should be mirrored in multiple places (CLAUDE.md + skill + hooks)"
**Universal form**: When should information be centralized vs distributed for reliability?

**Resolving questions**:
1. What's the cost of the information not being available when needed?
2. What's the cost of maintaining multiple copies?
3. Can copies become inconsistent, and how bad is that?
4. Is there a "primary" source and "backup" sources?

**When Single Source Wins**:
- Information changes frequently (maintenance burden)
- Inconsistency between copies is very costly
- Information is always available when needed
- Centralization aids understanding

**When Redundancy Wins**:
- Information may not be loaded when needed (skill not invoked)
- Consistency is manageable (critical rules don't change often)
- Cost of missing information is high (system failure)
- Redundancy provides defense in depth

**Resolution**: For CRITICAL ARAW rules (depth requirements, no early termination), redundancy wins. Mirror these to CLAUDE.md so they're always present. Keep detailed rules in skill doc, but ensure critical rules are never missing.

**Test**: Would the system fail catastrophically if this information wasn't present? If yes, add redundancy.

**Source**: araw_2026-01-27_why-araw-depth-failure.md

---

## 231. Constraint vs Freedom (OPTIMIZATION Trade-off)

**Specific instance**: "32 MUST requirements constrain search" vs "Vague 'optimal' goal allows freedom"
**Universal form**: How much constraint is optimal for search? More constraints narrow the space but may eliminate good solutions.

**Resolving questions**:
1. Are the constraints NECESSARY for success, or arbitrary?
2. Do constraints eliminate BAD solutions, or ALL solutions?
3. Can success be verified without constraints?
4. Is the search space finite or infinite without constraints?

**When More Constraints Win**:
- Constraints define success criteria (checkable)
- Search space is infinite without constraints
- Need to know when you're "done"
- Output must be defensible

**When Less Constraints Win**:
- Exploring genuinely novel territory
- Discovery task, not directed task
- Best solution might violate conventional constraints
- Judgment preferable to rigid rules

**Resolution**: For DIRECTED tasks (writing, building), explicit constraints beat vague goals. Constraints should be necessary (required for success), not arbitrary. The quality of constraints = quality of goal definition.

**Test**: Is the search space bounded (finite, completable)? If no, add constraints until it is.

**Source**: araw_2026-01-27_why-constitution-succeeded.md

---

## 232. Normative vs Descriptive Claims (EPISTEMIC Trade-off)

**Specific instance**: "We commit to X" (normative) vs "This works because Y" (descriptive)
**Universal form**: Should claims be framed as commitments (what we will do) or descriptions (what is true)?

**Resolving questions**:
1. Is the claim about reality (descriptive) or about values/intentions (normative)?
2. Can the claim be verified by checking behavior (normative) or external reality (descriptive)?
3. Is the goal to inform or to commit?
4. What kind of defense is needed?

**When Normative Wins**:
- Directed tasks (writing, building, committing)
- Output must be defensible on internal coherence
- Success criteria should be self-defined
- Don't need external validation

**When Descriptive Wins**:
- Research tasks (understanding, explaining)
- Claims must correspond to external reality
- Success = accuracy, not coherence
- External validation required

**Resolution**: For DIRECTED tasks, normative framing creates checkable success criteria. "We commit to X" can be verified (did you do X?). "This is true because Y" invites endless challenge.

**Test**: Can success be verified by checking the output alone (normative) or does it require external validation (descriptive)?

**Source**: araw_2026-01-27_why-constitution-succeeded.md

---

## 233. Goal Definition vs Goal Discovery (COMMITMENT Trade-off)

**Specific instance**: "Define requirements BEFORE writing" vs "Discover what's needed AS you write"
**Universal form**: Should goals be defined upfront or discovered through exploration?

**Resolving questions**:
1. Is this a DIRECTED task (known outcome type) or DISCOVERY task (unknown outcome type)?
2. Do you know what success looks like?
3. Will exploration reveal the goal, or just produce content?
4. Is the search space bounded without prior goal definition?

**When Define-First Wins**:
- Directed tasks with known outcome type
- Success criteria can be articulated
- Need bounded, completable search
- Output must be defensible

**When Discover-Through-Exploration Wins**:
- Genuine discovery tasks
- Don't know what success looks like yet
- Exploration reveals the problem structure
- Flexibility more valuable than efficiency

**Resolution**: For most writing tasks, define goals first. Even "exploratory" writing benefits from "what would make this exploration successful?" For genuine discovery, acknowledge you're in discovery mode and expect to iterate.

**Test**: Can you state what success looks like BEFORE starting? If yes, directed. If no, discovery.

**Source**: araw_2026-01-27_why-constitution-succeeded.md

---

## 234. Operational vs Aspirational Criteria (EPISTEMIC Trade-off)

**Specific instance**: "Must have X" (operational) vs "Should be good" (aspirational)
**Universal form**: Should success criteria be binary/checkable or qualitative/judgment-based?

**Resolving questions**:
1. Can a third party verify success without subjective judgment?
2. Is the criterion binary (did/didn't) or continuous (how much)?
3. Can you articulate what "good" means operationally?
4. Is defensibility required?

**When Operational Wins**:
- Output must be defensible to skeptics
- Multiple people must agree on success
- Need clear completion criteria
- Avoiding scope creep

**When Aspirational Wins**:
- Quality is genuinely ineffable
- Expert judgment is appropriate
- Over-operationalizing would miss the point
- Flexibility in interpretation is a feature

**Resolution**: For directed tasks requiring defensibility, operational beats aspirational. Convert aspirational to operational by asking "What would I observe if this were 'good'?" Use aspirational as tiebreaker between operationally-equivalent options.

**Test**: Could a skeptical third party verify success using only the criteria and the output? If no, make more operational.

**Source**: araw_2026-01-27_why-constitution-succeeded.md

---

## 235. Form vs Content (STRUCTURE Trade-off)

**Specific instance**: "What qualities should output have?" vs "What MUST be true of output?"
**Universal form**: Does the form of a question shape the content of answers?

**Resolving questions**:
1. Do different question forms produce systematically different answer types?
2. Is this effect strong enough to matter for output quality?
3. Can training overcome the form effect?

**When Form Matters More**:
- Users are unfamiliar with the domain
- Questions are templates used repeatedly
- Answer quality depends on answer type (operational vs aspirational)
- Form is the primary guidance

**When Content Matters More**:
- Users are experts who override form
- Questions are custom each time
- Multiple forms could produce same answer type
- Extensive examples provided

**Resolution**: For skill design, form shapes content significantly. "What qualities?" produces adjectives; "What MUST be true?" produces requirements. Design forms to shape desired outputs.

**Test**: Change question form, observe answer type distribution. If systematic difference, form matters.

**Source**: araw_2026-01-27_writing-skill-redesign.md

---

## 236. Criteria as Work vs Criteria as Overhead (EPISTEMIC Trade-off)

**Specific instance**: "Spend 60% on criteria, 40% on writing" vs "Spend 10% on criteria, 90% on writing"
**Universal form**: Is defining success criteria the core work or overhead before the real work?

**Resolving questions**:
1. Does criteria investment correlate with reduced rework?
2. Does "real work" intuition cause underinvestment in criteria?
3. At what point do criteria have diminishing returns?

**When Criteria IS the Work**:
- Directed tasks with known outcome type
- Multiple people must agree on success
- Output must be defensible
- Rework cycles are costly

**When Criteria IS Overhead**:
- Discovery tasks where goal emerges
- Solo work with internal standards
- Exploratory/creative work
- Iteration is cheap

**Resolution**: For directed tasks, criteria definition IS the hard intellectual work. Writing is just implementation. The "90% writing" intuition leads to rework cycles. Reframe criteria as the core work.

**Test**: Track correlation between criteria investment and rework cycles. High investment → low rework suggests criteria IS the work.

**Source**: araw_2026-01-27_writing-skill-redesign.md

---

## 237. Gate vs Phase (COMMITMENT Trade-off)

**Specific instance**: "Cannot proceed without MUST | TEST | EVIDENCE" vs "Define criteria, then proceed"
**Universal form**: Should quality checkpoints be mandatory gates or advisory phases?

**Resolving questions**:
1. Will users skip optional phases under time pressure?
2. Does gate friction cause valuable resistance or unproductive delay?
3. Can gate quality be verified mechanically?

**When Gate Wins**:
- Downstream quality depends critically on this step
- Users tend to rush this step
- Quality can be verified mechanically (template filled)
- Cost of proceeding without quality is high

**When Phase Wins**:
- Step quality is judgment-dependent
- User expertise varies (some don't need forcing)
- Gate friction exceeds quality benefit
- Iteration will fix problems anyway

**Resolution**: For criteria definition in directed writing, gate wins. Template provides mechanical verification. Cost of poor criteria (endless rework) exceeds gate friction.

**Test**: Compare outputs from gate-enforced vs phase-advisory processes. If gate produces better criteria and better final output, use gate.

**Source**: araw_2026-01-27_writing-skill-redesign.md

---

## 238. Outcome vs Process Criteria (EPISTEMIC Trade-off)

**Specific instance**: "Reader can identify subject" vs "Use active voice"
**Universal form**: Should success criteria specify outcomes (what results) or processes (how to produce)?

**Resolving questions**:
1. Are outcome criteria checkable?
2. Do process criteria guarantee outcome?
3. Does process specification constrain valid approaches?

**When Outcome Criteria Win**:
- Multiple processes could achieve same outcome
- Outcome is what actually matters
- Want to allow creative approaches
- Outcome is verifiable

**When Process Criteria Win**:
- Outcome is hard to specify but process is known
- Proven process reliably produces good outcomes
- Training novices who need guidance
- Process compliance is auditable

**Resolution**: For writing, outcome criteria are primary (what reader can do after). Process criteria are heuristics that may help achieve outcomes but shouldn't replace them. "Reader identifies subject easily" > "Use active voice."

**Test**: Can you verify the outcome directly? If yes, prefer outcome criteria. If outcome is ineffable, use process as proxy.

**Source**: araw_2026-01-27_writing-skill-redesign.md

---

## 218. Thoroughness vs Conciseness (Resource Trade-off)

**Specific instance**: ARAW requires deep exploration (thoroughness) vs Claude's training favors concise helpful output (conciseness)
**Universal form**: When do you explore exhaustively vs when do you summarize efficiently?

**Resolving questions**:
1. Does more exploration actually produce better insights?
2. Is the user's goal insight-finding or answer-getting?
3. What's the marginal value of additional exploration depth?

**When Thoroughness Wins**:
- High-stakes decisions
- Novel territory where answer isn't known
- Building foundational understanding
- User explicitly requests depth

**When Conciseness Wins**:
- Simple questions with clear answers
- Time-pressured situations
- User needs action, not analysis
- Marginal insight is low

**Test**: Would the user lose something important by getting a shorter answer?

**Source**: araw_2026-01-27_araw-quality-degradation-analysis.md

---

## 219. Rules vs Application (Structure Trade-off)

**Specific instance**: Adding more rules to ARAW skill vs actually following any rules
**Universal form**: When does adding specification improve compliance vs when does it reduce compliance?

**Resolving questions**:
1. Does additional specification help or overwhelm?
2. What's the processing capacity for rules?
3. Are rules positioned for visibility?

**When More Rules Win**:
- Edge cases need explicit handling
- Users can process additional rules
- Rules are well-organized and findable
- Rules don't conflict

**When Fewer Rules Win**:
- Processing capacity is limited
- Core rules are getting lost
- Rules are redundant or conflicting
- Simplicity aids compliance

**Test**: If you doubled the rules, would compliance increase or decrease?

**Source**: araw_2026-01-27_araw-quality-degradation-analysis.md

---

## 220. Meta-Analysis vs Core Analysis (Resource Trade-off)

**Specific instance**: ARAW meta-analysis (strategy selection, validation) vs ARAW core analysis (trees, exploration)
**Universal form**: When does checking the work crowd out doing the work?

**Resolving questions**:
1. Is meta-analysis producing value proportional to space taken?
2. Is core analysis being displaced?
3. What's the right ratio of doing vs checking?

**When Meta-Analysis Wins**:
- High risk of wasted work
- Strategy selection is uncertain
- Validation catches major errors
- Space is unlimited

**When Core Analysis Wins**:
- Space is limited
- Core work is primary value
- Meta-analysis becomes checkbox-filling
- Doing > planning in this context

**Test**: If meta-analysis were removed, would output quality improve or degrade?

**Source**: araw_2026-01-27_araw-quality-degradation-analysis.md

---

## 221. Explicit Checklists vs Implicit Quality (Epistemic Trade-off)

**Specific instance**: Checklists ensuring ARAW features are present vs checklists replacing meaningful work
**Universal form**: When do explicit quality checks ensure quality vs when do they substitute for quality?

**Resolving questions**:
1. Does checking items actually ensure the quality the items represent?
2. Can checklists be gamed (checked without substance)?
3. Is implicit quality recognizable without checklists?

**When Explicit Checklists Win**:
- Quality dimensions are well-defined and checkable
- Checkers can distinguish genuine vs superficial completion
- Checklists catch genuine misses
- Training new practitioners

**When Implicit Quality Wins**:
- Quality is holistic/gestalt
- Checklists can be gamed
- Practitioners know quality when they see it
- Explicit checks miss the point

**Test**: Can you complete the checklist while completely missing the point?

**Source**: araw_2026-01-27_araw-quality-degradation-analysis.md

---

## 222. Skill Length vs Skill Utility (Granularity Trade-off)

**Specific instance**: Comprehensive ARAW skill (1800 lines) vs shorter skill that gets fully followed
**Universal form**: When does comprehensive documentation help vs when does it overwhelm?

**Resolving questions**:
1. Is the full document being processed/followed?
2. What's the effective reading/processing limit?
3. Are the most important parts being lost in the length?

**When Longer Wins**:
- Users have capacity to process full document
- All content is necessary and non-redundant
- Structure allows finding relevant parts
- Reference use (lookup, not read-through)

**When Shorter Wins**:
- Processing capacity is limited
- Important content getting buried
- Users need complete read-through
- Real-time application (not reference)

**Test**: Would cutting the document in half improve adherence to the remaining rules?

**Source**: araw_2026-01-27_araw-quality-degradation-analysis.md

---

## 223. Training Bias vs Skill Override (Structure Trade-off)

**Specific instance**: Claude's base training (concise, helpful) vs ARAW skill instructions (thorough, exploratory)
**Universal form**: When do role/skill instructions override base behavior vs when does base behavior win?

**Resolving questions**:
1. How strong is the base behavior?
2. How explicit/strong is the override instruction?
3. Under what conditions does the override hold vs fail?

**When Override Wins**:
- Explicit, prominent instructions
- Base behavior conflicts clearly
- User explicitly requests override behavior
- Instruction is repeated/reinforced

**When Base Behavior Wins**:
- Instructions are buried or subtle
- Cognitive load favors default
- Instructions conflict with many base behaviors
- Long-running tasks (base behavior reasserts)

**Test**: Under stress/load, which behavior emerges - the override or the base?

**Source**: araw_2026-01-27_araw-quality-degradation-analysis.md

## 224. Completeness vs. Usability (Process Trade-off)

**Specific instance**: More ARAW documentation ensures nothing is missed vs More documentation means less gets read/followed
**Universal form**: When does adding documentation help vs hurt execution quality?

**Resolving questions**:
1. Does the marginal documentation change behavior?
2. What is the attention cost of additional content?
3. At what length does comprehension degrade?

**When AR wins**: Critical information that isn't obvious from context
**When AW wins**: When document length exceeds working memory capacity

**Test**: Compare execution quality at different documentation lengths for same task

**Source**: araw_2026-01-27_araw-skill-bloat-analysis.md

## 225. Enforcement vs. Enablement (Process Trade-off)

**Specific instance**: Mandatory markers ensure critical ARAW steps happen vs Mandatory markers create warning fatigue
**Universal form**: When does enforcement improve compliance vs reduce attention?

**Resolving questions**:
1. How many "mandatory" items before they lose meaning?
2. Does enforcement address capability or motivation gap?
3. Is the failure mode forgetfulness or intentional skip?

**When AR wins**: Few critical items, clear consequence of skipping, motivation gap
**When AW wins**: Many mandatory items, attention is the bottleneck, capability gap

**Test**: Track compliance rate as mandatory item count increases

**Source**: araw_2026-01-27_araw-skill-bloat-analysis.md

## 226. Guidance vs. Prescription (Process Trade-off)

**Specific instance**: Detailed depth tables guide appropriate ARAW depth vs Tables shift focus from quality to quotas
**Universal form**: When do numeric targets enable vs distort the goal?

**Resolving questions**:
1. Is the numeric target measuring the actual goal?
2. Does hitting the number become the goal itself?
3. Can qualitative criteria replace numeric targets?

**When AR wins**: Output easily measured, direct proxy for quality
**When AW wins**: Goodhart's law applies, meeting metrics ≠ meeting goals

**Test**: Compare output quality when targeting numbers vs targeting outcomes

**Source**: araw_2026-01-27_araw-skill-bloat-analysis.md

## 227. Self-Contained vs. Appropriate-Scope (Process Trade-off)

**Specific instance**: ARAW skill should include all needed information vs ARAW skill should focus on ARAW
**Universal form**: When should a tool/document include related information vs reference it?

**Resolving questions**:
1. How often is the related information needed during execution?
2. Does inclusion cause scope confusion?
3. Is there a natural boundary for the tool's purpose?

**When AR wins**: Related information used every execution, tight coupling
**When AW wins**: Related information used occasionally, blurs purpose, available elsewhere

**Test**: Does included information change execution or just sit there?

**Source**: araw_2026-01-27_araw-skill-bloat-analysis.md

## 228. Checklist Verification vs. Thoughtful Review (Process Trade-off)

**Specific instance**: Extensive checklists improve ARAW output vs Checklists enable mechanical checking without thought
**Universal form**: When do checklists improve quality vs create compliance theater?

**Resolving questions**:
1. Do checklist items require judgment or just confirmation?
2. Can checklist be "passed" without achieving underlying goal?
3. Does checking boxes replace or supplement real verification?

**When AR wins**: Checklist items binary, forgetting is main risk, items cover distinct failure modes
**When AW wins**: Items require judgment, checking mechanized, items overlap or cover same failure mode

**Test**: Compare "passed checklist" items against actual quality criteria

**Source**: araw_2026-01-27_araw-skill-bloat-analysis.md

## 229. Additive Problem-Solving vs. Subtractive Problem-Solving (Meta Trade-off)

**Specific instance**: Fix ARAW problems by adding rules/warnings vs Fix ARAW problems by removing complexity
**Universal form**: When should problems be solved by addition vs subtraction?

**Resolving questions**:
1. Is the problem caused by missing information or excess complexity?
2. Has prior addition created the current problem?
3. What is the cognitive cost of addition vs benefit?

**When AR wins**: Clear gap in coverage, addition addresses root cause, low cognitive cost
**When AW wins**: Complexity is the problem, addition is a patch not fix, high cognitive overhead

**Test**: Did previous additions solve or create problems?

**Source**: araw_2026-01-27_araw-skill-bloat-analysis.md

## 230. Generation Quantity vs Generation Quality (Process Trade-off)

**Specific instance**: More guesses = better coverage of possibility space vs More guesses = more noise to filter through
**Universal form**: When does quantity of options improve vs degrade final selection quality?

**Resolving questions**:
1. What is the selection mechanism's filtering capacity?
2. Does the domain have many valid solutions or few?
3. Is missing a good option worse than evaluating bad options?

**When AR wins**: Large solution space, strong selection, missing options is costly
**When AW wins**: Small solution space, weak selection, noise is costly

**Test**: Compare outcomes with 10 vs 50 initial options; does selection quality degrade with quantity?

**Source**: araw_2026-01-27_guess-generation-improvement.md

## 231. Questioning vs Accepting Claims (Process Trade-off)

**Specific instance**: Question everything for thoroughness vs Accept obvious things for efficiency
**Universal form**: When does questioning a claim add value vs waste effort?

**Resolving questions**:
1. What is the prior probability the claim is wrong?
2. What would change if the claim were wrong?
3. What is the cost of questioning vs accepting?

**When AR wins**: Low-confidence claims, high-impact if wrong, low questioning cost
**When AW wins**: High-confidence claims, low-impact if wrong, high questioning cost

**Test**: Use closedness heuristic (can you generate alternative in 30 seconds?)

**Source**: araw_2026-01-27_guess-generation-improvement.md

## 232. Structure vs Freedom in Creative Process (Process Trade-off)

**Specific instance**: Structure guides generation by prompting specific techniques vs Structure constrains generation by limiting exploration
**Universal form**: When does procedural structure help vs hurt creative output?

**Resolving questions**:
1. Does the executor struggle with blank-page starts?
2. Are the structures generative prompts or constraining rules?
3. Is the domain well-mapped or novel?

**When AR wins**: Executor needs prompts, structures are generative, domain is well-mapped
**When AW wins**: Executor is self-directed, structures feel constraining, domain is novel

**Test**: Structure the PHASES but leave freedom within phases

**Source**: araw_2026-01-27_guess-generation-improvement.md

## 233. Working With vs Working Against Training Bias (Meta Trade-off)

**Specific instance**: Use Claude's evaluation strength vs Compensate for Claude's generation weakness
**Universal form**: When should tools amplify existing strengths vs compensate for weaknesses?

**Resolving questions**:
1. Is the strength sufficient for the task?
2. Is the weakness critical for the task?
3. Can the weakness be compensated procedurally?

**When AR wins**: Strength is bottleneck, weakness is non-critical
**When AW wins**: Weakness is bottleneck, strength is already adequate

**Test**: Identify which capability is the actual bottleneck for task success

**Source**: araw_2026-01-27_guess-generation-improvement.md

## 234. Divergent Then Convergent vs Interleaved Thinking (Process Trade-off)

**Specific instance**: Generate all options, then evaluate all options vs Generate-evaluate-generate-evaluate cycle
**Universal form**: When is sequential phase separation better than interleaved execution?

**Resolving questions**:
1. Does evaluation contaminate generation (premature filtering)?
2. Does generation inform evaluation (building on feedback)?
3. What is the cognitive cost of mode-switching?

**When AR wins**: Evaluation kills wild ideas, generation doesn't need feedback, mode-switching is cheap
**When AW wins**: Evaluation enables targeted generation, feedback improves options, mode-switching is expensive

**Test**: Compare sequential vs interleaved on same problem; which produces more novel solutions?

**Source**: araw_2026-01-27_guess-generation-improvement.md

## 239. Intrinsic vs Relational Quality (EPISTEMIC Trade-off)

**Specific instance**: Quality is in the text vs Quality is in the text-reader-context-purpose interaction
**Universal form**: When is a property intrinsic to an object vs relational to its context?

**Resolving questions**:
1. Does the property remain constant across different observers/contexts?
2. Can the property be measured from the object alone, without knowing context?
3. Do experts in the domain agree that the property is context-independent?

**When AR wins**: Operational floor properties (spelling, grammar, format)
**When AW wins**: Ceiling properties (clarity, relevance, impact, elegance)

**Test**: Present same artifact to different audiences/contexts. If quality assessments diverge systematically, quality is relational.

**Source**: araw_2026-01-27_universal-writing-quality-principles.md

## 240. Operational vs Judgmental Criteria (OPTIMIZATION Trade-off)

**Specific instance**: Make all quality criteria checkable and binary vs Some quality requires human judgment
**Universal form**: When does formalization/operationalization help vs hurt?

**Resolving questions**:
1. Is consistency/reproducibility the priority?
2. Is excellence/innovation the priority?
3. What is lost when judgment is replaced by checklist?

**When AR wins**: Training, coordination, accountability, floor-level quality
**When AW wins**: Excellence, art, innovation, ceiling-level quality

**Test**: Take artifact that passes all operational criteria. Does quality variation remain? If yes, non-operational quality exists.

**Source**: araw_2026-01-27_universal-writing-quality-principles.md

## 241. Single vs Multiple Question Forms (STRUCTURE Trade-off)

**Specific instance**: One question ("What MUST be true?") vs Multiple questions for different phases
**Universal form**: When does uniformity in process help vs constrain?

**Resolving questions**:
1. Do different phases of work have different information needs?
2. Does form shape content (different questions get different answers)?
3. What is the cost of managing multiple forms?

**When AR wins**: Simplicity is priority, consistency matters, cognitive load is constraint
**When AW wins**: Flexibility needed, phases are distinct, form-content link is strong

**Test**: Same task with single vs multiple question forms. Compare outputs. If multiple produces richer/better output, multiple wins.

**Source**: araw_2026-01-27_universal-writing-quality-principles.md

## 242. Universal vs Context-Specific Meta-Criterion (OPTIMIZATION Trade-off)

**Specific instance**: Defensibility is always the ultimate test vs Meta-criterion varies by context
**Universal form**: When should standards be uniform vs adapted to situation?

**Resolving questions**:
1. Are the contexts genuinely different in what they optimize for?
2. Does uniform standard create false consistency across different goals?
3. What is the cost of context-switching standards?

**When AR wins**: High-stakes, adversarial contexts where challenge is expected
**When AW wins**: Creative, collaborative, practical contexts with different success criteria

**Test**: Ask practitioners in different contexts what they ultimately optimize for. If answers differ systematically, meta-criterion is context-specific.

**Source**: araw_2026-01-27_universal-writing-quality-principles.md

## 243. Explicit Primitives vs Implicit Model Richness (OPTIMIZATION Trade-off)

**Specific instance**: Make quality primitives explicit vs Rely on LLM's implicit understanding
**Universal form**: When is explicit knowledge better than implicit/tacit knowledge?

**Resolving questions**:
1. Is the implicit model systematically better or worse than explicit rules?
2. Is auditability/teachability important?
3. Does explicit knowledge constrain or enable?

**When AR wins**: Consistency needed, teaching/training needed, auditability required
**When AW wins**: Flexibility needed, novel situations common, implicit model captures more nuance

**Test**: Compare outputs from explicit-primitives vs implicit-knowledge approaches. If explicit produces comparable quality with better consistency, explicit wins.

**Source**: araw_2026-01-27_universal-writing-quality-principles.md

## 235. Exploration vs Questioning Framing (Process Trade-off)

**Specific instance**: ARAW framed as testing claims (questioning) vs expanding options (exploring)
**Universal form**: When does the framing of an operation change its reception even if the operation is the same?

**Resolving questions**:
1. Does the recipient perceive judgment or curiosity?
2. Does the framing change the executor's approach?
3. What prior relationship/trust exists?

**When AR wins**: High trust context, recipient wants challenge, executor is genuinely curious
**When AW wins**: Low trust context, recipient wants support, executor slips into judgment

**Test**: Same operation with different framing - compare user response

**Source**: araw_2026-01-27_is-araw-second-guessing.md

## 236. Trusting Request vs Testing Claims (Meta Trade-off)

**Specific instance**: ARAW trusts user's request for ARAW but tests user's claims within ARAW
**Universal form**: When is it coherent to trust someone's meta-judgment while questioning their object-level judgment?

**Resolving questions**:
1. Is meta-judgment about what to verify different from object-level claims?
2. Does the requester understand what testing entails?
3. Is the testing adversarial or collaborative?

**When AR wins**: User explicitly requested testing, understands process, wants thoroughness
**When AW wins**: User wanted execution, testing feels like distrust, process wasn't explained

**Test**: Did user explicitly ask for challenge, or just ask a question?

**Source**: araw_2026-01-27_is-araw-second-guessing.md

## 237. Confirmation Value vs Error-Finding Value (Information Trade-off)

**Specific instance**: ARAW provides value when confirming user is right, not just when finding errors
**Universal form**: When does verification that adds no new information still add value?

**Resolving questions**:
1. Does verification increase justified confidence?
2. Does it produce articulable reasoning?
3. Does it reveal conditions under which conclusion would change?

**When AR wins**: Verification produces reasoning, edge cases, defensible position
**When AW wins**: User already had sufficient confidence, verification is overhead

**Test**: After verification, can user explain WHY they're right better than before?

**Source**: araw_2026-01-27_is-araw-second-guessing.md

---

## 244. Depth vs Breadth in Skill Improvement (RESOURCE ALLOCATION Trade-off)

**Specific instance**: Deep improvement of few skills (rewrite 20 skills completely) vs broad improvement of many skills (add one section to 100 skills)
**Universal form**: When improving a system of components, when does depth beat breadth?

**Resolving questions**:
1. Are some components significantly more important than others?
2. Does incremental improvement compound or is threshold-based?
3. What's the effort ratio (10x more effort for how much more impact)?

**When depth wins**:
- Clear hierarchy of importance exists
- Threshold effects (good enough vs not good enough)
- Deep fix prevents class of problems vs surface fix addresses instance
- Core components have multiplier effect on peripheral

**When breadth wins**:
- Components are equally important
- Incremental improvements add value linearly
- Effort per component is low
- No threshold effects exist

**Test**: "Would one excellent skill with 10 mediocre beat 11 mediocre skills?"

**Key insight**: Start broad to establish baseline, go deep on high-impact components.

**Source**: araw_2026-01-27_claude-code-skills-quality-improvement.md

---

## 245. Template-Based vs Custom Improvement (OPTIMIZATION Trade-off)

**Specific instance**: Apply category templates to all skills vs customize improvement for each skill
**Universal form**: When do standardized solutions beat custom solutions for a portfolio of problems?

**Resolving questions**:
1. How much do problems actually vary?
2. What's the cost of customization vs template application?
3. Does templating miss important nuance?
4. Does customization produce consistent quality?

**When templates win**:
- 80% of variation is within template scope
- Custom effort is 5x+ template effort
- Template quality is sufficient (not optimal)
- Need to improve many items quickly

**When custom wins**:
- Each item is genuinely unique
- Template cannot capture important differences
- Quality matters more than speed
- Only few items need improvement

**Test**: "Apply template to 5 items - does it capture what matters for 4/5?"

**Key insight**: Templates for 80% of items, custom for 20% most important.

**Source**: araw_2026-01-27_claude-code-skills-quality-improvement.md

---

## 246. Delete vs Complete Stubs (COMMITMENT Trade-off)

**Specific instance**: Delete placeholder skills to reduce clutter vs complete them to have more capability
**Universal form**: When should incomplete work be abandoned vs finished?

**Resolving questions**:
1. Would anyone use this if completed?
2. Does the stub serve a coordination function (placeholder for future)?
3. What's the completion cost vs deletion benefit?
4. Does incomplete work create confusion?

**When delete wins**:
- No one would invoke the completed version
- Stub is orphaned (no one remembers why it exists)
- Completion would duplicate existing capability
- Stub creates confusion in navigation

**When complete wins**:
- Capability would be invoked if it existed
- Stub represents planned work
- Completion is low-cost
- Stub breaks expected workflows

**Test**: "Would I invoke this skill if it existed? When would I want it?"

**Key insight**: Stubs that survive multiple cleanup passes probably should be completed; stubs no one notices should be deleted.

**Source**: araw_2026-01-27_claude-code-skills-quality-improvement.md

---

## 247. Process Description vs Behavior Specification (STRUCTURE Trade-off)

**Specific instance**: Skills written as "process to follow" vs "behavior to exhibit"
**Universal form**: When writing instructions for an executor, when does describing the process beat describing the desired behavior?

**Resolving questions**:
1. Does the executor have implicit knowledge about the process?
2. Can process steps be mechanically followed?
3. Does behavior emerge from correct process?
4. How much adaptation is needed during execution?

**When process wins**:
- Executor lacks domain knowledge
- Process is well-defined and mechanical
- Deviation from process causes failure
- Process has been validated

**When behavior wins**:
- Executor has domain knowledge
- Situations vary requiring adaptation
- Process cannot anticipate all cases
- Outcome matters more than method

**When both needed (LLM context)**:
- LLM needs behavior specification (what to produce)
- PLUS process guidance (how to get there)
- PLUS decision points (how to adapt)

**Key insight**: For LLM skills, behavior specification is primary (what the output must be), process is secondary (how to get there), and decision points handle edge cases.

**Source**: araw_2026-01-27_claude-code-skills-quality-improvement.md

---

## 248. Operational vs Aspirational Definition (EPISTEMIC Trade-off)

**Specific instance**: "Produce 18 claims" (operational) vs "Be thorough" (aspirational)
**Universal form**: When should success criteria be measurable vs descriptive?

**Resolving questions**:
1. Can quality be measured?
2. Does measurement capture what matters?
3. Does aspirational guidance get followed under pressure?
4. Does operational guidance encourage box-checking over insight?

**When operational wins**:
- Quality CAN be measured (counts, formats, structures)
- Measurement is proxy for what matters
- Pressure exists (guidance will be optimized against)
- Self-verification is required

**When aspirational wins**:
- Quality is genuinely subjective
- Measurement would miss what matters
- Expert judgment is available
- Low-pressure context allows quality pursuit

**Test**: "If I meet the operational criteria but miss the spirit, is the output good?"

**Key insight**: Aspirational guidance degrades under pressure. Operational guidance creates accountability. Use both: operational as minimum, aspirational as target.

**Source**: araw_2026-01-27_claude-code-skills-quality-improvement.md

---

## 249. Skill Consolidation vs Granularity (STRUCTURE Trade-off)

**Specific instance**: Keep 300 skills for granular capability vs merge into fewer comprehensive skills
**Universal form**: When does having more components beat having fewer, more capable components?

**Resolving questions**:
1. Are components genuinely independent?
2. Does routing between components add value or overhead?
3. Do users invoke specific components or need the system to route?
4. Does maintenance scale with component count?

**When granularity wins**:
- Components serve distinct purposes
- Users invoke specific components knowingly
- Components can be improved independently
- Maintenance overhead is low per component

**When consolidation wins**:
- Components overlap significantly
- Users don't know which component to invoke
- Routing between components is complex
- Maintenance burden scales with count

**Test**: "Would merging these skills lose any capability? Would it simplify usage?"

**Key insight**: Consolidate when overlap is high, keep separate when purposes are distinct. Duplicates (v1, v2, v3) should be merged; complementary skills should stay separate.

**Source**: araw_2026-01-27_claude-code-skills-quality-improvement.md

## 250. Tool Guidance Specificity vs Flexibility (OPTIMIZATION Trade-off)

**Specific instance**: Detailed tool commands ("Use Bash: pytest -v") vs general guidance ("run the tests")
**Universal form**: How specific should execution instructions be?

**Resolving questions**:
1. Does specificity improve consistency more than it reduces adaptability?
2. Can tools/environments change without breaking guidance?
3. Does the executor have judgment for adaptation?
4. Is the task routine or novel?

**When specificity wins**:
- Complex procedures where consistency matters
- Executor lacks domain knowledge
- Environment is stable
- Task is routine/repeatable

**When flexibility wins**:
- Simple procedures where context varies
- Executor has judgment
- Environment varies
- Task is novel/exploratory

**Test**: "If the specific command fails, does the executor know what to try next?"

**Key insight**: For LLM execution, specificity improves consistency, but include fallback guidance. Specific commands with general explanation of purpose allows adaptation.

**Source**: araw_2026-01-27_fixing-skill-weaknesses.md

---

## 251. Fix vs Delete for Weak Components (RESOURCE ALLOCATION Trade-off)

**Specific instance**: Should stub skills be fixed (add content) or deleted (remove entirely)?
**Universal form**: When should resources be improved vs eliminated?

**Resolving questions**:
1. What's the opportunity cost of fixing?
2. What's the value if fixed?
3. Does anything depend on this component?
4. Is the purpose clear?

**When fix wins**:
- Clear purpose exists
- Other components invoke/depend on it
- Fixing is lower cost than recreation later
- Component fills a genuine gap

**When delete wins**:
- No clear purpose
- Nothing invokes it
- Fixing effort could go elsewhere
- Component is orphaned/forgotten

**Test**: "Would I miss this if it were gone? When would I want to invoke it?"

**Key insight**: If you can't articulate what a component would do or when you'd use it, deleting is usually better than fixing. Stubs that nobody invokes are usually cruft.

**Source**: araw_2026-01-27_fixing-skill-weaknesses.md

---

## 252. Human Methodology vs LLM Execution (STRUCTURE Trade-off)

**Specific instance**: debugging strategy is human-standard, but execution needs LLM-specific guidance
**Universal form**: When to adapt proven human methods vs create executor-native methods?

**Resolving questions**:
1. Is the methodology sound regardless of executor?
2. Does execution change the strategy or just tactics?
3. Can the executor follow the methodology with guidance?
4. Does the executor have capabilities humans lack?

**When preserve methodology wins**:
- Methodology is well-validated
- Strategy is universal, only tactics differ
- Executor can follow with execution guidance
- Human methodology captures domain knowledge

**When create new methodology wins**:
- Methodology assumes capabilities executor lacks
- Executor has unique capabilities not in methodology
- Strategy (not just tactics) would change
- Human methodology encodes unnecessary constraints

**Test**: "If I add execution guidance, does the human methodology produce good results?"

**Key insight**: Most human methodologies are sound - they encode domain expertise. LLMs often need execution guidance (HOW to do steps) not new methodology (WHAT steps). Add tool guidance before rewriting.

**Source**: araw_2026-01-27_fixing-skill-weaknesses.md

---

## 253. Verification Stringency vs Practical Application (OPTIMIZATION Trade-off)

**Specific instance**: Detailed verification criteria vs usable procedures with reasonable checks
**Universal form**: How much verification is optimal?

**Resolving questions**:
1. Does more verification improve outcomes?
2. Does verification create overhead that reduces application?
3. Are the stakes high enough to justify verification cost?
4. Can verification be automated or is it manual overhead?

**When stringent wins**:
- High-stakes decisions
- Repeated procedures (amortize verification design cost)
- Verification can be automated
- Errors are costly to fix

**When practical wins**:
- One-off tasks
- Low stakes
- Verification is manual overhead
- Speed matters more than perfection

**Test**: "Does this verification catch important errors, or just create busywork?"

**Key insight**: Verification should catch errors that matter. One checkable criterion that prevents the top failure mode is better than ten criteria that don't connect to failure modes.

**Source**: araw_2026-01-27_fixing-skill-weaknesses.md

---

## 254. Type-Level vs Skill-Specific Fixes (RESOURCE ALLOCATION Trade-off)

**Specific instance**: Fix all Template-Only skills the same way vs tailor fix to each skill's actual gaps
**Universal form**: When should solutions be generalized vs customized?

**Resolving questions**:
1. Do instances share enough structure for a common solution?
2. Does customization add value proportional to its cost?
3. Can a template be created that allows for customization?

**When type-level wins**:
- Instances have the same root cause
- Consistency across instances matters
- Resources for customization are limited
- Template can be applied mechanically

**When skill-specific wins**:
- Instances have different actual gaps despite same label
- Quality matters more than consistency
- Resources for analysis are available
- Prior categorization may be inaccurate

**Test**: "After reading each instance, would the same fix work?"

**Key insight**: Categories help organize thinking, but each instance needs verification. A category might group symptoms, not causes.

**Source**: araw_2026-01-27_fixing-skill-weaknesses.md

---

## 255. Prior Analysis Accuracy vs Current Analysis (INFORMATION Trade-off)

**Specific instance**: Trust prior 8x ARAW classifications vs re-analyze in current context
**Universal form**: When should we trust prior conclusions vs re-examine?

**Resolving questions**:
1. Was prior analysis based on the same data we now have?
2. Has context changed since prior analysis?
3. Is re-analysis cost justified by potential accuracy gain?

**When trust prior wins**:
- Prior analysis was thorough and well-documented
- Nothing relevant has changed
- Re-analysis is expensive and adds no information
- Prior conclusions were validated

**When re-examine wins**:
- Prior analysis may have been superficial
- Context has changed
- Re-analysis is cheap (can read the actual files)
- Prior conclusions were not validated

**Test**: "Can I verify prior conclusions in under 5 minutes?"

**Key insight**: Prior conclusions are hypotheses. When implementation is the next step, verify the hypothesis by reading actual artifacts.

**Source**: araw_2026-01-27_fixing-skill-weaknesses.md

---

## 256. Content Depth vs Usability (OPTIMIZATION Trade-off)

**Specific instance**: Add more patterns, examples, sections to skills vs keep skills concise and navigable
**Universal form**: How much content is optimal?

**Resolving questions**:
1. Does additional content improve outcomes?
2. Does content volume reduce usability?
3. Can content be structured to allow quick navigation?

**When depth wins**:
- Content is reference material (patterns, examples)
- Users need comprehensive coverage
- Good structure (tables, sections) aids navigation
- Content can be conditionally invoked (only read relevant parts)

**When brevity wins**:
- Content is procedure (steps to follow)
- Users need quick execution
- Content cannot be easily navigated
- All content must be processed

**Test**: "If I double the content, does usefulness double or halve?"

**Key insight**: Tables, pattern libraries, and examples scale well. Prose instructions do not. Add content as structured reference, not as longer instructions.

**Source**: araw_2026-01-27_fixing-skill-weaknesses.md

---

## 257. Aspirational Documentation vs Empirical Documentation (INFORMATION Trade-off)

**Specific instance**: Documenting how a system SHOULD work vs how it DOES work
**Universal form**: When should documentation describe intent vs reality?

**Resolving questions**:
1. Has the documented feature been implemented and tested?
2. Is the documentation for users who need it to work, or developers who will build it?
3. What's the cost of users following incorrect instructions?

**When aspirational wins**:
- Design docs before implementation
- Internal roadmap documentation
- Explicitly marked as "planned" or "proposed"
- Audience knows it's aspirational

**When empirical wins**:
- User-facing documentation
- Installation/usage instructions
- README files
- Anything users will actually try to follow

**Test**: "If a user follows these instructions exactly, will it work?"

**Key insight**: User-facing documentation must be empirical. If you haven't tested the instructions yourself, they're probably wrong. Describing a fictional system (like a non-existent plugin architecture) wastes user time and destroys trust.

**Source**: araw_2026-01-28_installation-failure-analysis.md

---

---

## 258. Claim vs Self-Application (LOGICAL Trade-off)

**Specific instance**: "You can't ask for unexpected" vs ARAW is a method for asking for unexpected
**Universal form**: When does a claim about limitations apply to the method making the claim?

**Resolving questions**:
1. Does the method's existence disprove the claim it makes?
2. Is the limitation truly fundamental or does the method escape it?
3. Is this a genuine logical contradiction or a scope issue?

**When AR wins** (claim holds):
- Limitation is truly fundamental
- Method doesn't actually escape it (just appears to)
- Claim applies at different level than method operates

**When AW wins** (claim disproved):
- Method successfully escapes the limitation
- The method's existence is evidence against the claim
- Claim needs qualification ("without structure/constraints")

**Test**: Can the method do what the claim says is impossible? If yes, claim needs revision.

**Source**: araw_2026-01-28_readme-self-examination.md

---

## 259. Truth-Access vs Structure-Forcing (EPISTEMIC Trade-off)

**Specific instance**: ARAW asks "truth-based questions" vs ARAW just structures exploration
**Universal form**: Does structured reasoning access truth or just organize thinking?

**Resolving questions**:
1. Does structured reasoning produce different conclusions than unstructured?
2. Are the conclusions more accurate or just more articulated?
3. What's the mechanism - discovery or organization?

**When truth-access wins**:
- Structure reveals truths that wouldn't emerge otherwise
- Conclusions are demonstrably more accurate
- Structure enables access to otherwise-hidden information

**When structure-forcing wins**:
- Structure just makes existing reasoning visible/checkable
- Conclusions are similar, just better articulated
- Value is auditability, not discovery

**Test**: Compare structured vs unstructured on problems with known answers. Is structured more accurate, or just more explainable?

**Source**: araw_2026-01-28_readme-self-examination.md

---

## 260. Single-Cause vs Multi-Cause Explanation (INFORMATION Trade-off)

**Specific instance**: "Expectation-following causes writing issues" vs multiple factors contribute
**Universal form**: When is a simple causal explanation sufficient vs misleadingly incomplete?

**Resolving questions**:
1. What percentage of variance is explained by the primary factor?
2. Are other factors merely minor or comparably important?
3. Does the simple explanation lead to wrong interventions?

**When single-cause wins**:
- One factor explains >70% of variance
- Other factors are minor/secondary
- Simple explanation leads to correct intervention

**When multi-cause wins**:
- Multiple factors explain comparable variance
- Ignoring other factors leads to incomplete solutions
- Simple explanation actively misleads

**Test**: Factor analysis. If primary factor explains <50%, multi-cause framing needed.

**Source**: araw_2026-01-28_readme-self-examination.md

---

## 261. Explanation Completeness vs Actionability (INFORMATION Trade-off)

**Specific instance**: Multi-factor explanation ("4 factors contribute") vs single-cause explanation ("the main cause is X")
**Universal form**: When does comprehensive explanation enable action vs paralyze it?

**Resolving questions**:
1. Does the reader need to understand the full landscape or take action?
2. Can all factors be addressed, or must they prioritize?
3. Does multi-factor explanation clarify or obscure what to do?

**When complete wins**:
- Reader needs systemic understanding
- All factors must be addressed simultaneously
- Reader is researcher/analyst, not practitioner
- Factors interact in ways that require understanding all

**When actionable wins**:
- Reader needs to DO something specific
- Resources force prioritization
- One factor dominates variance
- Action on one factor enables addressing others

**Test**: After reading, can the reader identify their first step? If not, explanation is too complete.

**Source**: araw_2026-01-28_araw-writing-unexpected.md

---

## 262. Instruction vs Mechanism (EPISTEMIC Trade-off)

**Specific instance**: "Find non-obvious" (instruction) vs "Assume obvious is wrong, explore alternatives" (mechanism)
**Universal form**: When can intent be communicated as goal vs when does it need structural enforcement?

**Resolving questions**:
1. Does the actor have internal mechanism to achieve the goal?
2. Does instruction alone reliably produce desired output?
3. What structure would FORCE the desired output?

**When instruction wins**:
- Actor has internal capability to achieve
- Goal is well-defined and actor understands it
- Iteration is possible to correct errors
- Actor is expert in the domain

**When mechanism wins**:
- Actor lacks internal capability (needs external scaffold)
- Instructions consistently fail to produce result
- Output must be right first time
- Actor is non-expert or the task fights their defaults

**Test**: Does actor reliably achieve goal from instruction alone across multiple attempts?

**Key insight**: "Find non-obvious" fails the same way "be creative" fails - it's an unstructured request. "List 3 obvious things, find what differs from all" succeeds because it's a mechanism.

**Source**: araw_2026-01-28_araw-writing-unexpected.md

---

## 263. Description vs Demonstration (INFORMATION Trade-off)

**Specific instance**: README describes how ARAW works vs README shows example ARAW prompt
**Universal form**: When is explaining how something works sufficient vs when must you show it working?

**Resolving questions**:
1. Can reader operationalize from description alone?
2. How much implementation detail is non-obvious?
3. What's the cost of misunderstanding?

**When description wins**:
- Process is simple or familiar
- Reader is expert who knows what to do with concepts
- Demonstration would be too long/distracting
- Multiple valid implementations exist

**When demonstration wins**:
- Process has non-obvious implementation details
- Reader is novice who needs concrete example
- Misunderstanding would waste significant effort
- One example clarifies better than pages of description

**Test**: Can reader correctly apply the concept after reading description alone, without examples?

**Key insight**: READMEs often describe what a tool does but don't show the actual prompt/command. For LLM tools especially, showing the actual prompt is often more valuable than explaining the concept.

**Source**: araw_2026-01-28_araw-writing-unexpected.md

---

## 264. Novelty-Seeking vs Completeness-Seeking (EPISTEMIC Trade-off)

**Specific instance**: Finding "unexpected" things vs finding all possibilities through universalization
**Universal form**: When is the goal to find novel items vs enumerate the complete space?

**Resolving questions**:
1. What matters more: surprising the audience or ensuring nothing is missed?
2. Is the search space bounded (completable) or unbounded (need heuristics)?
3. Is coverage more valuable than discovery?

**When novelty wins**:
- Creative tasks where surprise is the value
- Brainstorming where quantity/diversity matters more than coverage
- Audience explicitly wants to be surprised
- Search space is infinite (completeness impossible)

**When completeness wins**:
- Analysis where missing something is costly
- Decision-making where coverage prevents regret
- Logical/formal domains where completeness is achievable
- "Did I miss anything?" is more important than "Is this surprising?"

**Test**: What's worse - missing something important, or producing something boring? If missing is worse, seek completeness.

**Key insight**: "Find unexpected" is an unstructured request that fails the same way "be creative" fails. "Universalize then derive" is a mechanism that produces completeness, with unexpected as a side effect.

**Source**: araw_2026-01-28_universalization-approach.md

---

## 265. Vertical vs Horizontal Exploration (EPISTEMIC Trade-off)

**Specific instance**: Universalization (going up in abstraction) vs opposite-finding (staying at same level)
**Universal form**: When to explore by generalizing vs exploring by negating?

**Resolving questions**:
1. How many alternatives exist? (2 = horizontal OK; N = need vertical)
2. Is the claim binary (true/false) or one of many possibilities?
3. Does negation capture the relevant alternatives?

**When vertical (universalization) wins**:
- Many alternatives exist beyond simple negation
- Need to find the complete space of possibilities
- "What else could this be?" is more important than "What if this is false?"
- Framing/category is uncertain

**When horizontal (opposite-finding) wins**:
- Binary choice (yes/no, do/don't)
- Negation captures the relevant alternative
- Category is known, just need to test truth value
- Speed matters more than completeness

**Test**: If I list "X" and "not-X", have I captured all meaningful alternatives? If no, go vertical.

**Key insight**: Horizontal produces 2 options (X, not-X). Vertical produces N options (all instances of universal). Vertical subsumes horizontal (negation is one instance of the universal).

**Source**: araw_2026-01-28_universalization-approach.md

---

## 266. Finding Universal vs Deriving Specific (STRUCTURAL Trade-off)

**Specific instance**: "What is this an instance of?" vs "What instances exist?"
**Universal form**: When to generalize upward vs specialize downward?

**Resolving questions**:
1. Do I know the space I'm searching in?
2. Is the category clear or am I trying to identify it?
3. Is the problem too specific or too abstract?

**When universal-finding wins**:
- Don't know the space of possibilities
- Framing is unclear, need to identify category
- Stuck at a specific level, need to zoom out
- Need to see what else this could be

**When specific-deriving wins**:
- Universal is already known
- Need concrete options to evaluate
- Abstract understanding established, need instances
- Ready to act, need specific alternatives

**Test**: Do I know what this is an instance of? No = universalize first. Yes = derive instances.

**Key insight**: The process is recursive: universalize to find category, derive to find instances, universalize each instance to find its category, and so on. Both moves are needed.

**Source**: araw_2026-01-28_universalization-approach.md

---

## 267. One Master Question vs Many Techniques (STRUCTURAL Trade-off)

**Specific instance**: "What is this an instance of?" as only universalization question vs 12 distinct techniques
**Universal form**: When does a single principle with applications beat multiple distinct tools?

**Resolving questions**:
1. Can all techniques be derived from the single principle?
2. Do different techniques produce non-overlapping outputs?
3. Is simplicity (one principle) worth potential loss of coverage?

**When ONE wins**:
- Principle is genuinely fundamental
- Applications are derivable from principle
- Simpler to teach and remember
- Overhead of multiple tools exceeds benefit

**When MANY wins**:
- Techniques are genuinely distinct (different triggers, different outputs)
- Single principle misses cases that specific techniques catch
- Domain expertise encoded in specific techniques
- Practitioners benefit from variety of entry points

**Test**: Apply the single principle and the multiple techniques to same input. If single principle produces same coverage, ONE wins. If techniques produce non-overlapping useful outputs, MANY wins.

**Key insight**: For universalization, techniques may be applications of "What is [aspect X] an instance of?" - same core question applied to different aspects (time, role, scale, etc.).

**Source**: araw_2026-01-28_universalization-techniques.md

---

## 268. Framework Completeness vs True Completeness (EPISTEMIC Trade-off)

**Specific instance**: Universalization guarantees completeness within known dimensions vs unknown dimensions remain undiscovered
**Universal form**: When does systematic search within a framework guarantee coverage vs when do blind spots remain?

**Resolving questions**:
1. Is the domain well-understood with enumerable dimensions?
2. Are unknown unknowns likely in this domain?
3. Has the framework itself been validated externally?

**When FRAMEWORK wins**:
- Domain is mature and well-mapped
- Dimensions are enumerable and known
- Framework has been tested against external discovery
- Stakes don't justify exhaustive external search

**When TRUE COMPLETENESS fails**:
- Novel domain where framework may be incomplete
- Unknown unknowns are likely
- Framework itself may be wrong
- External validation has found items framework missed

**Test**: After completing framework-based search, deliberately use different methods (external experts, unrelated domains, random search). If these consistently find items the framework missed, true completeness requires more than framework.

**Key insight**: Universalization produces completeness WITHIN the dimensions you know to universalize on. It cannot surface dimensions you don't know exist.

**Source**: araw_2026-01-28_universalization-techniques.md

---

## 269. Mechanical Generation vs Judgmental Selection (STRUCTURAL Trade-off)

**Specific instance**: Generate all universalizations mechanically vs need judgment to select useful ones
**Universal form**: When can a process be fully automated vs when does it require human/contextual judgment?

**Resolving questions**:
1. Can selection criteria be formalized in advance?
2. Does "useful" depend on context that varies?
3. Is speed more important than optimality?

**When MECHANICAL wins**:
- Selection criteria are formalizable ("most abstract", "most instances")
- Context is stable or can be parameterized
- Speed/scale matters more than optimality
- Consistency across applications matters

**When JUDGMENTAL wins**:
- "Useful" depends on purpose that varies
- Context changes what counts as valuable
- Quality matters more than quantity/speed
- Novel situations require adaptation

**Test**: Can you specify selection criteria before seeing the outputs? If yes, mechanical selection is possible. If selection requires seeing outputs and context, judgment is needed.

**Key insight**: For universalization, GENERATION can be mechanical (apply all 12 techniques), but SELECTION among the results often requires judgment about which universalization serves the current purpose.

**Source**: araw_2026-01-28_universalization-techniques.md

---

## 270. Depth vs Breadth in Discovery (STRUCTURAL Trade-off)

**Specific instance**: ARAW digs deep by negating claims; Universalization explores wide by mapping equivalence classes
**Universal form**: When does deep probing of one path beat broad mapping of all paths?

**Resolving questions**:
1. Are we looking for flaws in a specific claim, or exploring a possibility space?
2. Is the claim central enough that its truth/falsity matters more than alternatives?
3. Do we have time for both depth and breadth?

**When DEPTH (ARAW) wins**:
- Testing a specific claim or assumption
- Looking for hidden flaws, contradictions, failure modes
- The claim is already narrow and specific
- Need to stress-test before committing

**When BREADTH (Universalization) wins**:
- Don't yet know the right question to ask
- Need complete coverage of possibility space
- Looking for analogies and transferred solutions
- Want to ensure nothing is missed

**Test**: Ask "Am I testing something specific, or exploring what's possible?" If testing → depth (ARAW). If exploring → breadth (Universalization).

**Key insight**: Depth and breadth serve different epistemic purposes. Combine them: Universalize to find the space, ARAW to stress-test specific candidates.

**Source**: araw_2026-01-28_araw-vs-universalization.md

---

## 271. Falsification vs Taxonomy (EPISTEMIC Trade-off)

**Specific instance**: ARAW seeks to falsify claims (Popperian); Universalization seeks to classify instances (Aristotelian)
**Universal form**: When should we try to disprove vs try to categorize?

**Resolving questions**:
1. Is the goal to validate truth or to understand structure?
2. Is binary truth sufficient, or do we need nuanced understanding?
3. Are we past the exploration phase and into verification?

**When FALSIFICATION wins**:
- Claims need validation before action
- Binary outcomes matter (works/doesn't work)
- Risk of false positives is high
- Need to eliminate bad options

**When TAXONOMY wins**:
- Understanding structure of the domain
- Multiple valid options exist (not binary)
- Need to see relationships between options
- Building a mental model, not testing a claim

**Test**: Ask "Do I need to know if X is true, or do I need to know what X is related to?" If truth → falsification. If relationships → taxonomy.

**Key insight**: Falsification and taxonomy are complementary. Taxonomy maps the territory; falsification tests specific locations. Use taxonomy first to find candidates, falsification second to test them.

**Source**: araw_2026-01-28_araw-vs-universalization.md

---

## 272. Convergent vs Divergent Thinking (COGNITIVE Trade-off)

**Specific instance**: ARAW converges by eliminating options; Universalization diverges by generating options
**Universal form**: When should thinking narrow down vs when should thinking expand?

**Resolving questions**:
1. Do we have enough options, or too few?
2. Are we early in the process (need options) or late (need decision)?
3. Is the bottleneck generation or selection?

**When CONVERGENT (ARAW) wins**:
- Already have candidates to evaluate
- Need to narrow down to decision
- Have generated enough options
- Time to commit, not explore

**When DIVERGENT (Universalization) wins**:
- Don't have enough options yet
- Stuck on one framing
- Need creative alternatives
- Early in process

**Test**: Count your options. If <3, diverge first (Universalization). If >3, converge (ARAW). The UAUA pattern (Universalize → ARAW → Universalize → ARAW) alternates between them.

**Key insight**: Optimal thinking alternates between divergent and convergent. Universalize to expand the space, ARAW to contract it, repeat until you have a robust answer that survived both expansion and contraction.

**Source**: araw_2026-01-28_araw-vs-universalization.md

---
