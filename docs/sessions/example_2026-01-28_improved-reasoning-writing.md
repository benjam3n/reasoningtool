The Structure of Careful Thought

Most people find out they were wrong about something important after the consequences show up. The flaw in the plan reveals itself when the plan fails. The gap in the argument becomes visible when someone else points it out. The missed option becomes obvious after it's no longer available.

This is strange, if you think about it. Smart people think carefully before acting. And yet, they keep getting surprised by things they feel like they should have seen coming. I've been trying to understand why this happens, and I think I've found part of the answer.

It has to do with guesses. A guess is any claim about the world that could turn out to be wrong: predictions, explanations, assumptions, interpretations, evaluations. These are different from definitions, which can be unclear but can't really be "wrong about the world" the way a prediction can. They're different from procedures, which are instructions rather than claims. And they're different from preferences, which aren't the kind of thing that gets falsified.

The distinction matters because guesses need testing, and other things don't. When the categories get confused, effort gets wasted on testing things that don't need it, or worse, things that desperately need testing slip through untested.

Here's what I've noticed about how people (including me) engage with their guesses. When something seems like it might be true, there's a natural pull toward asking why it could be true, what evidence supports it, what would follow if it's right. The same energy doesn't usually go toward asking why it could be false, what evidence contradicts it, what would follow if it's wrong.

This asymmetry seems to operate even in careful people. And here's the thing I didn't appreciate for a long time: knowing about confirmation bias doesn't fix this. Knowing about a bias doesn't automatically provide a mechanism for correcting it.

Think about what "trying to consider the other side" usually looks like in practice. Someone has a position. They remind themselves to think about objections. A couple of obvious objections come to mind. They find responses to those objections. They feel more confident in their original position. This is such a common pattern that it's almost the default mode of "being fair-minded."

The problem is that the dangerous objections aren't the obvious ones. The dangerous objections are the ones that haven't occurred to anyone yet, and finding them requires genuinely inhabiting the opposing position long enough to discover them. A quick visit to the other side of an argument, just to check the box, doesn't usually surface the surprises.

There's another asymmetry too. It's common to ask "what are the objections to my view?" It's less common to ask, with the same seriousness, "what would the world look like if those objections were right?" Taking an objection seriously means more than just acknowledging it exists; it means genuinely imagining the version of reality where that objection is the correct frame.

And there's a structural problem underneath all this. "Try to consider objections" is vague enough that it's easy to feel like it's been done when it hasn't really. Without some kind of process that forces specific operations, the defaults tend to reassert themselves.

I've been experimenting with something that seems to help. For any claim worth examining, I try to do two things with roughly equal effort. First, I take the claim as true and ask what supports it, what follows from it, what else would have to be true for it to hold. Then I take the same claim as false and ask what contradicts it, what would follow from that, what the failure mode is, what a better alternative might look like. And then I compare what I found.

The goal isn't for one side to "win." The goal is to have explored both directions with genuine effort, deep enough that something surprising shows up. And that's the key part: if the process doesn't surface at least one thing that wasn't already obvious, it probably wasn't done thoroughly enough.

Here's an example I keep coming back to. I was once involved in a hiring decision and the claim on the table was "we should hire this candidate." The obvious case for hiring looked strong: good skills, good interview, came recommended. But pushing on those specifics, questions started to emerge. Did the skills actually match what the role needed? Did interview performance really predict job performance for this kind of work? What were the recommender's incentives?

Going the other direction, the case against hiring also had obvious pieces: maybe the candidate was overstating their skills, maybe they wouldn't fit the culture. But pushing on those, different questions came up. How would we actually detect overstated skills? How were we assessing culture fit, and was our assessment method biased toward people who were similar to the existing team?

Neither side won. But questions emerged that hadn't been on the table before, and some of them turned out to matter a lot.

There's a related problem that this approach doesn't directly solve. Sometimes the issue isn't how to evaluate the options in front of you, but the fact that you're missing options entirely. Deciding between A and B is pointless if C is better than both, and you never thought of C. I've been exploring ways to systematically expand the space of options before evaluating any of them, though I'm less confident that I've found something reliable there.

Various other approaches exist for getting outside your own head. Red teaming has people adopt an adversarial role. Devil's advocate arguments push against the consensus. Pre-mortems imagine future failure. Steel-manning tries to construct the strongest version of the opposing case. These all seem valuable. My sense is that forcing yourself to genuinely inhabit both the "this is true" and "this is false" positions, with enough depth to find surprises, adds something that the other approaches don't always provide. But I could be wrong about that.

I don't know how much of this generalizes. The approach seems to work for me on certain kinds of decisions, especially ones where the stakes are high enough to justify the effort and the situation is complex enough that surface-level analysis misses things. For simple decisions, or reversible ones, it's probably overkill. And I'm sure there are failure modes I haven't encountered yet, ways this breaks down that I won't see until they bite me.

If any of this sounds interesting, the test is pretty straightforward. Pick something you currently believe, or a decision you're facing. Take it as true and push until you find something non-obvious. Take it as false and do the same. Compare what came up. If the process found something you wouldn't have found otherwise, it might be worth doing again. If it didn't, either the belief was simpler than it looked, or the process didn't go deep enough, or maybe it just doesn't work for everyone.

I'm still figuring this out myself, honestly. But the more I've paid attention to how I think, the more I've noticed the asymmetries, and the more I've wanted tools that force me to correct for them. This is one attempt at such a tool. I'm curious whether others have found similar things, or better things, or whether this is all just elaborate procrastination disguised as rigor.

The tools described here are available at: https://github.com/benjam3n/reasoningtool
