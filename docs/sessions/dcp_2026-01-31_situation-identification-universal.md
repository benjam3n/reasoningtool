

# DCP — Correctly Identifying What Situation You're Actually In

**Date**: 2026-01-31
**Input**: correctly identifying what situation you're actually in — the universal upstream step before any decision procedure

---

## Why This Procedure Exists

Every decision procedure has a Step 0: classify the situation so you know which procedure to follow. If you misclassify at Step 0, you execute the wrong procedure perfectly and arrive at the wrong answer with high confidence. This is the most dangerous failure mode in decision-making — not making bad decisions, but making good decisions about the wrong problem.

This procedure exists to be run *before* any other procedure. It answers one question: **What is actually happening here?**

---

## Step 1: Decision Dimensions

The following 12 dimensions determine whether you have correctly identified the situation you are in.

| # | Dimension | What It Captures |
|---|-----------|-----------------|
| 1 | **Surface-depth gap** | How far apart are the apparent problem and the actual problem? |
| 2 | **Novelty level** | Is this genuinely new, or does it resemble something you have seen before? |
| 3 | **Emotional loading** | How much is your emotional state shaping what you perceive? |
| 4 | **Stakeholder agreement** | Do all involved parties describe the same situation? |
| 5 | **Time pressure** | How much time do you have before you must act? |
| 6 | **Information completeness** | What fraction of the relevant facts do you actually have? |
| 7 | **Framing source** | Who or what framed the situation for you, and what did they leave out? |
| 8 | **Pattern-match confidence** | How certain are you that this matches a known pattern, and how dangerous is that certainty? |
| 9 | **Reversibility** | If you classify wrong, can you reclassify later, or are you locked in? |
| 10 | **Self-involvement** | Are you a disinterested observer or a participant with stakes? |
| 11 | **Causal clarity** | Can you trace the chain from cause to symptom, or are you seeing effects only? |
| 12 | **Classification stability** | Does the situation keep shifting, or has it settled into a legible form? |

---

## Step 2: Options per Dimension

### Dimension 1: Surface-Depth Gap
- **(a) No gap.** The apparent problem is the actual problem. (Example: "The server is down" and it is in fact down.)
- **(b) Moderate gap.** The apparent problem is real but is a symptom of something deeper. (Example: "We keep missing deadlines" — real, but caused by scope ambiguity nobody is naming.)
- **(c) Total inversion.** The stated problem is not the problem at all. (Example: "We need to hire more engineers" when the actual problem is the architecture makes engineers unproductive.)
- **(d) No problem exists.** Someone framed a non-problem as a problem. (Example: "Our conversion rate dropped" but it dropped because you changed how you measure it.)

### Dimension 2: Novelty Level
- **(a) Genuinely novel.** Nothing in your experience maps onto this. You have no pattern to match.
- **(b) Novel but analogous.** You have never seen this exact thing, but it structurally resembles something you have dealt with.
- **(c) Familiar with surface variation.** Same underlying situation as before, different cosmetic details.
- **(d) Identical recurrence.** You have been in this exact situation before and the dynamics are the same.
- **(e) Deceptively familiar.** It LOOKS like something you have seen but the underlying dynamics are different. THIS IS THE MOST DANGEROUS CATEGORY.

### Dimension 3: Emotional Loading
- **(a) Minimal.** You have no emotional stake. You can assess dispassionately.
- **(b) Moderate activation.** You care about the outcome but can still distinguish observation from reaction.
- **(c) High activation.** Strong emotions are present. Anger, fear, excitement, or shame is actively shaping your perception.
- **(d) Overwhelm.** Emotional flooding is occurring. Rational assessment is significantly degraded.

### Dimension 4: Stakeholder Agreement
- **(a) Full agreement.** Everyone involved describes the same situation in compatible terms.
- **(b) Partial agreement.** People agree on some facts but diverge on interpretation.
- **(c) Active disagreement.** Different people describe fundamentally different situations.
- **(d) Hidden disagreement.** People appear to agree but are actually describing different situations using the same words.
- **(e) Solo assessment.** You are the only one assessing. No external check is available.

### Dimension 5: Time Pressure
- **(a) No deadline.** You can take as long as you need.
- **(b) Soft deadline.** There is a cost to delay, but you have days or weeks.
- **(c) Hard deadline.** You must classify and act within hours.
- **(d) Immediate.** You must classify and act within minutes. Skip to Section C of the procedure.

### Dimension 6: Information Completeness
- **(a) High.** You have most of the relevant facts and know what you are missing.
- **(b) Moderate.** You have some facts but significant gaps exist that you can identify.
- **(c) Low with known unknowns.** You know little but can name what you do not know.
- **(d) Low with unknown unknowns.** You know little and cannot even identify what information would change your assessment.

### Dimension 7: Framing Source
- **(a) Self-framed.** You encountered the raw situation and formed your own frame.
- **(b) Single external source.** One person or document told you what the situation is.
- **(c) Consensus frame.** A group has collectively defined the situation.
- **(d) Institutional frame.** An organization, culture, or system has pre-classified this for you.
- **(e) Adversarial frame.** Someone with interests opposed to yours has defined the situation.

### Dimension 8: Pattern-Match Confidence
- **(a) No pattern recognized.** You do not know what this is.
- **(b) Tentative match.** You see a possible pattern but are not confident.
- **(c) Strong match.** You are quite confident this is a known pattern. WARNING: This is where premature closure occurs most often.
- **(d) Certain match.** You have no doubt what this is. WARNING: Maximum danger of misclassification. Certainty shuts down further investigation.

### Dimension 9: Reversibility
- **(a) Fully reversible.** You can reclassify at any point with no cost.
- **(b) Reversible with cost.** You can reclassify but will lose time, money, or credibility.
- **(c) Partially reversible.** Some consequences of your initial classification cannot be undone.
- **(d) Irreversible.** Once you classify and act, you cannot go back. Your classification becomes reality.

### Dimension 10: Self-Involvement
- **(a) Disinterested observer.** You have no stake in what the answer turns out to be.
- **(b) Interested observer.** You prefer one classification over another but can override that preference.
- **(c) Direct participant.** You are part of the situation. Your assessment changes the situation.
- **(d) Central actor.** You may BE the problem or the cause. Your self-assessment is structurally compromised.

### Dimension 11: Causal Clarity
- **(a) Clear causal chain.** You can trace cause to effect in a verifiable sequence.
- **(b) Probable causation.** You have a plausible causal story but cannot fully verify it.
- **(c) Correlation only.** You see things happening together but cannot establish causation.
- **(d) Opaque.** You see effects but have no causal model at all.

### Dimension 12: Classification Stability
- **(a) Stable.** The situation is not changing while you assess it.
- **(b) Slow-moving.** The situation is evolving but slowly enough to assess.
- **(c) Fast-moving.** The situation is changing faster than you can complete assessment.
- **(d) Chaotic.** The situation is changing in unpredictable ways. Classification may be impossible; triage instead.

---

## Step 3: Hidden Assumptions

The standard approach to situation identification assumes the following. Each assumption can be wrong.

### Assumption 1: The situation is classifiable
**How it fails:** Some situations are genuinely novel or exist between categories. Forcing them into a category distorts them. A startup experiencing something that is neither a "product-market fit problem" nor a "team problem" but an emergent interaction between the two will be misclassified by anyone who insists it must be one or the other.

### Assumption 2: There is one correct classification
**How it fails:** Many situations are legitimately multiple things at once. A conflict at work can simultaneously be a personality clash, a structural incentive misalignment, and a communication failure. Choosing one lens blinds you to the others.

### Assumption 3: The observer is separate from the situation
**How it fails:** In most human situations, the act of assessing the situation changes it. Asking "is my marriage in trouble?" changes the marriage. Investigating "is this team dysfunctional?" changes team dynamics. Your assessment is an intervention.

### Assumption 4: More information improves classification
**How it fails:** Additional information can increase confusion if you lack a framework to integrate it. It can also introduce noise that drowns out signal. In some cases, the first 20% of information gives you 80% of the classification accuracy, and further data just generates false precision.

### Assumption 5: Other people's classifications are data
**How it fails:** Other people's classifications are not neutral data points. They are assertions shaped by their own interests, biases, and information sets. Treating them as evidence rather than claims is a category error.

### Assumption 6: The situation has settled into its final form
**How it fails:** You may be assessing mid-process. What looks like a crisis may be a transition. What looks like stability may be the calm before a shift. The snapshot you are assessing may not represent the trajectory.

### Assumption 7: Your past experience is relevant
**How it fails:** Every situation you have successfully navigated creates a pattern that your brain will try to match against future situations. The more experienced you are, the faster you pattern-match, and the more confidently you misclassify novel situations as familiar ones.

### Assumption 8: Language accurately describes the situation
**How it fails:** The words available to you constrain what you can perceive. If your vocabulary for interpersonal dynamics consists of "he is a jerk" and "she is nice," you will classify all interpersonal situations into those buckets. The map is not the territory, and the vocabulary is not the map.

### Assumption 9: You want to identify the situation correctly
**How it fails:** Sometimes correct identification is threatening. If the situation is "I am the problem," or "this cannot be fixed," or "I have been wrong for years," the psychic cost of correct identification may cause you to unconsciously avoid it.

---

## Step 4: The Procedure

### STEP 0: TRIAGE — What kind of situation assessment is this?

**Action:** Answer the following three questions out loud or in writing. Do not skip this.

**0.1** How much time do I have before I must act?
- Less than 10 minutes → Go to **SECTION C** (Urgent)
- More than 10 minutes → Continue to 0.2

**0.2** Are other people telling me different things about what is happening?
- Yes, and their accounts conflict in substance → Go to **SECTION D** (Contested)
- No, or disagreements are minor → Continue to 0.3

**0.3** Have I seen something that looks like this before?
- Yes, I immediately recognize a pattern → Go to **SECTION B** (Familiar-seeming — DANGER ZONE)
- No, this feels new or confusing → Go to **SECTION A** (Novel)

---

### SECTION A: First-Time / Novel Situation

Use this when you genuinely do not recognize what is happening. The goal is to build an accurate description from raw observation before applying any framework.

**A.1 — Describe observables only.**
Write down what you can directly observe. No interpretations, no causes, no judgments.

WHAT YOU SHOULD SEE: A list of facts that a camera or microphone could record. "Revenue dropped 15% in Q3." NOT "The business is failing." "My partner did not respond to three messages." NOT "My partner is pulling away."

*Binary check:* Read each item. Does it contain a verb like "is," "feels," "seems," or "means"? If yes, rewrite it as a pure observable. Only proceed when every item passes.

**A.2 — List what you do NOT know.**
For each observable from A.1, write down what additional information would change your interpretation of it.

WHAT YOU SHOULD SEE: A list of questions, not answers. "Did the revenue drop happen across all segments or just one?" "Was my partner's phone working during that period?"

*Binary check:* Do you have at least one unknown for each observable? If not, you are likely treating assumptions as knowns. Go back and add genuine unknowns.

**A.3 — Generate three competing descriptions.**
Using only the observables from A.1, write three different one-sentence descriptions of the situation that are all consistent with the evidence.

WHAT YOU SHOULD SEE: Three descriptions that are meaningfully different from each other, not cosmetic variations. Example: (1) "The company is losing product-market fit." (2) "The company's go-to-market motion broke when the sales lead left." (3) "The market contracted and this has nothing to do with us."

*Binary check:* Would each description lead you to a different action? If two descriptions lead to the same action, they are not different enough. Replace one.

**A.4 — Identify which description you WANT to be true.**
Write down which of the three descriptions you are drawn to, and why.

WHAT YOU SHOULD SEE: An honest statement like "I want it to be #3 because that means it is not my fault" or "I want it to be #1 because that is the one I know how to fix."

*Binary check:* Does your preferred description also happen to be the most comfortable one for you personally? If yes, flag this as a bias risk and do not weight it higher.

**A.5 — Select the description you can most cheaply test.**
Of the three descriptions, which one can you gather evidence for or against most quickly and cheaply?

WHAT YOU SHOULD SEE: A specific action you can take in the next 24 hours to get data. "Pull the revenue by segment." "Ask my partner directly if something is wrong." "Check if competitors saw the same drop."

*Binary check:* Does the test have a clear outcome that distinguishes between at least two of your three descriptions? If no, find a better test.

**A.6 — Run the test. Update. Repeat or proceed.**
Execute the test from A.5. Based on the results:
- One description clearly wins → Proceed with that classification. Go to **VALIDATION CHECKPOINT** below.
- Still ambiguous → Return to A.3 with new information and generate new descriptions.
- You have cycled three times with no convergence → Label this situation as "genuinely ambiguous" and proceed with the most reversible action available.

---

### SECTION B: Familiar-Seeming Situation (DANGER: Pattern-Matching Errors)

Use this when your brain immediately says "I know what this is." This is the most dangerous section because the confidence of recognition suppresses further investigation.

**B.1 — Name the pattern you are matching to.**
Write down in one sentence what you think this situation is, and when you last saw it.

WHAT YOU SHOULD SEE: Something like "This is a scope creep problem, same as the Q2 project last year" or "This is my mother being manipulative, same pattern as always."

**B.2 — Force yourself to list three differences.**
Between this situation and the one you are matching it to, write down three things that are different. They can be small.

WHAT YOU SHOULD SEE: At least three concrete differences. "Different team members." "Higher stakes." "I am more tired this time." If you cannot list three differences, you are not looking hard enough — every situation has differences.

*Binary check:* Is any of the three differences structural rather than cosmetic? (A structural difference changes how the situation works, not just how it looks.) If yes, proceed to B.3 with heightened caution. If all three are cosmetic, the pattern match may be valid — proceed to B.4.

**B.3 — Ask: What would I see if this were NOT the pattern I think it is?**
Write down three observable things that would be present if your pattern match is wrong.

WHAT YOU SHOULD SEE: Specific, checkable indicators. "If this is NOT scope creep, I would expect the original requirements to still be ambiguous even if no new requests came in." "If this is NOT manipulation, I would expect my mother to accept my boundary when I state it clearly."

*Binary check:* Can you check for each indicator right now or within 24 hours? If yes, check them. If any indicator is present, your pattern match is wrong. Go to **SECTION A** and start fresh.

**B.4 — Check for the "Same Person, Different Problem" error.**
If the situation involves another person you have dealt with before, answer: Am I classifying the situation based on who is involved rather than what is happening?

*Binary check:* If you removed the identity of the person and described only the behavior, would you classify the situation the same way? If no, you are pattern-matching to the person, not the situation. Go to **SECTION A**.

**B.5 — Proceed with your classification but set a checkpoint.**
If you have passed all checks, your pattern match is likely valid. Proceed. But set a concrete future checkpoint: "If [specific observable] has not occurred by [date], I will reclassify."

WHAT YOU SHOULD SEE: A written statement with a specific observable and a specific date. Not "I will reassess if things do not improve" but "If the client has not signed by February 15, this is not a timing problem and I will reclassify as a fit problem."

---

### SECTION C: Urgent Situation (Limited Time)

Use this when you have less than 10 minutes before you must act. This is a stripped-down version designed for speed.

**C.1 — State what you think is happening in one sentence.**
Do not deliberate. Write or say the first classification that comes to mind.

**C.2 — Ask one question: What is the worst thing that happens if I am wrong?**
- Consequences are reversible or minor → Act on your classification. Reassess after.
- Consequences are severe and irreversible → Continue to C.3.

**C.3 — Generate exactly one alternative classification.**
The alternative must be the most dangerous plausible interpretation. Not the most likely — the most dangerous.

WHAT YOU SHOULD SEE: Your original classification and one alternative. "I think this is a server outage" vs. "This could be a security breach." "I think my child is overtired" vs. "My child could be having a medical event."

**C.4 — Ask: Can I act in a way that covers both classifications?**
- Yes → Do that. Reassess when time pressure lifts.
- No → Act on whichever classification has worse consequences if you are wrong and do nothing. This is minimax: minimize maximum regret.

**C.5 — After acting, mandatory reassessment.**
Once the immediate pressure has passed, return to **SECTION A** or **SECTION B** and do a full assessment. Urgent classifications are provisional. Never let them harden into permanent ones.

---

### SECTION D: Contested Situation (Multiple People Disagree)

Use this when different people are telling you different things about what is happening.

**D.1 — Collect all classifications.**
Write down every distinct description of the situation offered by anyone involved. Use their words, not your paraphrases.

WHAT YOU SHOULD SEE: A numbered list of direct quotes or near-quotes. "Marketing says: 'Sales is not following up on our leads.'" "Sales says: 'Marketing is sending us unqualified leads.'"

**D.2 — For each classification, write down what is true about it.**
Do not evaluate which is "right." For each one, identify what observable evidence supports it.

WHAT YOU SHOULD SEE: Evidence listed under each classification. Not "Marketing is right because..." but "Evidence supporting Marketing's view: lead response time data shows 48-hour average. Evidence supporting Sales' view: lead qualification score data shows 60% below threshold."

*Binary check:* Does each classification have at least some supporting evidence? If a classification has zero evidence, discard it. If every classification has some evidence, continue.

**D.3 — Identify what each classifier would lose if they are wrong.**
For each person or group offering a classification, write down what it would cost them personally or professionally if their classification turned out to be incorrect.

WHAT YOU SHOULD SEE: Concrete stakes. "If Marketing is wrong, it means their campaign strategy failed." "If Sales is wrong, it means their follow-up process is broken."

*Binary check:* Is there a classification where the person has nothing to lose by being wrong? That person's classification is more likely to be accurate (less motivated reasoning). Weight it accordingly.

**D.4 — Look for the classification nobody is offering.**
Write down one description of the situation that no involved party has suggested.

WHAT YOU SHOULD SEE: A description that would be uncomfortable for everyone. "Both Marketing and Sales are performing fine, but the product is no longer competitive." This is often the correct classification because it is the one nobody is incentivized to see.

*Binary check:* Is the missing classification one that would require systemic change rather than blaming a specific party? If yes, investigate it seriously.

**D.5 — Test the contested classifications.**
Identify one metric, fact, or experiment that would distinguish between the top two competing classifications. Execute it. Return to D.2 with the new evidence and reassess.

---

### VALIDATION CHECKPOINT

Run this after any section produces a classification.

**V.1 — State your classification in one sentence.**

**V.2 — What action does this classification imply?**

**V.3 — If you took that action and the situation got worse, what would that tell you?**

WHAT YOU SHOULD SEE: An answer that would cause you to reclassify. "If I treat this as a performance problem and coach the employee, and their performance gets worse, that would tell me it is actually a motivation or structural problem."

**V.4 — Set a reclassification trigger.**
Write down: "I will reclassify if [specific observable] by [specific date]."

**V.5 — Proceed to the appropriate decision procedure for your classified situation.**

---

## Step 5: Failure Modes

### Failure Mode 1: Premature Pattern Matching
**What happens:** You recognize the situation instantly and skip investigation. Your brain completes the pattern from partial data and you act on the completed pattern instead of the actual situation.
**How to detect:** You classified in under 30 seconds and feel confident. High confidence plus fast classification is a red flag, not a green one.
**How to recover:** Force yourself through Section B regardless of confidence.

### Failure Mode 2: Emotional Contamination
**What happens:** Your emotional state becomes the lens through which you see the situation. When you are anxious, every situation looks like a threat. When you are angry, every situation looks like someone's fault. When you are excited, every situation looks like an opportunity.
**How to detect:** Ask yourself: "If I were in a completely neutral emotional state, would I classify this the same way?" If you cannot honestly answer yes, your classification is contaminated.
**How to recover:** Wait until emotional intensity drops below 4/10 before classifying, or have someone with no emotional stake classify independently.

### Failure Mode 3: Authority Bias
**What happens:** Someone with status, expertise, or confidence tells you what the situation is, and you accept their classification without independent verification.
**How to detect:** Ask: "Did I arrive at this classification independently, or did someone hand it to me?" If someone handed it to you, treat it as a hypothesis, not a fact.
**How to recover:** Regardless of who classified it, run through Section A or B yourself with the raw observables.

### Failure Mode 4: Narrative Coherence Trap
**What happens:** You select the classification that makes the best story over the one that is most accurate. Humans are story-making machines, and a classification that creates a coherent narrative (clear villain, clear cause, clear resolution) is deeply attractive even when it is wrong.
**How to detect:** Is your classification a "good story"? Does it have a clear protagonist, antagonist, and arc? Reality is usually messier than that.
**How to recover:** Deliberately construct an equally coherent narrative that leads to a different classification. If you can, your narrative coherence is not evidence.

### Failure Mode 5: Anchoring to First Information
**What happens:** The first description of the situation you encounter becomes the anchor, and all subsequent information is processed as adjustments to that anchor rather than independent data.
**How to detect:** Remove the first thing you heard about this situation. Using only information received after that, would you classify it the same way?
**How to recover:** Deliberately re-derive your classification from scratch, starting with the most recent information rather than the first information.

### Failure Mode 6: Complexity Collapse
**What happens:** The situation is genuinely complex — multiple interacting causes, no single classification applies — but you force it into a single simple category because complexity is cognitively expensive.
**How to detect:** You have discarded or de-weighted multiple pieces of contradictory evidence to maintain a clean classification. Each piece of discarded evidence nags at you slightly.
**How to recover:** Allow the classification to be compound: "This is BOTH X and Y, and the interaction between them is what matters." This is harder to work with but more accurate.

### Failure Mode 7: Self-Serving Classification
**What happens:** You classify the situation in the way that makes you look best, requires the least change from you, or absolves you of responsibility.
**How to detect:** In your classification, where are you? Are you the hero, the victim, or the bystander? If you are never the cause, you are likely self-serving.
**How to recover:** Force yourself to generate a classification in which you are the primary cause. Evaluate it with the same rigor as your preferred classification.

### Failure Mode 8: Classification Paralysis
**What happens:** You become so aware of the ways classification can go wrong that you refuse to classify at all. You gather information endlessly, generate alternatives endlessly, and never commit.
**How to detect:** You have been assessing for longer than it would take to act, recover from a wrong action, and reassess. Your assessment is now more expensive than your error.
**How to recover:** Set a hard deadline. "I will classify by [time] with whatever information I have." Remember: a wrong classification that you correct quickly is better than no classification at all.

### Failure Mode 9: Recursive Trap
**What happens:** You try to apply this procedure to itself. "What situation am I in regarding identifying what situation I am in?" This infinite regress is real but must be cut off.
**How to detect:** You are reading this procedure for the third time without having written anything down.
**How to recover:** Start with Step A.1. Write down observables. The procedure is grounded in physical observation, which breaks the recursion.

---

## Step 6: Validation

Every step in the procedure above has been checked against the following criteria:

| Step | Is it a concrete action? | Does it have a binary check? | Can someone with no expertise execute it? | Does it produce a visible artifact? |
|------|------------------------|---------------------------|----------------------------------------|-----------------------------------|
| A.1 | Yes — write observables | Yes — camera test | Yes | Written list |
| A.2 | Yes — write unknowns | Yes — one per observable | Yes | Written list |
| A.3 | Yes — write three descriptions | Yes — different actions test | Yes | Three sentences |
| A.4 | Yes — write preference | Yes — comfort test | Yes | Written statement |
| A.5 | Yes — select testable description | Yes — distinguishing outcome test | Yes | Specific test action |
| A.6 | Yes — run test | Yes — convergence after 3 cycles | Yes | Test result |
| B.1 | Yes — write pattern | N/A | Yes | Written sentence |
| B.2 | Yes — list differences | Yes — structural test | Yes | Written list |
| B.3 | Yes — list counter-indicators | Yes — can check within 24h | Yes | Written list |
| B.4 | Yes — identity removal test | Yes — same classification? | Yes | Yes/No answer |
| B.5 | Yes — set checkpoint | Yes — has date and observable | Yes | Written statement |
| C.1 | Yes — state classification | N/A | Yes | One sentence |
| C.2 | Yes — assess consequences | Yes — reversible or not | Yes | One answer |
| C.3 | Yes — generate alternative | N/A | Yes | One sentence |
| C.4 | Yes — find covering action | Yes — possible or not | Yes | One action |
| C.5 | Yes — reassess after | N/A | Yes | Return to A or B |
| D.1 | Yes — collect quotes | N/A | Yes | Numbered list |
| D.2 | Yes — list evidence per view | Yes — each has evidence? | Yes | Evidence lists |
| D.3 | Yes — list stakes per classifier | Yes — zero-stake classifier? | Yes | Written list |
| D.4 | Yes — generate missing classification | Yes — systemic change test | Yes | One sentence |
| D.5 | Yes — identify distinguishing test | N/A | Yes | One test |

---

## QUICK REFERENCE CARD

```
SITUATION IDENTIFICATION — QUICK REFERENCE
============================================

BEFORE ANYTHING ELSE:
  1. How much time? (<10 min → Section C)
  2. Do people disagree? (Yes → Section D)
  3. Do I recognize this? (Yes → Section B / No → Section A)

SECTION A (NOVEL):
  Write observables → List unknowns → 3 descriptions →
  Which do I WANT? → Test cheapest → Act or loop

SECTION B (FAMILIAR — DANGER):
  Name the pattern → 3 differences → What if I am wrong? →
  Person vs. situation check → Proceed with checkpoint

SECTION C (URGENT):
  One-sentence classification → Worst case if wrong? →
  One dangerous alternative → Cover both or minimax → Reassess later

SECTION D (CONTESTED):
  Collect all views → Evidence for each → Stakes per person →
  Missing classification → Test to distinguish

ALWAYS END WITH:
  Classification sentence → Implied action →
  What if it gets worse? → Reclassification trigger

RED FLAGS:
  - Classified in <30 seconds with high confidence
  - Classification makes a "good story"
  - You are never the cause in your classification
  - Someone else handed you the classification
  - Your preferred classification is the comfortable one
```

---

## COMMON MISTAKES

### Mistake 1: Treating the symptom as the situation
You see someone crying and classify the situation as "person is sad." The situation may be: person is frustrated, person is relieved, person is manipulating, person is having an allergic reaction. The crying is an observable. The situation is what produced it. Always separate the observable from the interpretation.

### Mistake 2: Classifying based on what you can fix
If you are a hammer, every situation looks like a nail. Engineers classify interpersonal problems as systems problems. Therapists classify systems problems as interpersonal problems. You will unconsciously classify the situation as something within your skillset. Ask: "Would someone from a completely different background classify this the same way?"

### Mistake 3: Letting the most vocal person define the situation
In any group, one person typically speaks first, speaks loudest, and speaks with the most confidence. Their classification becomes the default. This has nothing to do with accuracy. Actively solicit the classification of the quietest person in the room.

### Mistake 4: Updating your classification without realizing it
You start with classification A. New information arrives that is mildly inconsistent with A. You do not reclassify — you subtly shift A to accommodate the new information, creating A', then A'', then A'''. By the end, your classification has drifted far from where it started, but you feel like you have been consistent. Write your classification down at the start. Compare it to your current classification periodically.

### Mistake 5: Confusing "I don't know what this is" with "this is not important"
Situations that resist classification feel uncomfortable. The most common escape from that discomfort is to minimize: "It is probably nothing." "I am probably overthinking this." If you cannot classify it, that is information. Something that resists classification deserves MORE attention, not less.

### Mistake 6: Using probability language to mask uncertainty
Saying "There is a 70% chance this is a market problem" sounds rigorous but is often a disguise for "I think this is a market problem and I want to sound calibrated." Unless you have a real base rate (from data, not intuition), your probability estimate is your confidence dressed up as math. Say "I believe" rather than "there is an X% chance" unless you can justify the number.

### Mistake 7: Classifying the situation once and never revisiting
Situations evolve. The correct classification on Monday may be wrong by Friday. Every classification is provisional. The checkpoint you set in the procedure is not optional — it is the mechanism that prevents stale classifications from driving current decisions.

---

## WHEN TO OVERRIDE THIS PROCEDURE

This procedure should be overridden in the following circumstances:

1. **True emergency with lives at stake.** If someone is in physical danger, act on instinct. Assess later. Do not run a procedure while someone is bleeding.

2. **The situation is genuinely trivial.** If the consequences of misclassification are less than the cost of running this procedure, skip it. Use judgment: "What's the worst that happens if I'm wrong?" If the answer is "I waste 20 minutes," do not spend 30 minutes on classification.

3. **You have domain expertise and verified track record.** If you are an emergency room doctor assessing a patient, you have earned the right to pattern-match fast. But only within your domain. The ER doctor should not pattern-match on business strategy.

4. **The system is already providing feedback.** If you are in an environment with fast, clear feedback loops (writing code with tests, cooking with tasting, playing a sport with a scoreboard), let the feedback loop correct your classification rather than front-loading analysis.

5. **You are stuck in a recursive loop.** If applying this procedure is itself creating paralysis, stop. Pick the most reversible classification, act on it, and let reality tell you whether you were right.

---

## WORKED EXAMPLES

### Example 1: Personal — "My friend is being distant"

**Step 0 — Triage:**
- Time pressure? No. → Continue.
- Others disagree about what is happening? No, this is solo assessment. → Continue.
- Do I recognize this pattern? Yes — this feels like when my college friend pulled away before ending the friendship. → **SECTION B.**

**Section B:**
- **B.1:** "This is the same as when Alex withdrew before cutting me off. Pattern: friend creates distance as precursor to ending relationship."
- **B.2:** Three differences: (1) This friend has been under work stress that Alex was not. (2) The "distance" is slower response times, not no responses. (3) We have been friends for 10 years, not 2.
- **B.3:** Structural difference check: Work stress is structural — it changes the dynamics because the distance may be resource-driven, not relationship-driven. Proceed with heightened caution.
- **B.4:** "What if this is NOT withdrawal?" I would expect: they still initiate contact sometimes (check: yes, they texted me last week). I would expect: they are distant with everyone, not just me (check: I can ask a mutual friend). One indicator present — pattern match may be wrong.
- **B.5:** Going to Section A with the observables.

**Section A:**
- **A.1:** Observables: Response time increased from hours to days. Last three invitations declined. Still initiates occasionally. Work promotion happened two months ago.
- **A.2:** Unknowns: Are they distant with others? What is their current workload? Have I done something specific?
- **A.3:** Three descriptions: (1) Friend is overwhelmed at work and has less social bandwidth. (2) Friend is creating distance because something in our friendship changed. (3) Friend is going through something personal they have not shared.
- **A.4:** I want #1 to be true because it is not about me and will resolve itself.
- **A.5:** Cheapest test: Ask a mutual friend if they have noticed the same distance. This distinguishes #1 (they would be distant with everyone) from #2 (only with me).
- **A.6:** Test result: mutual friend says "Yeah, they have been swamped since the promotion, I have barely seen them either." → Classification: work-driven reduced bandwidth. Not about the friendship.

**Validation:** If I proceed as if this is work-driven and they are still distant in two months even after work calms down, I will reclassify.

---

### Example 2: Business — "Our product launch failed"

**Step 0 — Triage:**
- Time pressure? Soft — board meeting in two weeks. → Continue.
- Others disagree? Yes — engineering says it was a market timing issue, marketing says the product was not ready, sales says neither team supported them. → **SECTION D.**

**Section D:**
- **D.1:** Engineering: "The market was not ready for this product. We were too early." Marketing: "The product had critical bugs that undermined our launch messaging." Sales: "We had no sales enablement materials and no engineering support for demos."
- **D.2:** Evidence for engineering's view: Two competitors launched similar products and also underperformed. Evidence for marketing's view: Bug tracker shows 47 unresolved P1 issues at launch. Evidence for sales' view: Sales enablement folder was empty, demo environment crashed during three client meetings.
- **D.3:** If engineering is wrong, it means they shipped a buggy product. If marketing is wrong, it means their messaging and positioning failed. If sales is wrong, it means their pipeline and relationships were not strong enough. Everyone has something to lose.
- **D.4:** Classification nobody is offering: "The product was conceived without customer input and does not solve a real problem. Launch execution is irrelevant because demand does not exist." This is uncomfortable for everyone because it invalidates the entire initiative.
- **D.5:** Test to distinguish: Pull data on inbound interest (demo requests, organic sign-ups) independent of outbound sales efforts. If inbound interest is near zero, the missing classification is likely correct — demand does not exist.

**Result:** Inbound interest was 12 requests in 6 weeks, vs. a target of 200. The product does not have market demand. The launch did not "fail" — the product was built without validating demand. Every team's classification was a downstream symptom of this upstream error.

**Validation:** Reclassification trigger: "If inbound interest increases to 50+/month after we implement marketing's suggested improvements, the demand exists and my classification is wrong."

---

### Example 3: Technical — "The system is slow"

**Step 0 — Triage:**
- Time pressure? Hard — customers are complaining, SLA breach in 4 hours. Not yet under 10 minutes, but close. Continue with urgency.
- Others disagree? Yes — backend says it is the database, frontend says it is the API, ops says it is the infrastructure. → **SECTION D**, but with time constraints from **SECTION C**.

**Hybrid C+D approach:**
- **D.1 (fast):** Backend: "Database queries are timing out." Frontend: "API response times tripled." Ops: "CPU utilization spiked on two nodes."
- **D.2 (fast):** All three observations can be true simultaneously and can share a single cause. These may not be competing classifications but different views of the same event.
- **Key question:** Is there a common upstream cause? A database problem would cause slow queries, which would cause slow API responses, which would cause CPU spikes as requests queue up. The causal chain runs: database → API → infrastructure symptoms.
- **C.3 — Dangerous alternative:** This is not a performance problem — this is a data corruption event or a security breach that manifests as slowness.
- **C.4 — Covering action:** Investigate the database first (addresses the performance hypothesis) while simultaneously checking access logs (addresses the security hypothesis).

**Result:** Database investigation reveals a missing index on a table that grew past a threshold overnight due to a batch import. The system is not "slow" — one specific query pattern became O(n) instead of O(log n). The situation is not "system performance" — it is "data growth exceeded an architectural assumption."

**Validation:** After adding the index, if response times do not return to normal within 30 minutes, this is not the root cause and reassessment is needed. (Response times returned to normal in 4 minutes.)

---

## Final Note

This procedure is itself subject to its own warnings. You are reading a classification system for situations, and you will be tempted to classify it as "just another framework" or "obviously correct" or "too complicated for real use." None of those classifications have been tested. Use the procedure on itself if you want: What observables tell you whether this procedure is useful? What would you see if it were not? What is the cheapest test?

The answer to that last question is: try it once on a real situation. That is the only test that matters.