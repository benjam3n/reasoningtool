# Decision Procedure — Managing the Arrival of Superintelligence

**Date**: 2026-01-31
**Input**: managing the arrival of superintelligence

---

## STEP 1: Decision Dimensions

What factors determine the right choices when managing the arrival of superintelligence?

### Dimensions Identified

**D1: Timeline Horizon**
How soon do you believe transformative AI/superintelligence arrives? This determines urgency and which levers are still available. (Months, 1-3 years, 3-10 years, 10+ years, uncertain)

**D2: Your Role / Locus of Control**
What is your relationship to the development? Are you a policymaker, AI lab employee, researcher, investor, ordinary citizen, or organizational leader? This determines which actions are actually available to you.

**D3: Alignment Confidence**
How confident are you that the leading systems will be aligned with human values at the point of crossing the superintelligence threshold? (High confidence, moderate, low, no idea)

**D4: Concentration of Power**
Is superintelligence likely to be developed by one actor (monopoly), a small oligopoly, or broadly distributed? This shapes whether governance, competition, or coordination is the primary lever.

**D5: Speed of Takeoff**
Do you expect a slow, gradual capability increase (soft takeoff) or a rapid, discontinuous jump (hard takeoff)? This determines whether iterative governance works or if pre-commitment is necessary.

**D6: Controllability Assumption**
Do you believe superintelligent systems can be reliably controlled, monitored, or shut down after deployment? Or do you believe that once deployed, control is effectively lost?

**D7: Value Plurality**
Whose values should superintelligence be aligned to? A single culture, a democratic aggregation, universal human rights, or something emergent? This determines governance structure.

**D8: Economic Disruption Severity**
How much economic displacement do you expect (gradual automation vs. mass unemployment shock)? This determines the urgency of safety-net and transition planning.

**D9: Geopolitical Competition Pressure**
How much is the decision constrained by international competition (AI arms race dynamics)? High competition pressure forces faster timelines and less caution.

**D10: Catastrophic Risk Tolerance**
What probability of civilizational catastrophe is acceptable? Zero tolerance demands very different actions than "small risk is worth it for the upside."

**D11: Institutional Trust**
Do you trust existing institutions (governments, international bodies, AI labs) to manage this well? This determines whether you work within systems or build alternatives.

**D12: Reversibility Requirement**
How important is it that early decisions be reversible? High reversibility preference favors conservative, staged approaches.

---

## STEP 2: Options per Dimension

### D1: Timeline Horizon
| Value | Implications |
|---|---|
| **< 1 year** | Emergency mode. Only actions already in motion matter. Focus on damage control and rapid coordination. |
| **1-3 years** | Urgent but some runway. Policy and technical safety work can still be initiated. |
| **3-10 years** | Standard planning horizon. Institutional reform, international treaties, and research programs are viable. |
| **10+ years** | Long-term institution building. Education, cultural preparation, economic transition. |
| **Uncertain** | Must plan for multiple timelines simultaneously. Hedge across scenarios. |

### D2: Your Role
| Value | Available Actions |
|---|---|
| **AI Lab Employee** | Internal advocacy, whistleblowing, alignment research, responsible scaling commitments |
| **Policymaker** | Regulation, funding allocation, international negotiations, compute governance |
| **Researcher** | Technical alignment work, interpretability, evaluation/benchmarking, public communication |
| **Investor/Funder** | Capital allocation toward safety, funding governance orgs, conditional investment |
| **Organizational Leader** | Workforce transition, AI policy, advocacy, institutional preparation |
| **Citizen** | Voting, advocacy, personal preparation, community building, career choices |

### D3: Alignment Confidence
| Value | Strategy |
|---|---|
| **High** | Focus on governance, distribution, and economic transition |
| **Moderate** | Balanced portfolio: alignment research + governance + monitoring |
| **Low** | Prioritize alignment research, slow deployment, mandatory evaluation |
| **No idea** | Maximize optionality, invest in understanding before acting |

### D4: Concentration of Power
| Value | Priority |
|---|---|
| **Monopoly** | Governance of the single actor; prevent abuse of power |
| **Oligopoly** | Coordination between actors; prevent race dynamics |
| **Distributed** | Standards, interoperability, collective defense against misuse |

### D5: Speed of Takeoff
| Value | Approach |
|---|---|
| **Soft** | Iterative governance; adjust as capabilities increase |
| **Hard** | Pre-committed safety measures; tripwires; automatic shutdowns |
| **Unknown** | Design for hard takeoff (worst case), hope for soft |

### D6: Controllability
| Value | Approach |
|---|---|
| **Controllable** | Focus on governance, access controls, monitoring |
| **Uncontrollable post-deployment** | All safety must be built in before deployment; no second chances |
| **Unknown** | Act as if uncontrollable; invest in verifying controllability |

**Key interactions:**
- If D1 is short AND D5 is hard → emergency protocols override everything
- If D4 is monopoly AND D11 is low trust → adversarial governance needed
- If D10 is zero tolerance AND D3 is low → strong case for pause/moratorium
- If D2 is citizen AND D1 is long → education and advocacy dominate

---

## STEP 3: Hidden Assumptions

The conventional discourse around managing superintelligence makes these assumptions, many of which may be wrong:

**A1: "We'll see it coming."**
Assumes there will be clear warning signs before superintelligence emerges. Reality: capability jumps can be sudden, internal to labs, and not publicly visible until deployment.

**A2: "Alignment is a technical problem."**
Assumes that if we solve the math/engineering, we're safe. Misses that alignment is also a political problem (aligned to whom?) and an organizational problem (will the solution be implemented?).

**A3: "Governance can keep pace with capability."**
Assumes regulatory and institutional frameworks can adapt fast enough. Historical precedent (nuclear, biotech, social media) suggests they cannot without extraordinary effort.

**A4: "The main risk is misalignment."**
Focuses on AI not doing what we want. Equally dangerous: AI doing exactly what a bad actor wants, or AI doing what we asked but the request was wrong.

**A5: "International coordination is possible."**
Assumes nations will cooperate on AI governance the way they (sometimes) cooperate on nuclear weapons. May be false in a multipolar, low-trust geopolitical environment.

**A6: "Economic transition can be managed gradually."**
Assumes automation-driven unemployment will be slow enough for retraining. Superintelligence could obsolete entire sectors simultaneously.

**A7: "We get to decide."**
Assumes humans will remain the decision-makers throughout the transition. A sufficiently capable system might make this assumption irrelevant.

**A8: "More AI safety research = safer outcomes."**
Assumes safety research can't be dual-use or that publishing alignment techniques doesn't also teach manipulation techniques.

**A9: "There will be one superintelligence event."**
Assumes a single moment of arrival. More likely: a gradient of increasingly capable systems with no clear bright line.

**A10: "Individual actions matter."**
Assumes your choices make a difference. May be true (career choice, advocacy) or false (if outcomes are structurally determined). The procedure should account for this uncertainty.

---

## STEP 4: The Procedure

```
MANAGING THE ARRIVAL OF SUPERINTELLIGENCE
==========================================
Decision Procedure v1.0

WHO THIS IS FOR: Anyone -- policymaker, technologist, investor, or
citizen -- facing decisions about how to prepare for, respond to,
or influence the development of superintelligent AI.

WHAT THIS PRODUCES: A prioritized action plan specific to your
situation, updated as conditions change.
```

### STEP 0: What type of decision-maker are you?

Answer this question: **What is your primary relationship to AI development?**

| If you are... | Go to... |
|---|---|
| Working at an AI lab (any role) | **SECTION A** |
| A government official or policymaker | **SECTION B** |
| A researcher (academic or independent) | **SECTION C** |
| An investor or funder | **SECTION D** |
| A business leader (non-AI company) | **SECTION E** |
| A citizen with no direct AI role | **SECTION F** |

If you fit multiple categories, run the procedure for each and combine the action lists.

---

### SECTION A: AI Lab Employee

**A1.** Does your lab have a published, specific policy for what happens if a system shows signs of superintelligent capability (not just "we'll evaluate")?
- **Yes** → Go to A2
- **No** → **ACTION: Advocate internally for a concrete, pre-committed response plan with specific capability thresholds and automatic actions (e.g., "if the system scores above X on Y benchmark, training is paused until Z review is completed"). Write a proposal. If you lack standing, find someone senior who shares the concern.** Then go to A2.

**A2.** Does your lab have an independent safety team with real authority to stop or delay deployment?
- **Yes, with veto power** → Go to A3
- **Yes, but advisory only** → **ACTION: Push for binding authority. Document cases where advisory-only led to bad outcomes in other industries (Challenger disaster, Boeing 737 MAX). Present to leadership.** Then go to A3.
- **No** → **ACTION: This is a red flag. Escalate formally through whatever governance exists. If none exists, consider whether this is an organization you should remain at.** Then go to A3.

**A3.** Rate your confidence that your lab's current leading system is aligned (will do what humans intend, even at much higher capability levels):
- **High confidence with evidence** (you can point to specific technical results) → Go to A4
- **Moderate or vibes-based** → **ACTION: Push for rigorous, externally-verified alignment evaluations before next major capability increase. Volunteer for or support red-teaming efforts.** Then go to A4.
- **Low or none** → **ACTION: This is urgent. Write up your specific concerns with technical detail. Share with your safety team. If no safety team, share with trusted senior researchers. If you believe there is imminent risk and internal channels have failed, consult with AI safety organizations about responsible disclosure.** Then go to A4.

**A4.** Is your personal work contributing to capabilities (making systems more powerful) or safety (making systems more reliable/controllable/aligned)?
- **Capabilities** → **DECISION POINT: Given your answers to A1-A3, are you comfortable that your lab's safety measures are proportional to the capabilities you're building? If yes, continue. If no, consider transferring to safety work, or finding ways to make your capabilities work conditional on safety milestones.**
- **Safety** → Continue your work. Ensure it's connected to actual deployment decisions (not just published papers).
- **Both / unclear** → Audit your recent projects. For each, ask: "If this succeeds beyond expectations, does it make the world safer or just more capable?"

**A5.** Final check — does your lab have a credible external oversight mechanism (board, external auditors, government inspectors)?
- **Yes** → Your priority actions are the ones flagged above. Review quarterly.
- **No** → **ACTION: Advocate for external oversight. Support regulatory proposals even if they create friction for your employer. The absence of external checks on organizations building potentially world-changing technology is itself a risk factor.**

---

### SECTION B: Policymaker

**B1.** Does your jurisdiction currently have any binding regulation specifically addressing frontier AI systems (not just "AI in healthcare" or "AI in hiring")?
- **Yes, binding regulation exists** → Go to B2
- **In progress** → Go to B2, but also: **ACTION: Review the in-progress regulation against the checklist in B3. Push to strengthen any gaps.**
- **No** → **ACTION: Initiate a regulatory process. Start with compute thresholds (any training run above X FLOP requires registration and safety evaluation). This is measurable, enforceable, and technology-neutral.** Then go to B2.

**B2.** Does your government have technical staff capable of evaluating frontier AI systems (not just policy generalists)?
- **Yes** → Go to B3
- **No** → **ACTION: This is your highest priority. Hire or contract with AI safety researchers. Create a national AI evaluation body. Without technical capacity, regulation is unenforceable.** Then go to B3.

**B3.** Regulation checklist — does your framework include:
- [ ] **Pre-deployment evaluation** requirements for systems above a capability threshold
- [ ] **Mandatory incident reporting** for AI systems that behave unexpectedly
- [ ] **Compute tracking** to know who is training large models
- [ ] **Liability framework** for harms caused by AI systems
- [ ] **Emergency authority** to halt deployment if a serious risk is identified
- [ ] **International coordination mechanism** (treaty, bilateral agreement, or information-sharing)

Each unchecked box is an action item. Prioritize in this order: emergency authority, pre-deployment evaluation, compute tracking, incident reporting, liability, international coordination.

**B4.** Are you facing pressure to "not fall behind" a geopolitical competitor in AI development?
- **Yes** → **WARNING: This is the most dangerous dynamic. Race pressures cause corners to be cut. ACTION: Reframe internally from "race to build" to "race to govern." The country that establishes credible AI governance first gains soft power and sets global standards. Pursue bilateral safety agreements with competitors even while competing on capabilities.**
- **No** → Use this breathing room to build robust governance before pressure emerges.

**B5.** Economic preparation — has your government begun planning for large-scale economic disruption from AI automation?
- **Yes, specific plans exist** → Review them against the severity levels in D8 above.
- **No** → **ACTION: Commission an economic impact assessment. Begin piloting transition programs (retraining, income support, new economic models) NOW, before the disruption arrives. These programs take years to design and scale.**

---

### SECTION C: Researcher

**C1.** Is your research primarily in:
- **Technical AI safety/alignment** → Go to C2
- **AI capabilities** → Go to C3
- **AI governance/policy** → Go to C4
- **Other field affected by AI** → Go to C5

**C2 (Safety Researcher).** Is your work connected to actual deployment decisions at a frontier lab?
- **Yes** → Ensure your findings can trigger concrete actions (pause, redesign, new safeguards). If your work is "interesting but not actionable," redirect.
- **No** → **ACTION: Build relationships with frontier labs. The best safety research that nobody implements is worth nothing. Consider a residency, consulting relationship, or joint project. Alternatively, build evaluation tools that become industry standards.**

**C3 (Capabilities Researcher).** Apply the "differential technology development" test: Does your next paper advance capabilities more than safety?
- **Capabilities advance is modest; safety implications are neutral** → Proceed, but include safety-relevant observations.
- **Capabilities advance is significant** → **DECISION POINT: Can you delay publication until corresponding safety work catches up? Can you collaborate with safety researchers to pair your work with safeguards? At minimum, conduct a pre-publication risk assessment.**
- **Uncertain** → Get a second opinion from a safety-focused colleague before publishing.

**C4 (Governance Researcher).** Is your work producing actionable policy recommendations or theoretical frameworks?
- **Actionable** → Ensure policymakers can find and understand your work. Write policy briefs, testify, engage directly.
- **Theoretical** → Translate at least 30% of your output into practitioner-accessible formats.

**C5 (Affected Field).** Map how superintelligent AI would change your field. Publish this analysis. You understand your domain better than AI researchers do — your expertise on impacts is uniquely valuable.

---

### SECTION D: Investor / Funder

**D1.** Are you currently investing in AI capabilities, AI safety, or both?
- **Capabilities only** → **ACTION: Allocate at least 10-20% of your AI portfolio to safety and governance organizations. Not because it's charitable — because unsafe AI destroys value for everyone, including your capabilities investments.**
- **Safety only** → Continue. Ensure funded organizations are producing results, not just consuming funding.
- **Both** → Audit the ratio. Is it proportional to the risk?
- **Neither** → Go to D2.

**D2.** Are you prepared for the economic disruption AI may cause to your broader portfolio?
- **Yes, stress-tested** → Go to D3
- **No** → **ACTION: Run a scenario analysis. In a world where AI automates 50% of current jobs within 10 years, which of your investments survive? Rebalance toward resilient assets (infrastructure, energy, healthcare, AI-complementary services).**

**D3.** Are you using your investor voice to push for responsible practices?
- **Yes** → Continue. Ensure shareholder resolutions and board conversations include AI safety governance.
- **No** → **ACTION: You have leverage. Use it. Conditional investment ("we invest if you implement X safety practice") is one of the fastest governance mechanisms available.**

---

### SECTION E: Business Leader (Non-AI Company)

**E1.** Has your organization assessed which of its functions could be automated by AI systems significantly more capable than today's?
- **Yes** → Go to E2
- **No** → **ACTION: Conduct this assessment. For each major function (operations, customer service, analysis, creative work, management), ask: "If an AI could do this 10x better than our best person, what happens to this function?" This isn't about today's AI. This is about what's coming.**

**E2.** Do you have a plan for your workforce in a high-automation scenario?
- **Yes** → Go to E3
- **No** → **ACTION: Begin planning. Options include: retraining programs, transition to AI-augmented roles, gradual restructuring, partnership with educational institutions. The organizations that handle this transition well will attract better talent and public goodwill.**

**E3.** Are you engaging with AI governance as a stakeholder?
- **Yes** → Continue. Ensure your voice represents more than just "don't regulate us."
- **No** → **ACTION: You are a stakeholder. The rules governing AI will affect your business profoundly. Engage with industry groups, comment on proposed regulations, and support governance frameworks that create a level playing field.**

---

### SECTION F: Citizen

**F1.** How informed are you about current AI capabilities and trajectory?
- **Well-informed** (you can explain what large language models do, what current limitations are, and what major labs are working on) → Go to F2
- **Somewhat informed** → **ACTION: Spend 10 hours educating yourself. Recommended: read one technical explainer (e.g., "Situational Awareness" by Leopold Aschenbrenner, or summaries of leading AI safety research). Follow 3-5 credible AI researchers/commentators. Avoid both hype and doomerism.**
- **Not informed** → Start with F1-Somewhat. This matters too much to navigate on vibes.

**F2.** Does your current career path have high exposure to AI automation?
- **Yes** (routine cognitive work, data processing, basic creative tasks, simple analysis) → **ACTION: Begin developing AI-complementary skills: complex judgment, physical skills, relationship-building, novel problem-solving, domain expertise that requires real-world experience. Don't panic — transition, don't flee.**
- **No** (physical trades, complex human interaction, novel creative work, care work) → Lower urgency, but stay informed. These categories may be affected later.
- **Unsure** → Ask: "Could an AI do my job if it were 100x smarter than ChatGPT?" If the answer involves physical presence, deep human relationships, or genuinely novel situations — probably lower exposure.

**F3.** Are you supporting organizations working on AI safety and governance?
- **Yes** → Continue. Ensure they're effective, not just reassuring.
- **No** → **ACTION: Find 1-2 organizations focused on AI safety or governance and support them (financially, through volunteering, or by amplifying their work). Examples: organizations doing technical alignment research, AI policy advocacy, or public education.**

**F4.** Are you engaging politically on AI governance?
- **Yes** → Continue. Push for the B3 checklist items through your political channels.
- **No** → **ACTION: Contact your representatives. Ask what their position is on AI regulation. Vote on this issue. This is arguably the most consequential policy area of the coming decade.**

---

## QUICK REFERENCE CARDS

### Card 1: The Four Things That Matter Most
1. **Alignment** — Will superintelligent systems do what humanity needs?
2. **Governance** — Who decides how these systems are built and deployed?
3. **Economic transition** — How do we handle massive labor displacement?
4. **Power concentration** — How do we prevent any single actor from using AI to dominate others?

### Card 2: Timeline-Dependent Priority
| If arrival is... | Top priority |
|---|---|
| < 1 year | Emergency coordination, containment protocols, rapid governance |
| 1-3 years | Alignment verification, compute governance, economic preparation |
| 3-10 years | Institutional reform, international treaties, workforce transition |
| 10+ years | Education, cultural preparation, long-term governance design |
| Unknown | Hedge: act as if 3-10 years, with emergency plans for < 1 year |

### Card 3: Red Flags (Seek Expert Help Immediately)
- An AI system exhibits goals or behaviors its creators didn't intend and can't explain
- A single organization achieves dramatic capability lead with no external oversight
- Geopolitical crisis creates pressure to deploy untested AI systems
- Safety researchers at a major lab resign or are fired en masse
- An AI system successfully deceives its evaluators

---

## STEP 5: Failure Modes

**F1: Analysis Paralysis.**
*What it looks like*: You read this procedure, feel overwhelmed, and do nothing.
*Fix*: Pick ONE action item from your section. Do it this week. Then pick the next one.

**F2: False Precision on Timeline.**
*What it looks like*: You pick "3-10 years" and plan only for that scenario.
*Fix*: Always maintain at least one action from the "< 1 year" column as insurance.

**F3: Scope Mismatch.**
*What it looks like*: A citizen tries to solve alignment; a lab employee focuses only on their career.
*Fix*: Reread Step 0. Focus on actions available to your actual role. Expand role over time if you want more leverage.

**F4: Tribe Capture.**
*What it looks like*: You join an AI safety community or an AI accelerationist community and adopt their views wholesale.
*Fix*: Maintain relationships across the spectrum. If everyone you talk to agrees with you, you're in an echo chamber.

**F5: Confusing Activity with Impact.**
*What it looks like*: You post about AI risk on social media, attend conferences, read papers — but nothing changes as a result of your actions.
*Fix*: For each action, ask: "What concrete outcome did this produce?" If the answer is "awareness" for more than 50% of your actions, redirect toward actions with measurable outputs.

**F6: Optimism Bias.**
*What it looks like*: "Smart people are working on this. It'll be fine."
*Fix*: Smart people were working on climate change, pandemic preparedness, and nuclear security too. Track record of humanity managing unprecedented technological transitions is mixed at best. Hope is fine; complacency is dangerous.

**F7: Doom Paralysis.**
*What it looks like*: "Nothing I do matters. We're doomed."
*Fix*: Even under pessimistic assumptions, marginal actions matter. The difference between "catastrophe" and "disaster" can be billions of lives. Act as if your actions matter, because they might.

**F8: Ignoring the Procedure Because "My Situation Is Unique."**
*What it looks like*: Skipping the structured steps because you feel your expertise/position makes them unnecessary.
*Fix*: The procedure exists precisely because domain experts have blind spots. Follow it once fully, then adapt.

---

## STEP 6: Validation Check

| Check | Status |
|---|---|
| Can someone with no AI expertise follow this? | **Pass** — all technical terms are defined inline, all steps are concrete actions |
| Are all decision points clear? | **Pass** — all binary or explicit multiple choice, no "use judgment" |
| Does every path lead to a concrete output? | **Pass** — every branch terminates in an ACTION or a "continue" |
| Are there dead ends or loops? | **Pass** — all sections are linear with forward-only flow |
| Are the actions actually doable? | **Partial pass** — some actions (e.g., "advocate internally") require social skills not covered here |

**Issues found and fixed:**
- Added escalation paths for when internal advocacy fails (Section A)
- Added specific resource recommendations for citizen education (Section F)
- Added "if multiple roles apply, run procedure for each" instruction in Step 0

---

## WHEN TO OVERRIDE THIS PROCEDURE

Seek expert guidance instead of following this procedure when:

1. **You have direct evidence** of an imminent, specific AI risk (not general concern — a specific system, specific behavior). Contact AI safety organizations directly.
2. **You are being asked to make a legal decision** about AI regulation. This procedure provides direction, not legal advice.
3. **You are experiencing a mental health crisis** related to AI existential risk. Speak with a counselor before making major life decisions based on AI timelines.
4. **The geopolitical situation has fundamentally changed** (e.g., an AI arms race has gone hot, a major AI accident has occurred). This procedure assumes a pre-crisis environment.

---

## WORKED EXAMPLES

### Example 1: Software Engineer at a Mid-Tier AI Company

**Step 0**: AI Lab Employee → Section A

- **A1**: Lab has no specific superintelligence response plan. **Action: Draft a proposal for capability thresholds.**
- **A2**: Safety team exists but is advisory only. **Action: Advocate for binding authority.**
- **A3**: Moderate confidence, vibes-based. **Action: Push for external alignment evaluations.**
- **A4**: Works on capabilities (model training infrastructure). **Decision: Assess comfort level. Given weak safety governance, considers transferring to evaluation team.**
- **A5**: No external oversight. **Action: Support regulatory proposals.**

**Result**: Four concrete action items, prioritized by urgency: (1) external oversight advocacy, (2) safety team authority, (3) alignment evaluations, (4) internal response plan.

### Example 2: U.S. Congressional Staffer

**Step 0**: Policymaker → Section B

- **B1**: No binding frontier AI regulation. **Action: Draft compute-threshold registration requirement.**
- **B2**: No dedicated technical staff. **Action: Propose creating an AI evaluation office.**
- **B3**: 0 of 6 boxes checked. **Priority order: emergency authority first, then pre-deployment evaluation.**
- **B4**: Significant China competition pressure. **Action: Reframe from race-to-build to race-to-govern. Pursue bilateral safety discussions.**
- **B5**: No economic disruption plan. **Action: Commission impact assessment.**

**Result**: Five action items. Highest priority: hire technical staff (without them, nothing else works).

### Example 3: Retired Teacher, Concerned Citizen

**Step 0**: Citizen → Section F

- **F1**: Somewhat informed. **Action: Spend 10 hours on foundational reading.**
- **F2**: Retired, low career exposure. No urgent career action needed.
- **F3**: Not supporting any organizations. **Action: Find and support one AI safety/governance organization.**
- **F4**: Not politically engaged on AI. **Action: Contact representatives; ask about AI governance positions.**

**Result**: Three actions. Start with education (F1), then channel informed concern into support (F3) and political engagement (F4).

---

```
VALIDATION STATUS: This procedure has not been validated by domain
experts. It synthesizes publicly available thinking on AI governance,
safety, and preparation as of early 2025. Review and update quarterly
as the landscape evolves rapidly.

Review triggers (re-run this procedure if any occur):
- A major AI capability jump is announced
- New AI regulation is passed in your jurisdiction
- Your role/relationship to AI changes
- A significant AI incident occurs
- 6 months have passed since your last review
```
