# Decision Procedure — Finding What Nobody Has Ever Considered

**Date**: 2026-01-31
**Input**: finding what nobody has ever considered

---

## Step 1: Discover Decision Dimensions (`/dd`)

What factors determine whether you can find something genuinely unconsidered, and which method will get you there?

### D1: Domain Maturity
How long have smart people been thinking about this space?
- **Why it matters**: Finding something unconsidered in a 2-year-old field is fundamentally different from finding it in a 2,000-year-old one. Mature domains have more explored territory but also more blind spots created by shared assumptions.

### D2: Your Position Relative to the Domain
Are you an insider, outsider, or adjacent?
- **Why it matters**: Insiders know where the edges are but share the field's blind spots. Outsiders lack blind spots but don't know where to look. Adjacent people (experts in a neighboring field) have the best hit rate — they know enough to ask meaningful questions but aren't trapped by the domain's conventions.

### D3: Type of "Unconsidered"
What kind of novelty are you seeking?
- **Why it matters**: "Nobody has considered" could mean: (a) a new fact, (b) a new combination of known facts, (c) a new framing of a known problem, (d) a question nobody thought to ask, or (e) an assumption nobody noticed they were making. Each requires a different search method.

### D4: Verification Capability
Can you test whether your finding is actually novel?
- **Why it matters**: Without verification, you'll "discover" things that are well-known in adjacent fields, already published in obscure papers, or obviously wrong. The procedure must include reality-checking, not just generation.

### D5: Scope of Search
How large is the space you're searching?
- **Why it matters**: "Something nobody considered about photosynthesis" is tractable. "Something nobody considered about anything" is not. Narrower scope paradoxically increases the chance of genuine novelty because you can be more thorough.

### D6: Resource Depth
How much time, expertise, and access to information do you have?
- **Why it matters**: A weekend brainstorm produces different novelty than a 6-month research program. The procedure must match ambition to available resources or you'll generate shallow "insights" that feel novel but aren't.

### D7: Tolerance for Being Wrong
How many dead ends can you absorb before giving up?
- **Why it matters**: The hit rate for genuinely novel ideas is very low. Most "nobody has considered this" moments turn out to be "I hadn't considered this." The procedure must include mechanisms for distinguishing personal novelty from universal novelty, and the searcher must tolerate a high failure rate.

### D8: Stakes of the Finding
What happens if you succeed?
- **Why it matters**: Finding something unconsidered in a pub quiz is different from finding it in cancer research. High-stakes domains require more rigorous verification. Low-stakes domains allow faster, looser exploration.

### D9: Collaboration Access
Can you recruit other minds?
- **Why it matters**: Cognitive diversity is the single strongest predictor of novel insight generation. One person searching alone hits their own blind spots quickly. A group with different backgrounds covers more space — but groupthink can cancel the benefit if not managed.

### D10: Existing Anomaly Awareness
Do you already have a nagging feeling, a weird data point, or a "that's strange" observation?
- **Why it matters**: Most genuine discoveries start from an anomaly that someone noticed and everyone else dismissed. If you already have one, the procedure should exploit it rather than generate from scratch.

### D11: Structural vs. Content Novelty
Are you looking for a new answer or a new question?
- **Why it matters**: New answers to known questions are incremental. New questions that reframe the entire space are revolutionary. The methods differ completely — answer-finding is convergent (narrow down), question-finding is divergent (open up).

### D12: Cross-Domain Fluency
How many unrelated fields can you think in?
- **Why it matters**: Nearly all breakthrough insights come from importing a concept from one domain into another where it hasn't been applied. The more domains you can draw from, the more combination possibilities exist.

---

## Step 2: Enumerate Options per Dimension (`/se`)

### D3: Type of "Unconsidered"
| Type | Description | Example |
|---|---|---|
| **New fact** | Something empirically true that hasn't been observed/measured | A species nobody catalogued |
| **New combination** | Known elements connected in a way nobody has linked | Combining game theory + epidemiology |
| **New framing** | Same problem, completely different way of seeing it | "What if cancer is an engineering problem, not a medical one?" |
| **New question** | A question nobody thought to ask | "Why does no one study what DOESN'T cause cancer?" |
| **Hidden assumption** | Something everyone assumes without noticing | "We assumed users want more features" |
| **Missing category** | A type/class that the existing taxonomy doesn't include | "There's a 6th taste nobody named" |
| **Inverted obvious** | The opposite of conventional wisdom, which turns out to be true | "What if friction HELPS learning?" |

### D2: Position Relative to Domain
| Position | Strengths | Weaknesses |
|---|---|---|
| **Deep insider (10+ years)** | Knows the frontier, knows what's been tried | Shares field's blind spots, anchored to paradigm |
| **Informed outsider** | Fresh eyes, different frameworks | May "discover" what's already known |
| **Adjacent expert** | Cross-pollination potential, enough context to be relevant | May underestimate domain complexity |
| **Complete novice** | No assumptions at all | High noise-to-signal, can't distinguish trivial from profound |

### D6: Resource Depth
| Level | Available Effort | Appropriate Method |
|---|---|---|
| **1 hour** | Quick brainstorm only | Assumption inversion, analogy transfer |
| **1 day** | Structured exploration | Systematic combination, cross-domain mapping |
| **1 week** | Research + generation | Literature gaps, expert interviews, anomaly hunting |
| **1 month+** | Deep investigation | Original research, empirical testing, paradigm analysis |

### D10: Existing Anomaly
| State | Description |
|---|---|
| **Have a specific anomaly** | Something weird you noticed that doesn't fit existing explanations |
| **Have a vague intuition** | "Something feels off about X" but can't articulate what |
| **No anomaly, seeking one** | Starting from scratch, looking for where to search |

---

## Step 3: Surface Hidden Assumptions (`/aex`)

**A1: "Novel ideas come from genius or inspiration."**
False. Novel ideas come from systematic processes: combining known elements in new ways, inverting assumptions, importing cross-domain concepts, and following anomalies. The procedure encodes these processes mechanically.

**A2: "If it were possible to find, someone would have found it already."**
This is the central assumption that prevents people from looking. Fields have systematic blind spots created by: shared training, publication incentives, disciplinary boundaries, and cultural assumptions. Entire categories of ideas are invisible to everyone trained the same way.

**A3: "Novelty requires deep expertise."**
Partially false. Deep expertise is needed to verify novelty, but it often prevents generating it. The best process separates generation (where naivety helps) from verification (where expertise helps).

**A4: "The unconsidered must be complex."**
Often false. Many unconsidered ideas are embarrassingly simple — so simple that experts assume someone must have thought of them and dismisses the thought. The procedure must protect simple ideas from premature dismissal.

**A5: "You'll know a novel idea when you see it."**
False. Novel ideas usually feel wrong, trivial, or obvious at first. The emotional response to genuine novelty is often discomfort ("that can't be right") or dismissal ("that's too simple"), not excitement. The procedure must counteract this.

**A6: "Brainstorming produces novel ideas."**
Rarely. Standard brainstorming produces variations on existing ideas because participants share the same cultural frame. The procedure must use structured methods that force thinking outside shared frames.

**A7: "Reading more will help you find gaps."**
Paradoxically, excessive reading can prevent novelty by filling your mind with existing frames. The procedure must balance learning (knowing what exists) with structured ignorance (forcing you to think before knowing the "answer").

**A8: "The goal is one big breakthrough."**
Misleading. Most genuinely unconsidered things are small: a missing distinction, an unasked question, a silent assumption. The procedure should optimize for finding small true novelties, not waiting for paradigm shifts.

---

## Step 4: The Procedure (`/stg`)

```
FINDING WHAT NOBODY HAS EVER CONSIDERED — DECISION PROCEDURE
=============================================================

STEP 0: Classify Your Search
-----------------------------

Q1: Do you have a specific domain or topic you're
    searching within?
    → YES: Continue to Q2
    → NO:  You must pick one. You cannot search "everything"
           for novelty. Choose the domain where you have the
           most nagging feeling that something is missing, OR
           the domain you know best. Then continue to Q2.

Q2: Do you already have an anomaly, a "that's weird"
    observation, or a nagging intuition about something
    that doesn't fit?
    → YES: Go to SECTION A (Exploit the Anomaly)
    → NO:  Continue to Q3

Q3: How well do you know this domain?
    → Expert (5+ years deep): Go to SECTION B (Assumption Mining)
    → Moderate (some knowledge): Go to SECTION C (Cross-Domain Transfer)
    → Novice (little knowledge): Go to SECTION D (Naive Interrogation)


================================================================
SECTION A: EXPLOIT THE ANOMALY
================================================================
(You noticed something that doesn't fit. This is your
highest-probability path to genuine novelty.)

Step A1: WRITE DOWN THE ANOMALY
         State it as precisely as possible:
         "I noticed that [specific observation] which doesn't
         fit because [what it contradicts]."

         → If you can state it precisely: Go to Step A2
         → If it's just a vague feeling: Write 10 sentences
           starting with "Something is off about..." until
           the feeling crystallizes. Then go to A2.

Step A2: CHECK IF IT'S ACTUALLY ANOMALOUS
         Search for your anomaly in three places:
         a) Google Scholar / academic search: "[key terms]"
         b) Domain-specific forums or communities
         c) Ask someone knowledgeable: "Has anyone explained
            why [anomaly]?"

         → Someone has explained it:
           Read their explanation. Does it ACTUALLY resolve
           the anomaly, or does it explain it away without
           really addressing it?
           → Truly resolved: Your anomaly was personal
             novelty, not universal. Return to Step 0 and
             try Section B, C, or D.
           → Explained away but not resolved: This is a
             LIVE LEAD. The existing explanation has a gap.
             Go to Step A3.
         → Nobody seems to have addressed it:
           This is a STRONG LEAD. Go to Step A3.

Step A3: INTERROGATE THE ANOMALY
         Answer these questions in writing:
         a) What would have to be true for this anomaly to
            be the normal case, not the exception?
         b) If everyone in the field saw this anomaly, why
            might they dismiss it? (too small, doesn't fit
            funding priorities, contradicts prestigious work,
            seems obvious, falls between disciplines)
         c) What would change if this anomaly turned out to
            be important?
         d) What is the simplest experiment, observation, or
            argument that would confirm or disprove it?

         → Go to SECTION E (Verify Novelty)


================================================================
SECTION B: ASSUMPTION MINING
================================================================
(You know the domain well. Your best shot at novelty is
finding what everyone — including you — takes for granted.)

Step B1: THE AXIOM LIST
         Write down 15-20 things that "everyone in this
         field knows" or "goes without saying." These are
         the water the fish don't notice.

         Prompts to generate the list:
         - "In this field, we always..."
         - "It's obvious that..."
         - "The whole point of [field] is..."
         - "We've always done it this way because..."
         - "Nobody would seriously argue that..."
         - "The foundational papers all assume..."

         → Write fast. Don't evaluate. Get 15 minimum.
         → Go to Step B2.

Step B2: THE INVERSION TEST
         For EACH item on your list, write the opposite
         and check if it's actually impossible or just
         unconventional:

         Format: "[Assumption]" → "What if [opposite]?"
         Then ask: Is this opposite...
           (a) Logically impossible (violates math/physics)?
           (b) Empirically disproven (tested and failed)?
           (c) Just assumed (never actually tested)?
           (d) True in another field but not applied here?

         → Any items marked (c) or (d): These are LEADS.
           Circle them.
         → If zero items marked (c) or (d): You may be too
           deep inside the paradigm to see its edges. Go to
           Section C and bring in an outside perspective.
         → Go to Step B3.

Step B3: THE ABSENCE AUDIT
         For your domain, answer:
         a) What questions does nobody ask? (Not "hard
            questions nobody has solved" — questions nobody
            thinks to pose at all.)
         b) What data does nobody collect?
         c) What combinations has nobody tried?
         d) What adjacent fields does nobody in this domain
            read or cite?
         e) Who is affected by this domain's work but never
            consulted or studied?

         → Each answer is a potential search area. Pick the
           one that produces the strongest "huh, that IS
           weird" reaction.
         → Go to SECTION E (Verify Novelty)


================================================================
SECTION C: CROSS-DOMAIN TRANSFER
================================================================
(You know some things about the target domain and have
knowledge from other fields. Your best shot is importing
a concept.)

Step C1: THE DOMAIN MAP
         Write down:
         a) The target domain (where you want to find novelty)
         b) 3-5 other domains you know well (hobbies, other
            work, academic training, life experience)

         → Go to Step C2.

Step C2: THE STRUCTURE EXTRACTION
         For each non-target domain, write down its
         CORE MECHANISM — the fundamental pattern that
         makes it work. Use this format:

         "[Domain] works because [mechanism]."

         Examples:
         - "Ecology works because species occupy niches
           and compete for resources"
         - "Comedy works because setup creates expectation
           and punchline violates it"
         - "Cooking works because heat transforms molecular
           structure and combination creates emergent flavors"

         → Go to Step C3.

Step C3: THE FORCED CONNECTION
         For each core mechanism, ask:
         "What would [target domain] look like if it worked
         like [source domain]?"

         Force yourself to take this seriously for at least
         5 minutes per combination, even if it seems absurd.
         Write at least 3 sentences per pairing.

         → What you should see: Most pairings produce
           garbage. 1-2 will produce something that makes
           you pause. The pause is the signal.
         → If nothing produces a pause after all combinations:
           Add more source domains (ask friends from different
           fields to describe how their domain works).
         → Take your best pairing to SECTION E.

Step C4: THE VOCABULARY SWAP
         Take 5 technical terms from your source domain
         and substitute them into a description of your
         target domain's central problem.

         Does the substitution:
         a) Make no sense at all? → Skip this pairing
         b) Make weird but intriguing sense? → LEAD
         c) Make surprisingly good sense? → STRONG LEAD,
            and someone may have already done this. Check
            before claiming novelty.

         → Take leads to SECTION E.


================================================================
SECTION D: NAIVE INTERROGATION
================================================================
(You know little about the domain. Your advantage is
that you haven't been trained what NOT to ask.)

Step D1: THE CHILD'S QUESTIONS
         Read one introductory explanation of the domain
         (Wikipedia article, textbook chapter 1, a good
         explainer article). Then immediately — BEFORE
         reading more — write down every question that
         comes to mind, especially:

         - "Why?" questions (Why is it done this way?)
         - "Why not?" questions (Why doesn't anyone do X?)
         - "What if?" questions (What if you reversed Y?)
         - "Says who?" questions (Who decided this was the
           right way?)

         → Write at least 15 questions. Speed over quality.
         → Go to Step D2.

Step D2: THE EXPERT CHECK
         Show your questions to someone knowledgeable in
         the domain. For each question, record their
         response:

         | Response Type | What It Means |
         |--------------|---------------|
         | Quick confident answer | Known territory. Drop it. |
         | "That's a good question, actually..." | Possible LEAD |
         | Defensive or dismissive response | Strong possible LEAD (you hit a nerve) |
         | "Nobody's really looked at that" | STRONG LEAD |
         | Confused silence | Either nonsensical or genuinely novel |

         → Collect your leads (good-question, defensive,
           nobody's-looked-at responses).
         → If zero leads: Read more about the domain and
           repeat D1 with deeper material.
         → If 1+ leads: Go to SECTION E.

Step D3: THE STUPID QUESTION PROTOCOL
         Take your strongest lead and ask the dumbest
         possible version of it to 3-5 people in the field:

         "I know this might be a stupid question, but
         why doesn't anyone [your question]?"

         Record every response. Look for:
         - Inconsistent answers (different people give
           different reasons → no one actually knows)
         - Circular answers ("because that's how it's done")
         - Historical answers ("we used to, but stopped") →
           ask WHY they stopped
         - No answer at all

         → Any of these patterns → Go to SECTION E.


================================================================
SECTION E: VERIFY NOVELTY
================================================================
(You have a candidate. Now determine if it's genuinely
unconsidered or just new to you.)

Step E1: THE LITERATURE CHECK
         Search for your idea using 5 different phrasings.
         People in different fields describe the same concept
         with different words. Search:
         a) Your phrasing
         b) The most technical version you can construct
         c) The simplest plain-language version
         d) The concept translated into an adjacent field's
            vocabulary
         e) The PROBLEM your idea solves, rather than the
            idea itself

         → If you find it described somewhere:
           Read it. Is it EXACTLY your idea, or related but
           different?
           → Exactly: Not novel. But the source may lead you
             to the ACTUAL frontier. Read what that source
             cited as "future work" or "open questions."
           → Related but different: Your specific angle may
             still be novel. Articulate precisely what's
             different. Continue to E2.
         → If you cannot find it after thorough searching:
           Continue to E2.

Step E2: THE EXPERT STRESS TEST
         Describe your idea to 3 people:
         a) One domain expert
         b) One smart person from a different field
         c) One person with no specialized knowledge

         For each, record:
         - Did they understand it?
         - Was their first reaction "that's obvious" or
           "that's interesting"?
         - Could they name prior work on exactly this?
         - Did they immediately find a fatal flaw?

         Scoring:
         - Expert says "obvious" → Probably not novel
         - Expert says "interesting, I haven't seen that" →
           Possible novelty
         - Expert says "that can't work because..." AND the
           reason is an assumption, not a proof → STRONG
           signal (you found an assumption worth testing)
         - Expert gets defensive → Very strong signal

         → If at least one expert finds it non-obvious and
           can't cite prior work: Go to Step E3
         → If all three say it's obvious or already known:
           Return to your section (A/B/C/D) and generate
           another candidate.

Step E3: THE FALSIFICATION ATTEMPT
         Spend real effort trying to DISPROVE your own idea.
         a) What evidence would make this idea false?
         b) Does that evidence exist?
         c) What's the strongest argument AGAINST this idea?
         d) Can you refute that argument?

         → If you cannot disprove it after genuine effort,
           AND experts couldn't immediately dismiss it,
           AND literature search found nothing:

           You likely have something genuinely unconsidered.
           → Go to Step E4.

Step E4: DOCUMENT AND SHARE
         Write it up in this format:
         a) THE OBSERVATION: What you noticed (1-2 sentences)
         b) THE GAP: Why this hasn't been considered
            (training bias, disciplinary boundary, hidden
            assumption, missing data)
         c) THE IMPLICATION: What changes if this is right
         d) THE TEST: How someone could verify or falsify it

         → Share with the most rigorous person you know.
         → Their response determines your next step:
           - "Publish this" → Pursue formal channels
           - "Interesting but needs work" → Iterate
           - "Here's why it's wrong: [specific proof]" →
             Accept it gracefully. Return to your section.
           - "I need to think about this" → Best possible
             response. You've found something real.

         → COMPLETE


================================================================
QUICK REFERENCE CARDS
================================================================

CARD 1: WHERE UNCONSIDERED IDEAS HIDE
  1. Between disciplines (nobody in field A reads field B)
  2. Under assumptions (things "everyone knows" that
     nobody has tested)
  3. In absences (data nobody collects, questions nobody
     asks, people nobody consults)
  4. In anomalies (the "weird" result that got footnoted
     and forgotten)
  5. In simplicity (ideas so simple experts assume they
     must be wrong or already known)
  6. In inversions (the opposite of conventional wisdom)
  7. In recombinations (known element A + known element B
     in a configuration nobody tried)

CARD 2: THE NOVELTY FALSE POSITIVE CHECKLIST
  Before claiming novelty, rule these out:
  □ "It's novel to ME" — Have you checked if others know?
  □ "It's novel in THIS field" — Have you checked adjacent
    fields? (Most ideas exist somewhere.)
  □ "Nobody has published it" — Unpublished ≠ unconsidered.
    Practitioners may know it but not write it down.
  □ "I can't find it on Google" — Have you tried 5+
    different phrasings? Different fields use different
    words for the same concept.
  □ "Experts I asked haven't heard of it" — Have you asked
    experts from adjacent fields, not just the target?

CARD 3: EMOTIONAL SIGNATURES OF GENUINE NOVELTY
  - YOUR reaction: Discomfort, "that can't be right,"
    urge to dismiss your own thought
  - EXPERTS' reaction: Pause, defensiveness, "let me
    think about that," changing the subject
  - WRONG signals: Immediate excitement (usually means
    it's a variation, not a novelty), "wow that's so
    creative" (social response, not epistemic)
```

---

## Step 5: Failure Modes (`/fla`)

**F1: Confusing personal novelty with universal novelty.**
- How to recognize: You're excited about an idea but haven't checked if it's known. You avoided the literature check because you didn't want to find out.
- Fix: Section E, Step E1 is mandatory, not optional. Most "novel" ideas are new-to-you, not new-to-humanity. That's fine — but don't confuse the two.

**F2: Generating ideas that are novel but trivial.**
- How to recognize: Your finding is technically unconsidered but nobody cares because it doesn't matter. ("Nobody has studied the relationship between shoelace color and mood on Tuesdays.")
- Fix: Apply the "so what?" test at Step E3. If the implication of your finding being true changes nothing meaningful, it's trivia, not discovery. Return and search for something with stakes.

**F3: Dismissing your own finding because it seems too simple.**
- How to recognize: You thought of something, immediately decided it was "too obvious" to be novel, and moved on without checking. This is the #1 way genuine novelty is lost.
- Fix: NEVER self-dismiss. Every candidate goes through Section E. Your internal sense of "that's obvious" is unreliable — it fires on genuinely novel simple ideas the same way it fires on actually obvious ones.

**F4: Getting stuck in infinite reading before generating.**
- How to recognize: You've been "researching" for weeks but haven't written down a single candidate idea. You keep finding more things to read.
- Fix: Cap research time. Section B, C, and D all generate candidates BEFORE deep research. Read enough to be dangerous, then generate, then verify. The order matters.

**F5: Only searching within the dominant paradigm.**
- How to recognize: All your candidate ideas are extensions or improvements of existing approaches, not challenges to underlying assumptions.
- Fix: Return to Section B, Step B1 and force yourself to list more axioms. The unconsidered lives in what you CAN'T see because you share the field's framing. If you can't escape it alone, recruit an outsider (Section D's approach applied through someone else).

**F6: Giving up after first candidate fails verification.**
- How to recognize: Your first idea turned out to be already known, and you concluded "everything has been thought of."
- Fix: Expected hit rate is 5-15%. You need 7-20 candidates to find one genuine novelty. Build a pipeline, not a lottery ticket. Return to your section and generate more candidates.

**F7: Seeking novelty for its own sake rather than because it matters.**
- How to recognize: You're optimizing for "nobody has thought of this" rather than "this is true and important." You're attracted to contrarian positions because they're contrarian.
- Fix: Novelty is a means, not an end. The procedure aims to find unconsidered things that are TRUE and USEFUL. Step E3 (falsification) exists precisely to filter out novelty-for-novelty's-sake.

**F8: Not recognizing the finding when it appears.**
- How to recognize: You had a thought that felt uncomfortable or "wrong," dismissed it, and only later realized it was the insight. Genuine novelty disguises itself as error.
- Fix: Keep a written log of EVERY idea that produces discomfort, defensiveness in others, or the urge to dismiss. Review the log weekly. The pattern will emerge.

---

## Step 6: Validation Check (`/pv`)

- **Step 0**: Three questions, clear routing. No ambiguity. ✓
- **Section A**: Requires a specific anomaly before entering. Writing prompts concretize vague feelings. Literature check is specific (three named sources). ✓
- **Section B**: Axiom list has specific prompts and a minimum count (15). Inversion test has four defined categories (a-d) — mechanically checkable. ✓
- **Section C**: Structure extraction uses fill-in-the-blank format. Forced connection specifies minimum time (5 min per pairing) and minimum output (3 sentences). ✓
- **Section D**: Child's questions has four specific question types and minimum count (15). Expert check has a clear scoring rubric. ✓
- **Section E**: Five-phrasing search is specific. Expert stress test has defined scoring. Falsification has four concrete sub-questions. ✓
- **Ambiguity found and addressed**: Step E2 "smart person from a different field" was vague. Clarified: anyone with expertise in a domain unrelated to your target who can evaluate logical structure.
- **Dead-end check**: All paths terminate at E4 (document and share) or loop back with clear instructions. No dead ends. ✓
- **Loop risk**: Sections can loop (generate → verify → fail → generate). Added expected hit rate (F6) to prevent infinite loops — if 20+ candidates all fail verification, the scope may need narrowing.

---

## COMMON MISTAKES

1. **Treating "nobody has considered" as meaning "nobody smart has considered."** Smart people miss things systematically, not randomly. The gaps are in their shared blind spots, not in their intelligence.
2. **Searching where everyone else is searching, just harder.** If the frontier is crowded, novelty is elsewhere. Look where nobody is looking, not where everyone is looking more carefully.
3. **Confusing "contrarian" with "unconsidered."** Contrarian positions are considered and rejected. Genuinely unconsidered things aren't on the map at all — they're not the opposite of conventional wisdom, they're orthogonal to it.
4. **Assuming novelty requires new information.** Most unconsidered ideas use information that already exists. The novelty is in the connection, framing, or question — not the data.
5. **Working alone when cognitive diversity would help.** A room of people with the same training will find the same things. One outsider asking "dumb questions" (Section D) is worth ten insiders brainstorming.

## WHEN TO OVERRIDE THIS PROCEDURE

- **You are in a field where novelty is dangerous** (medicine, engineering, public policy). Unconsidered ideas must go through rigorous institutional review before acting on them. This procedure finds candidates — it does not validate them for high-stakes application.
- **You are experiencing mania or grandiosity.** If EVERY idea feels like a breakthrough and you can't sit still, the signal is neurological, not epistemic. Talk to someone you trust before publishing or acting.
- **You have found something genuinely novel and important.** At that point, this procedure ends and domain-specific procedures (publishing, patenting, building) begin.
- **The domain is pre-paradigmatic** (so new that there isn't established conventional wisdom yet). In this case, almost everything is unconsidered. The challenge is not finding novelty but finding truth. Switch to verification-heavy methods.

## WORKED EXAMPLES

### Example 1: Software Engineer Notices Something Odd
- Q1: Domain? Yes — software testing. Q2: Anomaly? Yes — "I've noticed that the bugs users report almost never overlap with what our automated tests catch."
- → **Section A**, Step A1: "Users find bugs in workflows that cross module boundaries, but our tests are written within modules. The coverage metrics say 90% but the coverage is shaped wrong."
- → Step A2: Searches "cross-module testing gaps," "integration test coverage metrics." Finds articles about integration testing but nothing about the systematic shape mismatch between test coverage and user-found bugs.
- → Step A3: If this is true, coverage percentage is meaningless without coverage topology. Nobody in the testing community talks about topology of coverage.
- → **Section E**: Literature check finds nothing on "coverage topology" specifically. Asks two senior engineers — one says "huh, that's interesting," the other says "that's why we do integration tests" (circular answer). Falsification attempt: Could check whether user-bug locations predict untested module boundaries.
- → E4: Writes it up. Documents the gap between coverage volume and coverage shape.

### Example 2: Philosophy Student Explores Ethics
- Q1: Domain? Ethics. Q2: Anomaly? No. Q3: Knowledge level? Moderate.
- → **Section C**, Step C1: Target domain = ethics. Other domains = ecology, music production, cooking.
- → Step C2: "Ecology works because competing species create stable equilibria." "Music production works because layering imperfect signals creates emergent texture." "Cooking works because sequence determines outcome — same ingredients in different order produce different dishes."
- → Step C3: "What if ethics worked like cooking — where the ORDER in which you apply moral principles changes the conclusion, not just which principles you apply?" Pause. Every ethical framework treats principles as simultaneous, but real moral reasoning is sequential — you encounter facts in an order, and that order shapes your judgment.
- → **Section E**: Searches "moral reasoning order effects," "sequential ethical judgment." Finds work on moral psychology (order effects in trolley problems) but nothing in normative ethics that takes ordering seriously as a structural feature rather than a cognitive bias to eliminate.
- → E2: Ethics professor says "that's... actually a gap. We always treat ordering as a bias, never as information."

### Example 3: Parent with No Special Expertise
- Q1: Domain? Education. Q2: Anomaly? No. Q3: Knowledge? Novice.
- → **Section D**, Step D1: Reads Wikipedia on "pedagogy." Questions generated: "Why do all schools use the same age-grouping system?" "Why is homework given to kids but not to teachers?" "Why does nobody measure what kids forget, only what they remember?" "Why do schools test subjects separately when real problems cross subjects?"
- → Step D2: Shows questions to a teacher friend. Gets quick answers for most. But on "why does nobody measure what kids forget": long pause, then "well, we can't really... hmm." Defensive response on homework question: "kids NEED practice." Nobody-looked-at response on the forgetting question.
- → Step D3: Asks 4 educators: "Why don't we systematically study what knowledge students lose after a course ends?" Gets: inconsistent answers, one circular answer, one "we don't have the funding" (resource excuse, not conceptual objection).
- → **Section E**: Literature check finds "forgetting curve" research (Ebbinghaus) but almost nothing on systematically measuring post-course knowledge decay at institutional scale to redesign curricula. The measurement exists in cognitive science; the application to curriculum design doesn't.

---

*This procedure has not been validated by domain experts. It is a structured method for generating and verifying novel ideas. Genuine novelty requires verification beyond what this procedure provides — especially in high-stakes domains.*
