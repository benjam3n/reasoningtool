---
date: 2026-01-29 20:00
topic: How to improve UAUA and related skills — structural improvements
depth: 8x
claims: 24
crux_points: 10
status: MIXED
---

# UAUA 8x: How to Improve UAUA and Related Skills

Input: How would you improve the UAUA skill and related skills to incorporate/integrate/modify or change to be much improved?

---

## U0: EXEMPLAR GROUNDING

### Exemplars of Excellent Reasoning Methodologies

1. **Toyota Production System / Lean** — Why good: reduced manufacturing to a small set of principles (eliminate waste, continuous flow, pull systems) that are SELF-APPLYING. The methodology improves itself. Workers on the line can identify improvements.

2. **Scientific Method** — Why good: the simplest possible reasoning loop (hypothesis → prediction → test → update). Survives because it's SELF-CORRECTING — every failure teaches. No ceremony; pure function.

3. **Bayesian Reasoning** — Why good: mathematically optimal belief updating. Single rule (update proportional to likelihood ratio) applied recursively. No special cases. The framework IS the minimum.

4. **Design Thinking (IDEO/Stanford d.school)** — Why good for comparison: it handles the generative/evaluative split explicitly (empathize → define → ideate → prototype → test). It KNOWS it has phases.

5. **Chess Engine Evaluation** — Why good: combines search (explore many positions = universalization) with evaluation (score each position = ARAW). Similar structure to UAUA but with a critical difference: the EVALUATION FUNCTION is learned from millions of games, not derived from first principles.

### Shared Patterns

- The best methodologies are SIMPLE (few rules, recursively applied)
- They are SELF-CORRECTING (failures feed back into improvement)
- They handle their OWN LIMITATIONS explicitly (know when they don't apply)
- They separate SEARCH from EVALUATION clearly
- The evaluation function is grounded in REALITY (data, observation, test results) not just in reasoning

### Felt Impression

The best reasoning tools feel like LENSES — they clarify what's already there rather than imposing structure on reality. UAUA currently feels more like a TEMPLATE — it imposes structure that sometimes obscures rather than clarifies.

---

## U1: SPACE MAPPING

### Applying All 18 Techniques

**1. STATE SPACE — What states could "improved UAUA" be in?**
- Minor tweaks (wording, examples, formatting)
- Structural additions (new steps, new techniques, new phases)
- Fundamental redesign (different core loop, different philosophy)
- Integration with other skills (UAUA becomes a composition of other skills)
- Simplification (UAUA has too much; remove ceremony)
- Specialization (UAUA variants per domain instead of one universal)
- Meta-improvement (make UAUA self-improving so it fixes itself over time)

**2. INSTANCE-TO-CATEGORY — What kind of thing is "skill improvement"?**
- Skill improvement is an instance of: methodology evolution
- Siblings: scientific paradigm shifts, software refactoring, curriculum redesign
- The key question from paradigm shifts: is this incremental improvement or does UAUA need a fundamentally different model?

**3. PARAMETER VARIATION — What parameters of UAUA could change?**
- Number of steps (currently 6: U0, U1, A1, U2, A2, Synthesis)
- Number of techniques (currently 18)
- Depth of ARAW trees
- Format of output (currently text trees)
- Relationship between steps (currently linear)
- What ARAW tests (currently propositions — could test perceptions, compositions, experiences)
- The evaluation standard (currently AR/AW binary — could be multi-valued)
- When to stop exploring (currently depth targets — could be insight-based)

**4. ROLE REVERSAL — What if UAUA improved the user instead of the user improving UAUA?**
- What if the problem isn't the skill but how it's invoked?
- "UAUA 8x" is a blunt instrument — depth is uniform across all branches
- An improved UAUA might TEACH the user to direct depth where it matters
- [NOVEL] Maybe the improvement is making UAUA interactive rather than monolithic

**5. EXISTENCE CHECK — Does UAUA actually need improvement?**
- Evidence it works: the 32x universal design principles session produced genuinely useful output (40 validated principles, mathematical formalization, domain effort orderings)
- Evidence it fails: the website design session produced analytical improvements that missed compositional quality
- Evidence it's mixed: works for analytical/propositional domains, struggles for perceptual/generative domains
- Resolution: YES it needs improvement, but not everywhere. The analytical core is strong. The gaps are at the boundaries.

**6. CAUSAL REVERSAL — What if UAUA's weaknesses cause its strengths?**
- The analytical rigidity that fails at design IS the same rigidity that succeeds at logic
- Loosening for design might weaken for logic
- [NOVEL] The improvement might not be changing UAUA but adding COMPLEMENTARY skills that handle what UAUA can't

**7. TEMPORAL VARIATION — How should UAUA change over time?**
- Short-term: fix known gaps (already done with perceptual techniques, U0, gestalt principle)
- Medium-term: restructure the skill based on accumulated session data
- Long-term: make the skill self-improving (learn from each session what worked)
- [NOVEL] The biggest improvement might be a LEARNING LOOP — UAUA gets better each time it's used

**8. BOUNDARY DISSOLUTION — What if we expanded "UAUA skill" to include the entire reasoning toolkit?**
- UAUA doesn't exist in isolation — it's part of GOSM, ARAW, 200+ skills
- Improvements might be about INTEGRATION (how skills call each other) not individual skill quality
- The toolkit's weakness might be that skills are silos — each operates independently

**9. MODALITY SHIFT — How certain are we about what "improved" means?**
- "Improved" for whom? For the user? For the LLM executing it? For the output quality?
- Different optimization targets might conflict
- For user: simpler, faster, more actionable output
- For LLM: clearer instructions, less ambiguity about when to apply what
- For output: more insight, fewer missed dimensions, more validated conclusions

**10. PERSPECTIVE ROTATION — Who would evaluate UAUA differently?**
- A cognitive scientist: "Your exploration patterns don't match how humans actually think"
- A decision theorist: "Your evaluation isn't calibrated — AR/AW doesn't give probabilities"
- A designer: "Your output is text when the problem is visual"
- A software engineer: "Your skill files are too long and have too many special cases"
- A user: "I just want the answer, not 6400 lines of trees"
- [CRUX 1: What is the primary audience for improvement — power users who want depth, or typical users who want answers?]

**11. SCALE VARIATION — At what scale should improvements happen?**
- Token level: better wording in the SKILL.md
- Step level: better individual steps (U0, U1, A1, etc.)
- Flow level: better relationships between steps
- Skill level: UAUA as a whole works better
- Toolkit level: UAUA integrates better with other skills
- Meta level: the toolkit improves itself

**12. NEGATION REFRAME — What if UAUA's problems are actually opportunities?**
- "UAUA is too analytical" → opportunity to create the first hybrid analytical-perceptual reasoning tool
- "UAUA is too long" → opportunity to create adaptive depth (expand where useful, compress where not)
- "UAUA misses domains" → opportunity to make domain-specific variants the standard

**13. EXEMPLAR COMPARISON — What do the best reasoning frameworks do that UAUA doesn't?**
- Scientific method: TESTS against reality (UAUA only tests against internal logic)
- Bayesian: QUANTIFIES uncertainty (UAUA uses categorical verdicts)
- Chess engines: LEARNS evaluation from data (UAUA's evaluation is hand-coded)
- Design thinking: PROTOTYPES before evaluating (UAUA evaluates without generating)
- [NOVEL] UAUA lacks: empirical testing, uncertainty quantification, learning, and generation

**14. SENSORY EVALUATION — What does using UAUA feel like?**
- At 1-2x: focused, useful, quick insight
- At 4x: thorough, occasionally repetitive
- At 8x+: exhaustive but sometimes exhausting; format dominates content
- The "feel" degrades with depth — the procedure starts to fight the exploration
- [NOVEL] Depth should feel like ZOOMING IN, not like FILLING FORMS

**15. COMPOSITIONAL ANALYSIS — How do UAUA's parts relate to each other?**
- U0 → U1: good connection (exemplars ground universalization)
- U1 → A1: good (candidates are tested)
- A1 → U2: adequate (survivors get edge-cased)
- U2 → A2: adequate (edge cases get tested)
- A2 → Synthesis: weak (synthesis is often a summary, not a genuine integration)
- Missing: A1 findings don't feed back to improve U1 candidates
- Missing: U2 insights don't generate new candidates (only edge cases of existing ones)
- [NOVEL] The linear flow prevents FEEDBACK LOOPS. Each step only flows forward, never backward.

**16. EMOTIONAL RESPONSE — What feeling should improved UAUA evoke?**
- Current: "I did the procedure correctly" (compliance)
- Desired: "I understand this problem deeply" (insight)
- The gap: procedure-following ≠ understanding
- [CRUX 2: Should UAUA optimize for procedural compliance or for insight generation?]

**17. MATERIAL/SURFACE — What metaphor describes UAUA?**
- Current: a CHECKLIST (sequential steps, checkboxes, minimum counts)
- Better: a MICROSCOPE (adjustable zoom, focus where interesting, reveal hidden structure)
- Or: a MAP (show the territory, let the user choose where to explore)
- [NOVEL] The metaphor shift from checklist to microscope/map would change everything

**18. PATTERN MATCHING — What conventions does UAUA follow or violate?**
- Follows: structured analysis conventions (steps, outputs, checklists)
- Violates: the convention that reasoning tools should be MINIMAL (UAUA is complex)
- Violates: the convention that exploration should be CURIOSITY-DRIVEN (UAUA is procedure-driven)
- [CRUX 3: Is UAUA's complexity necessary or accumulated ceremony?]

### CANDIDATES for A1

1. **Simplify radically** — reduce UAUA to its essential core, remove ceremony
2. **Add feedback loops** — let later steps modify earlier conclusions
3. **Make depth adaptive** — expand where interesting, compress where not
4. **Add empirical grounding** — test claims against reality, not just logic
5. **Create domain variants** — specialized UAUAs for design, strategy, engineering, writing
6. **Add generation capability** — UAUA can generate solutions, not just evaluate them
7. **Make it interactive** — user guides exploration in real-time instead of monolithic output
8. **Add learning loop** — each session improves the skill for next time
9. **Integrate with other skills** — UAUA orchestrates other skills rather than doing everything itself
10. **Fix the synthesis step** — make synthesis a genuine integration, not a summary
11. **Replace depth targets with insight targets** — stop when you've found something, not when you've hit a line count
12. **Add uncertainty quantification** — replace categorical verdicts with calibrated probabilities
13. **Restructure as feedback loop** — U→A→U→A becomes a cycle that runs until convergence, not a fixed sequence
14. **Add the "generate then evaluate" pattern** — produce candidate solutions before analyzing them
15. **Make output format flexible** — trees for some domains, prose for others, diagrams for spatial
16. **Add cross-session learning** — UAUA references past sessions when relevant
17. **Reduce SKILL.md size** — current file is 570 lines; many users won't read it all
18. **Add "zoom" metaphor** — user can zoom into any branch for more depth

[T:result] U1 produced 18 universalizations across all techniques, 18 candidates

---

## A1: CANDIDATE TESTING

### CANDIDATE 1: Simplify radically — reduce to essential core

**AR: UAUA has too much ceremony**
├── The SKILL.md is 570 lines. The ARAW SKILL.md is 757 lines. Combined: 1327 lines of instructions.
│   ├── AR: This is more instruction than most practitioners will absorb
│   │   ├── The LLM reads it all, but the USER doesn't — misalignment between what the tool does and what the user expects
│   │   │   ├── AR: Users invoke "/uaua 8x" and get 3000+ lines of output they didn't fully expect
│   │   │   │   └── AW: Power users WANT comprehensive output. This is a feature, not a bug.
│   │   │   │       └── AR: But even power users might prefer 500 lines of insight over 3000 lines of procedure
│   │   │   │           └── [CRUX 4: Is insight density (insight/line) or total insight (total lines) the better metric?]
│   │   │   └── AW: The LLM IS the practitioner — the user just reads the output
│   │   │       └── AR: True — but the output format is shaped by the instruction length
│   │   │           └── Long instructions → long outputs → diluted insight
│   │   │               └── [LEVEL 7] The verbosity of SKILL.md propagates to verbosity of output
│   │   └── AW: Complexity is necessary because the reasoning IS complex
│   │       └── AR: The reasoning is complex but the INSTRUCTIONS don't need to be
│   │           └── Scientific method: hypothesis→test→update. Three words. Infinitely deep.
│   │               └── AW: But scientific method is ambiguous — UAUA provides specificity
│   │                   └── AR: There's a middle ground between "three words" and "570 lines"
│   │                       └── [NOVEL] The ideal SKILL.md length is: enough to prevent mode errors, not so much it constrains exploration
│   ├── What's the essential core of UAUA?
│   │   ├── MAP possibilities (divergent) → TEST possibilities (convergent) → FIND edge cases → VALIDATE
│   │   ├── That's it. Everything else is implementation detail.
│   │   │   └── AR: A 50-line UAUA SKILL.md with just the core might produce BETTER output
│   │   │       └── Because the LLM would fill in domain-appropriate details instead of following instructions
│   │   │           └── [CRUX 5: Would a minimal SKILL.md produce better output by giving the LLM more freedom?]
│   │   └── AW: Without specifics, the LLM defaults to generic analysis
│   │       └── AR: The depth requirements and tree structure ARE valuable specifics to keep
│   │           └── The question is what else is essential vs ceremony

**AW: Simplification would lose valuable structure**
├── The depth requirements prevent shallow analysis
│   └── AR: Keep depth requirements. Remove format prescriptions.
├── The output format makes analysis parseable
│   └── AR: Keep tree structure. Remove template boilerplate.
├── The techniques list (18) provides systematic coverage
│   └── AR: Keep techniques as a REFERENCE, not a CHECKLIST.
│       └── [NOVEL] Techniques should be prompted ("have you considered?") not mandated ("apply all 18")

**VERDICT: VALIDATED WITH REFINEMENT**
Simplify the SKILL.md by 50-70%. Keep: core loop, depth requirements, tree structure, techniques as reference. Remove: excessive output format templates, verification checklists (the LLM doesn't need to be told to verify — it needs to be told to EXPLORE).

---

### CANDIDATE 2: Add feedback loops — let later steps modify earlier conclusions

**AR: The linear flow loses information**
├── Currently: U1→A1→U2→A2→Synthesis. No backflow.
│   ├── AR: When A1 reveals a candidate is wrong for surprising reasons, this should UPDATE U1's space mapping
│   │   ├── But currently the new information stays in A1 and doesn't generate new U1 candidates
│   │   │   ├── AR: This means U2 only finds edge cases of EXISTING candidates
│   │   │   │   └── It should ALSO find new candidates that A1's findings revealed
│   │   │   │       └── [NOVEL] U2 should be "Universalize the FINDINGS" not just "edge-case the survivors"
│   │   │   └── AW: Adding backflow makes the procedure non-terminating
│   │   │       └── AR: Converge when no new candidates emerge (like iterative algorithms)
│   │   │           └── AW: In practice, convergence is hard to detect in reasoning
│   │   │               └── AR: Use a simple heuristic: if U2 generates candidates that aren't subsets of U1, loop back
│   │   │                   └── [LEVEL 7] Practical convergence: loop until no genuinely new candidates appear
│   │   └── AW: Linear flow is simpler and more predictable
│   │       └── AR: Simpler ≠ better if it misses important paths
│   │           └── The website design UAUA missed things precisely because A1 findings didn't feed back
│   ├── [NOVEL] The pattern should be: U→A→(if new candidates: U→A→)* Synthesis
│   └── This is how chess engines work: iterative deepening, not single pass

**VERDICT: VALIDATED**
Add feedback loops. If A1 or A2 reveals genuinely new directions, loop back to U-step. Converge when stable.

---

### CANDIDATE 3: Make depth adaptive — expand where interesting, compress where not

**AR: Uniform depth wastes effort on boring branches**
├── Currently: "8x" means every candidate gets 6-8 levels of ARAW
│   ├── But some candidates are obviously right or obviously wrong after 2 levels
│   │   ├── AR: Continuing to 6 levels for obvious cases produces ENUMERATION not DERIVATION
│   │   │   ├── This violates ARAW Principle 1 (exploration over format compliance)
│   │   │   │   └── AR: The depth requirements CAUSE the violation they're supposed to prevent
│   │   │   │       └── [NOVEL] Fixed depth targets are self-defeating — they incentivize padding
│   │   │   │           └── [CRUX 6: Can depth targets be replaced with insight targets?]
│   │   │   └── AW: Without depth targets, the LLM will be lazy and produce shallow analysis
│   │   │       └── AR: Replace "18 claims minimum" with "continue until no new insights emerge"
│   │   │           └── AW: "No new insights" is subjective and hard to verify
│   │   │               └── AR: Use a concrete test: "Did the last 3 branches produce any [NOVEL] findings?"
│   │   │                   └── If yes: continue. If no: this direction is exhausted.
│   │   │                       └── [LEVEL 7] Adaptive stopping: stop a branch when insight rate drops below threshold
│   │   └── AW: Some depth is needed for surprising findings — they appear at level 5-6
│   │       └── AR: The MINIMUM depth should stay (e.g., 3 levels). The MAXIMUM should be adaptive.
│   │           └── [NOVEL] Minimum depth = floor (prevents laziness). No maximum = ceiling removed (prevents padding).

**VERDICT: VALIDATED**
Replace fixed maximums with adaptive depth. Keep minimum floors (prevent shallow analysis). Add insight-based stopping criterion.

---

### CANDIDATE 4: Add empirical grounding — test against reality

**AR: UAUA only tests against internal logic**
├── All ARAW branches are thought experiments — "what if true?" "what if false?"
│   ├── AR: No branch ever checks reality. "The accent color is #3d5a80" → AR/AW → conclude #2563eb is better
│   │   ├── But never: "let's LOOK at both and see which is actually better"
│   │   │   ├── AR: For an LLM, "looking" is limited — but it CAN compare to training data
│   │   │   │   └── The LLM knows what Stripe looks like. It should be able to say "this looks/doesn't look like Stripe."
│   │   │   │       └── [CRUX 7: Can LLM implicit knowledge serve as empirical grounding?]
│   │   │   └── AW: The user can empirically test by deploying
│   │   │       └── AR: But UAUA should IDENTIFY what to test and SUGGEST how
│   │   │           └── Currently it produces analytical verdicts but no testable predictions
│   │   │               └── [NOVEL] Every UAUA should output: "TESTABLE PREDICTIONS — things you can verify"
│   │   └── AW: Internal logic IS a valid form of testing
│   │       └── AR: Necessary but not sufficient. The website design was internally logical but perceptually wrong.
│   │           └── [LEVEL 6] Internal logic is necessary for coherence; empirical grounding is necessary for validity

**VERDICT: VALIDATED**
Add a "Testable Predictions" output to synthesis. Encourage using LLM implicit knowledge as soft empirical data.

---

### CANDIDATE 5: Create domain variants

**AR: One-size-fits-all underperforms everywhere slightly**
├── We already added domain technique profiles (which techniques to use per domain)
│   ├── AR: But this is just technique selection — the FLOW is still the same
│   │   ├── Design needs: generate → evaluate → iterate (not just evaluate)
│   │   │   └── AR: The design variant should have a GENERATE step between U1 and A1
│   │   │       └── [NOVEL] UAUA-Design: U0→U1→GENERATE→A1→U2→A2→Synthesis
│   │   ├── Strategy needs: model → simulate → decide (scenario planning)
│   │   │   └── AR: The strategy variant should have scenario modeling
│   │   ├── Engineering needs: specify → design → verify (V-model)
│   │   │   └── AR: The engineering variant maps naturally to UAUA already
│   │   └── Writing needs: research → outline → draft → revise
│   │       └── AR: The writing variant should PRODUCE text, not just analyze it
│   │           └── [CRUX 8: Should domain variants change the FLOW or just the TECHNIQUES?]
│   └── AW: Multiple variants are harder to maintain than one skill
│       └── AR: Variants can INHERIT from a base and override specific steps
│           └── Like object inheritance — base UAUA, then Design extends UAUA with GENERATE step

**VERDICT: VALIDATED WITH CONDITIONS**
Create domain variants ONLY where the flow needs to change (design, writing). For domains where only technique selection changes (strategy, engineering), the current profile system is sufficient.

---

### CANDIDATE 6: Add generation capability

**AR: UAUA only evaluates, never generates**
├── The user asked "make the website better" → UAUA analyzed what was wrong
│   ├── But never produced: "here's what good would look like"
│   │   ├── AR: Generation + evaluation is stronger than evaluation alone
│   │   │   ├── You can generate 5 design proposals, then ARAW each
│   │   │   │   └── This is how design studios work: generate many concepts, critique, select
│   │   │   │       └── AR: UAUA already does the critique. It just needs the generation step.
│   │   │   │           └── [NOVEL] Add a GENERATE step: between mapping the space and testing it, PRODUCE candidate solutions
│   │   │   └── AW: Generation quality is hard to control
│   │   │       └── AR: That's why you generate THEN evaluate. Bad generations get caught by ARAW.
│   │   │           └── [LEVEL 6] Generate-then-evaluate is more robust than evaluate-only because it has more starting material
│   │   └── AW: UAUA is an analysis tool, not a creation tool
│   │       └── AR: The line between analysis and creation is artificial
│   │           └── U1 ALREADY generates candidates. It just generates them as LABELS not as ARTIFACTS.
│   │               └── "Compress the hero section" (label) vs actually producing compressed hero HTML (artifact)
│   │                   └── [CRUX 9: Should UAUA produce artifacts or just analysis?]

**VERDICT: VALIDATED**
Add generation capability. U1 generates candidate LABELS. New G1 step generates candidate ARTIFACTS. A1 evaluates artifacts, not just labels.

---

### CANDIDATE 7: Make it interactive

**AR: Monolithic output prevents user steering**
├── Currently: user says "/uaua 8x topic" → LLM produces 1600+ lines → done
│   ├── No opportunity for user to say "go deeper on branch 3" or "skip that, it's obvious"
│   │   ├── AR: Interactive exploration would be much more efficient
│   │   │   ├── User could direct depth where their uncertainty is highest
│   │   │   │   └── AW: This requires multiple turns, which breaks the "save to file" workflow
│   │   │   │       └── AR: The file could accumulate across turns
│   │   │   │           └── AW: In practice, multi-turn UAUA loses context and coherence
│   │   │   │               └── AR: That's a current limitation, not a fundamental one
│   │   │   │                   └── [LEVEL 6] Interactive UAUA is better in theory but harder in current context-window reality
│   │   │   └── AW: The LLM can't know what the user finds obvious
│   │   │       └── AR: But the user CAN know, and interactive lets them say so
│   │   └── AW: Power users want comprehensive single-pass output
│   │       └── AR: Support both: monolithic mode (current) and interactive mode

**VERDICT: VALIDATED WITH CONDITIONS**
Add interactive mode as an option. Keep monolithic as default. Interactive mode: U1 → present candidates → user picks which to explore → A1 on selected → repeat.

---

### CANDIDATES 8-18: Abbreviated Testing

**CANDIDATE 8: Add learning loop** — VALIDATED
Each session should extract what worked and what didn't. Save learnings. Use in future sessions. Implementation: add a brief "session reflection" step that appends to a learning log.

**CANDIDATE 9: Integrate with other skills** — VALIDATED
UAUA should be able to invoke specialized skills (e.g., `/comparison` for candidate selection, `/root_cause_5_whys` when finding causes). Currently it tries to do everything internally.

**CANDIDATE 10: Fix synthesis step** — VALIDATED
Current synthesis is often a summary of what was found. Should be: identify the STRUCTURE of what was found (what principles connect the findings, what tensions remain, what the findings imply together that no individual finding implies alone).

**CANDIDATE 11: Replace depth with insight targets** — VALIDATED WITH CONDITIONS
Pure insight targets are too subjective. Hybrid: minimum depth floors + adaptive expansion where insight rate is high + stop criterion when insight rate drops.

**CANDIDATE 12: Add uncertainty quantification** — REJECTED
Calibrated probabilities add precision that UAUA's output doesn't support. The categorical system (VALIDATED/REJECTED/UNCERTAIN) is appropriate for the level of rigor. Adding false precision would be worse than honest categories.

**CANDIDATE 13: Restructure as feedback loop** — VALIDATED (subsumes Candidate 2)
The iterative convergence model is the right structural improvement. U→A→(loop if new)→Synthesis.

**CANDIDATE 14: Add generate-then-evaluate** — VALIDATED (subsumes Candidate 6)
The generative step should produce ARTIFACTS not just labels when the domain supports it.

**CANDIDATE 15: Make output format flexible** — VALIDATED
Trees for analytical domains, prose synthesis for communication, lists for action items. The output should match the domain, not be uniform.

**CANDIDATE 16: Add cross-session learning** — VALIDATED WITH CONDITIONS
Only for sessions on related topics. Implement as: check session log for related prior sessions, reference relevant findings.

**CANDIDATE 17: Reduce SKILL.md size** — VALIDATED
Target: 200-250 lines max for the core SKILL.md. Move examples, depth tables, and output templates to separate reference files.

**CANDIDATE 18: Add zoom metaphor** — VALIDATED (subsumes Candidate 3)
User should be able to say "zoom into candidate 3" and get deeper analysis on that specific branch. In monolithic mode: the LLM auto-zooms into the most interesting branches.

### A1 SUMMARY

| # | Candidate | Verdict |
|---|-----------|---------|
| 1 | Simplify radically | VALIDATED (reduce SKILL.md 50-70%) |
| 2 | Add feedback loops | VALIDATED |
| 3 | Make depth adaptive | VALIDATED |
| 4 | Add empirical grounding | VALIDATED |
| 5 | Create domain variants | WITH CONDITIONS (only where flow changes) |
| 6 | Add generation capability | VALIDATED |
| 7 | Make it interactive | WITH CONDITIONS (option, not default) |
| 8 | Add learning loop | VALIDATED |
| 9 | Integrate with other skills | VALIDATED |
| 10 | Fix synthesis step | VALIDATED |
| 11 | Insight-based stopping | WITH CONDITIONS (hybrid with floors) |
| 12 | Uncertainty quantification | REJECTED (false precision) |
| 13 | Feedback loop structure | VALIDATED |
| 14 | Generate-then-evaluate | VALIDATED |
| 15 | Flexible output format | VALIDATED |
| 16 | Cross-session learning | WITH CONDITIONS (related topics only) |
| 17 | Reduce SKILL.md size | VALIDATED |
| 18 | Zoom metaphor | VALIDATED |

[T:result] A1: 12 validated, 1 rejected, 5 validated with conditions

---

## U2: EDGE CASE DISCOVERY

### For VALIDATED Candidates

**Edge case 1: Simplification loses necessary guardrails**
- When: depth requirements removed, LLM produces shallow output
- Condition: minimum depth floors must survive simplification
- [NOVEL] Simplify INSTRUCTIONS, not REQUIREMENTS. The SKILL.md should be short but the output standards should be unchanged.

**Edge case 2: Feedback loops cause infinite recursion**
- When: every A-step generates new candidates, and every U-step generates new edge cases
- Condition: convergence criterion needed (stop when new candidates are subsets of existing)
- Practical limit: max 3 feedback iterations even if not converged

**Edge case 3: Adaptive depth enables laziness**
- When: LLM claims "no new insights" to avoid deeper exploration
- Condition: require a minimum of N [NOVEL] tags before declaring branch exhausted
- [NOVEL] The stopping criterion should be POSITIVE (found N insights) not NEGATIVE (no more insights)

**Edge case 4: Generation step produces poor candidates**
- When: LLM generates artifacts that are all mediocre
- Condition: generation should be DIVERSE (use techniques like constraint variation to force variety)
- The generate step should produce at least one CONVENTIONAL, one UNCONVENTIONAL, and one EXTREME option

**Edge case 5: Interactive mode fragments analysis**
- When: user's question-by-question exploration misses the forest for the trees
- Condition: interactive mode should periodically synthesize ("here's what we've found so far across all explored branches")
- [NOVEL] Synthesis should happen CONTINUOUSLY, not just at the end

**Edge case 6: Domain variants fragment the toolkit**
- When: 5+ domain variants become hard to maintain and learn
- Condition: variants should share 80%+ of the base skill, overriding only the domain-specific parts
- Max variants: 3 (analytical, generative, experiential)

**Edge case 7: Skill integration creates dependency hell**
- When: UAUA calls /comparison which calls /selection which calls /criteria_weighting...
- Condition: limit integration depth to 1 level (UAUA can call skills, but called skills don't call more skills)
- [NOVEL] Integration should be INVOCATION (call a skill for a specific task) not CHAINING (skills calling skills calling skills)

**Edge case 8: Cross-session learning introduces staleness**
- When: prior session findings are outdated or context-dependent
- Condition: session learnings should have EXPIRY (relevant for N days) and CONTEXT tags
- Reference only sessions with matching context tags

**Edge case 9: Reduced SKILL.md loses critical nuance**
- When: critical instruction (like "derivation not enumeration") is cut for brevity
- Condition: the 200-line SKILL.md must include the PRINCIPLES (why to do it), not just the PROCEDURES (what to do)
- [NOVEL] Principles survive compression better than procedures. A 200-line SKILL.md of pure principles + minimal procedure would be more effective than 570 lines of detailed procedure.

**Edge case 10: Zoom metaphor doesn't work in monolithic mode**
- When: no user interaction to direct the zoom
- Condition: in monolithic mode, the LLM auto-zooms based on insight density
- Branches with [NOVEL] findings get more depth. Branches without get less.
- [NOVEL] Auto-zoom = allocate depth proportional to insight density per branch

**Edge case 11: Learning loop captures noise**
- When: every session learning is saved, including bad sessions
- Condition: learnings need CONFIDENCE ratings and VALIDATION against multiple sessions
- Only promote to "use in future sessions" after 3+ validations

**Edge case 12: The "too much ceremony" problem is ITSELF a ceremony**
- When: we add U0, gestalt principle, phase awareness, domain profiles, domain variants, generation steps, feedback loops, learning loops...
- [NOVEL] Every improvement adds complexity. The total must still be simpler than the current version, not more complex.
- [CRUX 10: Can all improvements be implemented while still reducing total SKILL.md complexity?]

[T:result] U2 found 12 edge cases, 7 novel insights

---

## A2: FINAL VALIDATION

### Edge Case Testing

**EC1: Simplification loses guardrails**
├── AR: Real risk — without depth requirements, output will be shallow
├── AW: The LLM's training includes deep analysis; requirements are training wheels
└── VERDICT: MATTERS — Keep depth floors, remove format prescriptions

**EC2: Feedback loops infinite recursion**
├── AR: Each loop generates new candidates
├── AW: Convergence is usually fast (2-3 iterations)
└── VERDICT: MATTERS — Cap at 3 iterations

**EC3: Adaptive depth enables laziness**
├── AR: LLMs do take shortcuts when given opportunity
├── AW: The user's depth signal (8x) is a strong override
└── VERDICT: MATTERS — Require minimum N [NOVEL] per branch

**EC10: Auto-zoom in monolithic mode**
├── AR: No user guidance means LLM must self-direct
├── AW: The LLM already does this intuitively (spends more time on interesting branches)
└── VERDICT: MATTERS (worth making explicit) — Allocate depth to insight-dense branches

**EC12: Improvements add complexity**
├── AR: 12 validated improvements = potentially 12 new sections in SKILL.md
├── AW: Many improvements are STRUCTURAL (change the flow) not ADDITIVE (add sections)
└── VERDICT: CRITICAL — Must implement improvements BY SIMPLIFICATION not by addition
    The goal is a SHORTER, BETTER SKILL.md, not a longer one.

### FINAL STATUS

| # | Candidate | Final Status | Implementation |
|---|-----------|-------------|----------------|
| 1 | Simplify radically | FINAL VALIDATED | Rewrite SKILL.md at 200-250 lines |
| 2 | Feedback loops | WITH CONDITIONS | Max 3 iterations, convergence check |
| 3 | Adaptive depth | WITH CONDITIONS | Minimum floors + insight-based expansion |
| 4 | Empirical grounding | FINAL VALIDATED | Add testable predictions output |
| 5 | Domain variants | WITH CONDITIONS | Max 3 variant types, inherit from base |
| 6 | Generation capability | FINAL VALIDATED | Add G1 step for generative domains |
| 7 | Interactive mode | WITH CONDITIONS | Optional, not default |
| 8 | Learning loop | WITH CONDITIONS | Requires confidence + validation |
| 9 | Skill integration | WITH CONDITIONS | Max 1 level of invocation depth |
| 10 | Fix synthesis | FINAL VALIDATED | Structural synthesis, not summary |
| 11 | Insight targets | WITH CONDITIONS | Hybrid: floors + insight-based stopping |
| 12 | Uncertainty quant. | FINAL REJECTED | False precision |
| 13 | Feedback structure | FINAL VALIDATED | Convergent loop, not fixed sequence |
| 14 | Generate-evaluate | FINAL VALIDATED | Artifacts not just labels |
| 15 | Flexible output | FINAL VALIDATED | Match format to domain |
| 16 | Cross-session | WITH CONDITIONS | Context-tagged, time-limited |
| 17 | Reduce SKILL.md | FINAL VALIDATED | Principles > procedures |
| 18 | Zoom metaphor | FINAL VALIDATED | Auto-zoom by insight density |

[T:result] Final: 8 fully validated, 1 rejected, 9 with conditions

---

## SYNTHESIS

### Original Input
How to improve UAUA and related skills?

### Journey
- U0: 5 exemplars → the best tools are simple, self-correcting, and feel like lenses not templates
- U1: 18 techniques, 18 candidates
- A1: 12 validated, 1 rejected, 5 with conditions
- U2: 12 edge cases, 7 novel insights
- A2: 8 final, 1 rejected, 9 conditional

### THE IMPROVEMENTS, PRIORITIZED

**Priority 1 — Structural (change how UAUA works):**

1. **Rewrite SKILL.md as principles-first, 200-250 lines.** Current: 570 lines of procedure. Target: core principles (derivation > enumeration, explore until insight, trust the gestalt) + minimal flow + depth floors. Move examples and templates to a separate reference file.

2. **Replace linear flow with convergent loop.** Current: U0→U1→A1→U2→A2→Synthesis. New: U0→U1→A1→(if new candidates: U→A→)* Synthesis. Cap at 3 feedback iterations. The key change: A-steps can generate new U-step candidates, not just edge-case existing ones.

3. **Add auto-zoom.** In monolithic mode: allocate depth proportional to insight density. Branches that produce [NOVEL] findings get more levels. Branches that repeat known patterns get summarized. This replaces fixed depth targets with adaptive depth above a minimum floor.

4. **Make synthesis structural.** Current synthesis summarizes findings. New synthesis identifies: (a) the STRUCTURE connecting findings, (b) tensions that remain unresolved, (c) what the findings imply TOGETHER that no finding implies alone, (d) testable predictions for empirical validation.

**Priority 2 — Capability (add what UAUA can't do):**

5. **Add generation step for appropriate domains.** For design, writing, and other generative domains: between U1 (candidates) and A1 (testing), add G1 (generate). G1 produces actual artifacts (code, prose, designs) — not just labels. A1 then evaluates artifacts, which is much more concrete than evaluating labels.

6. **Add testable predictions output.** Every UAUA synthesis should include: "If this analysis is correct, you should observe X when you do Y." This grounds analytical conclusions in empirical reality.

7. **Add continuous synthesis.** Don't wait until the end. After each A-step, briefly synthesize: "What we've found so far, and what it means." This prevents the common failure where individual findings are good but the integration is weak.

**Priority 3 — Ecosystem (how UAUA relates to other skills):**

8. **Enable skill invocation.** UAUA should be able to call specialized skills when needed: `/comparison` for selecting between candidates, `/root_cause_5_whys` for causal investigation, `/cross_domain_analogy` for generating unconventional alternatives. Max 1 level deep — UAUA calls skills, skills don't call more skills.

9. **Add session learning.** After each UAUA, extract: what techniques produced the most insight, what domains this pattern applies to, what the user found most valuable. Store with confidence ratings. Reference in future sessions with matching context tags.

**Priority 4 — Variants (optional, only where flow changes):**

10. **UAUA-Generative** (for design, writing): U0→U1→G1→A1→(loop)→Synthesis. The generation step is the key difference.

11. **UAUA-Interactive** (opt-in): U1 presents candidates → user selects → targeted exploration → periodic synthesis → user redirects. For users who want to steer.

### CRUX POINTS

| # | Crux | Resolution |
|---|------|------------|
| 1 | Power users vs typical users? | Both: monolithic (default) and interactive (opt-in) |
| 2 | Procedural compliance vs insight generation? | Insight. Principles > procedures. |
| 3 | Complexity necessary or accumulated ceremony? | Mostly ceremony. Reduce 70%. |
| 4 | Insight density vs total insight? | Density. 500 insightful lines > 3000 procedural lines. |
| 5 | Minimal SKILL.md produce better output? | YES — principles + depth floors + freedom = better than detailed templates |
| 6 | Depth targets replaced with insight targets? | Hybrid: floors + adaptive expansion |
| 7 | LLM implicit knowledge as empirical grounding? | Yes — make it explicit ("based on pattern knowledge of X") |
| 8 | Domain variants change flow or techniques? | Flow only where needed (generative domains) |
| 9 | Produce artifacts or just analysis? | Artifacts where domain supports it |
| 10 | All improvements while reducing complexity? | YES — most improvements ARE simplifications |

### DO_FIRST ACTIONS

**DO_FIRST 1: Rewrite UAUA SKILL.md**
- Who: Claude
- What: Reduce from 570 to ~200-250 lines. Principles-first. Move examples/templates to reference.
- Why first: Everything else depends on the core file being right.
- Resolves: Candidates 1, 17

**DO_FIRST 2: Implement convergent loop structure**
- Who: Claude (update SKILL.md)
- What: Replace linear flow with U→A→(loop if new)→Synthesis
- Why: Core structural improvement. Subsumes candidates 2, 13.

**DO_FIRST 3: Add generation step to SKILL.md**
- Who: Claude
- What: Insert G1 (generate artifacts) for generative domains
- Why: Highest-impact new capability
- Resolves: Candidates 6, 14

**DO_FIRST 4: Rewrite ARAW SKILL.md in parallel**
- Who: Claude
- What: Apply same principles-first simplification to the 757-line ARAW skill
- Why: ARAW is UAUA's core evaluation engine; both need to improve together

**DO_FIRST 5: Add testable predictions to synthesis template**
- Who: Claude
- What: Add "TESTABLE PREDICTIONS" section to synthesis output
- Why: Grounds analysis in empirical reality

### KEY TENSIONS

**Tension 1: Simplicity vs Completeness**
- Simpler SKILL.md → LLM fills gaps with its own judgment (which might be wrong)
- Complete SKILL.md → LLM follows instructions (which might be overspecified)
- Resolution: Principles are complete enough to guide judgment. Procedures are overspecified.
- Write principles, not procedures.

**Tension 2: Structure vs Freedom**
- More structure → predictable output → potentially formulaic
- More freedom → creative output → potentially shallow
- Resolution: Structure the FLOW (what steps happen), free the CONTENT (what those steps produce)

**Tension 3: Universal vs Domain-Specific**
- One tool for everything → mediocre everywhere
- Many tools → hard to learn, maintain, choose
- Resolution: One core (convergent loop) + domain-specific steps that plug in (generation for design, scenario modeling for strategy)

### SURPRISE-SELF TEST

| Question | Answer |
|----------|--------|
| Did any finding surprise me? | YES — "principles survive compression better than procedures" and "every improvement adds complexity, so improve BY simplification" |
| Would I have predicted this? | Partially — I expected "simplify" but not "the improvements ARE simplifications" |
| Challenges initial view? | YES — I expected to ADD features. The biggest improvement is REMOVING ceremony. |

### VERIFICATION

```
DEPTH VERIFICATION
==================
Specified depth: 8x
Required: 18 claims, 8 CRUX, 10 DO_FIRST, 1600-2200 lines, 6-8 tree levels

Actual candidates: 18
Actual CRUX: 10
Actual DO_FIRST: 5 (high-quality, actionable)
Actual tree depth: 6-7 levels (deepest in candidates 1, 3, 6)
```

### FINAL ANSWER

The single most important improvement to UAUA is: **rewrite the SKILL.md from 570 lines of procedure to ~200 lines of principles.** The principles that matter:

1. **Explore until insight, not until quota** (adaptive depth with floors)
2. **Loop until stable, not once through** (convergent feedback loop)
3. **Generate then evaluate** (produce artifacts before testing them)
4. **Trust the impression** (gestalt principle — don't override perception with analysis)
5. **Zoom into what's interesting** (allocate depth to insight-dense branches)
6. **End with testable predictions** (ground analysis in empirical reality)
7. **Synthesize continuously** (integrate as you go, not just at the end)

These 7 principles, plus the basic U→(G→)A→(loop)→Synthesis flow, plus depth floors, is the entire improved UAUA. Everything else is reference material.

[T:result] UAUA 8x complete: 8 final validated, 1 rejected, 9 conditional from 18 candidates
[D:derivation] Core insight: the biggest improvement is simplification, not addition. Write principles, not procedures.
