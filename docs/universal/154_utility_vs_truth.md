# Universal: Utility vs Truth (154)

**Category**: SEARCH - What Criterion Evaluates Models?
**Source**: Bandler & Grinder "Frogs into Princes" - "Everything we tell you is a lie... our lies work"
**Structure**: Questions ordered by Value of Information (action divergence)

---

## VOI Rating = Action Divergence

- **HIGH**: Different answers → completely different action paths
- **MED**: Different answers → same general direction, different approach
- **LOW**: Different answers → same actions, different framing/flavor

---

## Core Insight

**"Everything we're going to tell you here is a lie. All generalizations are lies."**

**"We're not interested in whether what we offer you is true or not... We're only interested in what works."**

Two criteria for evaluating models:
- **Truth**: Does it accurately represent reality?
- **Utility**: Does it produce useful results?

These are different questions. A true model might be useless. A useful model might be "false" (a simplification, approximation, or deliberate distortion).

**The question**: Which criterion applies to this search?

---

## Q1: What criterion are you actually using?

[VOI: HIGH - wrong criterion = wrong evaluation]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| Using truth when utility matters | HIGH | Rejecting useful models | Switch to utility | Discard working tools because "inaccurate" |
| Using utility when truth matters | HIGH | Accepting false models | Switch to truth | Use broken map in critical situation |
| Unknown which using | MED | May be mismatched | Criterion check | Either direction wrong |
| Criterion matched to situation | LOW | Correct evaluation | Continue | Actually mismatched |

---

## Q2: Does accuracy matter for this use case?

[VOI: HIGH - some uses require truth, others don't]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| Accuracy critical (navigation, medicine) | HIGH | Need truth criterion | Verify accuracy | Useful but wrong = disaster |
| Accuracy not critical (heuristics, rough models) | HIGH | Can use utility | Accept working approximations | Demand accuracy where unnecessary |
| Unknown accuracy requirement | MED | May be wrong criterion | Requirement analysis | Either criterion |

---

## Q3: Is this model producing results?

[VOI: HIGH - utility is empirical]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| Model produces results | MED | Passes utility test | Keep using | Discard working model |
| Model doesn't produce results | HIGH | Fails utility test | Discard or modify | Keep using broken model |
| Unknown if produces results | MED | Need to test | Run test | Either keep or discard wrong |

---

## Q4: Are you rejecting useful models because they're "wrong"?

[VOI: HIGH - common error]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| Rejecting useful for being inaccurate | HIGH | Losing working tools | Accept useful approximations | Demand impossible accuracy |
| Accepting useful despite inaccuracy | LOW | Pragmatic | Continue | Actually should reject |
| Unknown if rejecting useful | MED | May be losing tools | Rejection audit | Either losing or keeping |

---

## Q5: Are you accepting broken models because they're "true"?

[VOI: MED - opposite error]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| Accepting broken for being accurate | MED | Using useless truth | Require utility | Accurate but useless |
| Requiring both truth and utility | LOW | High standard | Continue | Actually should relax |
| Unknown if accepting broken | MED | May be stuck | Acceptance audit | Either stuck or not |

---

## Q6: What's the cost of being wrong?

[VOI: MED - determines criterion threshold]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| High cost of error | MED | Need higher accuracy | Favor truth criterion | Useful but catastrophic when wrong |
| Low cost of error | MED | Can accept approximation | Favor utility criterion | Overly cautious |
| Unknown cost | MED | May be miscalibrated | Cost analysis | Either direction |

---

## Q7: Is this model a useful lie?

[VOI: MED - some lies are tools]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| Useful lie (simplification, heuristic) | MED | Tool, not claim | Use as tool | Reject because "false" |
| Claim presented as truth | MED | Must evaluate truth | Check accuracy | Accept false claim |
| Unknown status | MED | May be misclassifying | Status check | Either misuse |

---

## Q8: Can you test utility?

[VOI: LOW - determines how to evaluate]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| Can test utility | LOW | Run tests | Test | Theorize when could test |
| Cannot test utility | LOW | Must rely on other criteria | Use other criteria | Fake tests |
| Unknown if testable | LOW | May have option | Testability check | Either direction |

---

## Q9: What would count as "working"?

[VOI: LOW - defines success for utility]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| Clear success criteria | LOW | Can evaluate | Test against criteria | Vague evaluation |
| Unclear success criteria | LOW | Hard to evaluate | Define criteria first | False evaluation |
| Unknown clarity | LOW | May be vague | Criteria check | Either direction |

---

## Q10: Is there a better model available?

[VOI: LOW - comparative evaluation]

| Entry | VOI | Action Divergence | ASSUME RIGHT | ASSUME WRONG |
|-------|-----|-------------------|--------------|--------------|
| Better model exists | LOW | Switch | Adopt better model | Stay with worse |
| No better model | LOW | Keep current | Continue | Miss better option |
| Unknown if better exists | LOW | May be missing | Search for alternatives | Either miss or waste search |

---

## Summary Statistics

- Total questions: 10
- Total entries: 34
- HIGH VOI: 8 (24%)
- MED VOI: 14 (41%)
- LOW VOI: 12 (35%)

---

## Question Order by Action Divergence

**Ask first (HIGH VOI):**
1. Q1: Which criterion using? - evaluation basis
2. Q2: Does accuracy matter? - requirement
3. Q3: Producing results? - utility test
4. Q4: Rejecting useful for being "wrong"? - common error

**Ask if relevant (MED VOI):**
5. Q5: Accepting broken for being "true"? - opposite error
6. Q6: Cost of being wrong? - threshold
7. Q7: Is this a useful lie? - tool vs claim

**Low priority (LOW VOI):**
8. Q8: Can test utility? - method
9. Q9: What counts as "working"? - criteria
10. Q10: Better model available? - comparison

---

## Key Insight

**Two different questions:**
- Is it TRUE? (Does it accurately represent reality?)
- Does it WORK? (Does it produce useful results?)

A map can be "wrong" and still get you where you're going.
A map can be "accurate" and still be useless.

**The trap**: Using truth criterion when utility is what matters. Rejecting useful simplifications because they're "not accurate."

**The move**: First ask which criterion applies. For heuristics, approximations, and practical models - utility. For navigation, medicine, safety - truth (and utility).

---

## The Modeling Stance

From Bandler & Grinder:

"We have no idea about the 'real' nature of things, and we're not particularly interested in what's 'true.' The function of modeling is to arrive at descriptions which are useful."

"We do not test the description we arrive at for accuracy... All we do in order to understand whether our description is an adequate model... is to find out whether it works or not."

**This is a different game than science.** Science seeks accurate models. Modeling seeks useful models. Both are valid - for different purposes.

**The question for your current search**: Which game are you playing?
