---
import Base from '../layouts/Base.astro';
---

<Base title="About" activeNav="about" description="The philosophy behind reasoning-toolkit: why alternation works, how it compares to other approaches, and what's proven vs. claimed.">
  <h1>About</h1>

  <h2 id="the-problem">The problem</h2>
  <p>There are two fundamentally different operations in reasoning:</p>
  <ol>
    <li><strong>Exploration</strong>: What options exist? What haven't I considered? What's the full space of possibilities? This is divergent — it expands your view.</li>
    <li><strong>Testing</strong>: Is this actually true? What happens if it's wrong? What survives scrutiny? This is convergent — it narrows your view.</li>
  </ol>
  <p>Most tools do one or the other. Brainstorming tools explore but don't test. Debate tools test but don't explore. Chain-of-thought does a bit of both but neither systematically. And none of them <em>alternate</em> — exploring, then testing what you found, then exploring the edge cases of what survived, then testing again.</p>
  <p>The alternation matters because the two operations have structural blind spots that don't overlap. Exploration alone finds options but can't tell you which ones work. Testing alone validates what you're looking at but can't tell you what you're not looking at. Alternating covers both.</p>

  <h2 id="methods">The core methods</h2>
  <h3>ARAW (Assume Right / Assume Wrong)</h3>
  <p>Takes any claim and branches it: what follows if true? What follows if false? Then recurses — every conclusion is another claim. The result is a tree of tested claims stored in SQLite, with leverage scores, crux detection, and domain classification.</p>
  <p>ARAW is operationalized Popperian falsification for everyday thinking. It forces symmetric treatment — whatever you do for "assume right," you must equally do for "assume wrong." This addresses confirmation bias not by knowing about it, but by having a process that forces the corrective operation.</p>

  <h3>UAUA (Universalize → ARAW → Universalize → ARAW)</h3>
  <p>Alternates between two mathematically distinct search operations:</p>
  <ul>
    <li><strong>Universalization</strong> operates on N-valued type theory. It asks "what is this an instance of?" and derives all instances — searching horizontally across the possibility space.</li>
    <li><strong>ARAW</strong> operates on binary Boolean logic. It asks "is this true or false?" and eliminates branches through contradiction — searching vertically into depth.</li>
  </ul>
  <pre><code>U1: Map the space (divergent, N-valued) → candidates
 ↓
A1: Test candidates (convergent, binary) → validated/rejected
 ↓
U2: Find edge cases of survivors (divergent) → new candidates
 ↓
A2: Final validation (convergent) → what survived all rounds</code></pre>
  <p>Each pass uses a fundamentally different logic — type enumeration vs. binary elimination — so their blind spots don't overlap. Information-theoretically, each ARAW pass maximizes entropy reduction (selecting the crux that most constrains remaining uncertainty), while each universalization pass maximizes entropy expansion (finding dimensions not yet explored).</p>

  <h3>207 structured skills</h3>
  <p>Each skill is a structured prompt that guides you through a specific type of thinking. Skills are classified by <strong>universality</strong> — see below.</p>

  <h2 id="universal-vs-heuristic">Universal vs. heuristic</h2>
  <p>Some skills are <strong>universal</strong> — their applicability is entailed by the problem itself. If you have a complex problem, decomposition applies; that follows from what "complex" means. If you have a claim, testing it against its negation applies; that follows from what "claim" means. You don't need to check whether these skills are relevant. They are relevant whenever their trigger condition exists, and the trigger condition is defined by the skill.</p>
  <p>Other skills are <strong>heuristics</strong> — they work well in specific contexts, but whether they apply to <em>your</em> context requires judgment. SCAMPER is a useful ideation framework, but whether it helps depends on what you're ideating about. SWOT analysis surfaces strategic factors, but whether those are the right factors depends on your situation.</p>
  <p>The <a href="/skills">skill tiers</a> loosely reflect this. Tier 1 skills tend to be universal. Tier 4 skills tend to be context-dependent. The boundary isn't always clean — some skills are universal in principle but heuristic in execution — but the distinction tells you which skills to trust on contact and which to validate before relying on.</p>
  <p>For the full philosophical treatment, see <a href="/essays/universal-principles">Universal Principles of Mathematical Problem Solving</a>.</p>

  <h2 id="comparison">How it compares</h2>
  <table>
    <thead>
      <tr><th></th><th>reasoning-toolkit</th><th>AI agents</th><th>Prompt libraries</th><th>Chain-of-thought</th></tr>
    </thead>
    <tbody>
      <tr><td>Structured exploration</td><td>Yes (universalization)</td><td>No</td><td>No</td><td>Implicit</td></tr>
      <tr><td>Systematic testing</td><td>Yes (ARAW)</td><td>No</td><td>No</td><td>Implicit</td></tr>
      <tr><td>Alternation between both</td><td>Yes (UAUA)</td><td>No</td><td>No</td><td>No</td></tr>
      <tr><td>Persistence across sessions</td><td>Yes (SQLite)</td><td>Varies</td><td>No</td><td>No</td></tr>
      <tr><td>Interactive visualization</td><td>Yes (Sigma.js)</td><td>No</td><td>No</td><td>No</td></tr>
      <tr><td>Cross-run synthesis</td><td>Yes</td><td>No</td><td>No</td><td>No</td></tr>
      <tr><td>Classifies skills by universality</td><td>Yes (tiered)</td><td>No</td><td>Some</td><td>No</td></tr>
      <tr><td>Philosophy of <em>why</em> it works</td><td>Yes (essays)</td><td>No</td><td>No</td><td>No</td></tr>
    </tbody>
  </table>
  <p>The difference isn't features — someone could add any of these to an existing framework. The difference is design intent. Agents execute tasks. Prompt libraries produce better outputs. This toolkit checks whether you're solving the right problem before you solve it.</p>

  <h2 id="honesty">What's proven and what's claimed</h2>
  <p><strong>Proven (by the structure itself):</strong></p>
  <ul>
    <li>If you test both "assume right" and "assume wrong," you will consider alternatives you wouldn't have otherwise. This is logically necessary — the operation forces it.</li>
    <li>If you alternate between exploration and testing, you cover failure modes that either operation alone misses. This follows from the fact that their blind spots are complementary.</li>
  </ul>
  <p><strong>Claimed (based on principles, not yet empirically validated):</strong></p>
  <ul>
    <li>The alternation produces better decisions than non-alternating methods.</li>
    <li>The universal/heuristic skill classification helps users pick the right tools faster.</li>
    <li>The full UAUA cycle (4 passes) is meaningfully better than a 2-pass version.</li>
  </ul>
  <p>This is experimental. The philosophy is grounded in epistemology and information theory. The implementation is v1. If you find where it breaks, that's valuable — <a href="https://github.com/benjam3n/reasoning-toolkit/issues">open an issue</a>.</p>

  <h2>License</h2>
  <p>Apache 2.0. Use it, modify it, build on it.</p>
  <p><a href="https://github.com/benjam3n/reasoning-toolkit">Source on GitHub</a></p>
</Base>
